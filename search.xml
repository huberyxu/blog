<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Abird平台OLAP技术架构的一些思考(2)]]></title>
    <url>%2F2018%2F02%2F26%2FAbird%E5%B9%B3%E5%8F%B0OLAP%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83-2%2F</url>
    <content type="text"><![CDATA[承接上文，讨论一下Abird平台OLAP技术架构之——实时 的一些思考 背景简单描述下实时的业务背景：对于部分用户体验数据以及其他数据，业务方有实时查询的需求，并且也要结合多维分析来定位异常问题。如实时vv/成功率/卡顿率/秒开率等指标，而且需要省份、运营商、网络制式、app版本等维度辅助定位陡升、陡降问题力求第一时间监控报警，还有诸如心跳等实时业务。 现状上文说了，我们采用MOLAP的实现方法，实时的技术架构也不外如是。我们目前采用TT+Blink+Streaming+OTS的方式，注意：Blink依据数据量级以及ETL复杂度选择使用。在Streaming中我们实现了配置化cube构建引擎，以降低后续流计算的接入成本。由于主要集中于业务，并未为此付出过多精力进行数据膨胀的优化。导致部分业务完全立方体的构建极易造成数据膨胀。其次，streaming集群跨数据中心io瓶颈、任务不稳定等随着业务增加任务增加，日渐增加我们的人肉运维成本。 改进方案我们考虑如下几种改进方案： SQL On Blink streaming通用化OLAP的cube构建引擎基于Blink重构，并完善优化 基于Druid实时OLAP引擎——对Druid的介绍详见笔者Druid专栏 方案一对于方案1，集团Bayes平台已足够成熟，能够支持工程开发以及运维优化，但是随着业务增加，需要接入的TT流更多，业务SQL增多会面临离线一样的问题，尽管这样看似离线和实时在某些公共的业务上能整合计算逻辑，但实际上编写OLAP多维度的复杂SQL同样不易维护，而且会持续增加开发、运维成本。针对独立性强的、处理逻辑简明的可以采取该种方案。 方案二对于方案2，能形成自主研发的基于Flink的实时OLAP引擎，从长远来看意义重大，在已有的Streaming通用化方案上迁移容易，但难点在其三： 由于前期cube构建采用完全立方体构建，实际上是对n个维度做了2^n条记录的分发再聚合，并未考虑后续的优化方案（这里只有必要维度的优化，对于维度组、衍生维度等比较实用的优化没有），而如果进行后续优化，实际上是对cube算法的重构，我们可以采用”By Layer”算法实现清晰易维护的MR流计算逻辑或者采用“Fast Cubing Algorithm”算法。具体不在此展开，参考kylin cube构建算法：http://kylin.apache.org/blog/2015/08/15/fast-cubing/但不可否认的是，前者需要存储中间结果后者也需要做一定研发工作，真实实践仍需对其On Blink做性能评估，长远来看是有价值的也是值得一试的。 由于目前使用Blink只能依赖Bayes平台，暂不提供jar任务提交方式。——硬伤，必须解，解不了此方案可pass。 最终cube结果存储。如果考虑该种方案，要对存储立方体数据做出一套规划，如采取kv分布式存储Hbase等如何面对非rowkey格式查询，以及一些runtime聚合操作的实现（如衍生维度在查询时聚合、如时间维度上的下行采样等场景）。可以借鉴kylin/opentsdb等对HBase的应用，有同学会问，kylin2.0以上版本正在支持实时cube构建，能够达到亚秒级响应，为何不用？具体原因参见上一篇，说白了还是平台兼容性，我们作为业务部门balabala… 方案三对于方案3，是笔者在前段考虑实时通用化时重点考虑的方案。在此重点研讨优缺点如下： 我们能得到什么？ Druid对事件数据聚合，此类数据有较为明显的三类数据列构成：时间戳列、维度数据列（字符串数据，可用来过滤）、指标数据列（druid在导入数据时会根据配置对原始数据的数值列进行roll up得到指标数据列）。这种做法实际上是将Cube的计算逻辑后移（这种说法不严谨，实际上并无cube计算，而是基于最细粒度聚合的再计算），我们只需梳理事实表即可。这里说的最细粒度聚合实际上就是group by A,B,C,……所有一开始指定的维度的聚合计算。由此，我们可以避免cube计算以及优化等一系列“故事”。 避免了cube计算也就避免了上文提到的streaming稳定性的问题，更进一步，Druid这种方式更具查询灵活性，最细粒度聚合值有了，在其上所有需要的聚合层级都可得，也能支持秒级查询响应。 具备了上述灵活性，查询受用面积更广，实际上更进一步避免了“实时热更新”的问题。不在此展开（这个问题曾一度困扰笔者，解决思路如新版Flink的CEP和cloudera的streaming+drools），附链接：http://blog.cloudera.com/blog/2015/11/how-to-build-a-complex-event-processing-app-on-apache-spark-and-drools/ 我们还需考虑什么？ Druid目前不支持join查询，这意味着我们许多指标如成功率/卡顿率/秒开率等需要将相关因子在ETL时打平并落入一个datasource，这里Blink完全能够胜任不足为虑。 集团内Druid集群服务。据目前了解，只有个别团队提供Druid对外服务，但其产品未对外开放，运维成本能降低（自行搭建的Druid角色较多运维难度较大），但运维门槛会加大（毕竟未开放想让人家提供无偿贴心服务么）。而且资源受限，是否能够支撑重点业务，数据ttl等问题都需要根据业务进行评估。 综合考虑之，还是建议用Blink+Druid实现实时OLAP，这是目前综合困扰我们的稳定性和灵活性而言较好的方案。 以上两篇文章的思考皆是立足于现实，如果读者完全拥抱开源无需多虑选择合适的方案即可，这也是为什么笔者在两个重点技术中用的是“我们能得到什么？”和“我们还需考虑什么？”的原因，是笔者项目实际情况所致而非技术严格意义上的优缺点。当然，即使是开源的技术应用于生产时也是有很多路要走。思考良多发现技术没有一劳永逸的，哪怕根据实际情况选择了最合适的架构，你也要为之付出许多的研发、运维成本，所以不要被“我们还需考虑什么？”吓坏，经过团队研讨后确定了的走下去就不怕错，至少比面对挫折犹豫不决踌躇不前要好，就当技术储备也能获益良多。]]></content>
      <tags>
        <tag>OLAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Abird平台OLAP技术架构的一些思考(1)]]></title>
    <url>%2F2018%2F02%2F26%2FAbird%E5%B9%B3%E5%8F%B0OLAP%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83-1%2F</url>
    <content type="text"><![CDATA[背景本文重点讲述abird平台下vpm项目的需求梳理和架构思考。vpm项目主要是优酷“用户体验数据”的多维度分析产品，旨在辅助业务方收集、展示、监控和处理用户体验问题，反应用户体验的一些通用指标诸如vv/uv/成功率/卡顿率/秒开率此类，此外，还有错误码分布、异常日志分析等功能。 平台上OLAP场景大致分为两类： 临时查询：指用户通过手写SQL来完成一些临时的数据分析需求。这类需求的SQL形式多变、逻辑复杂，对响应时间没有严格的要求。 固化查询：指对一些固化下来的取数、看数的需求，通过数据产品的形式提供给用户，从而提高数据分析和运营的效率。这类需求的SQL有固定的模式，对响应时间有比较高的要求。 多数情况下，vpm项目下的用户体验多维分析属于固化查询，有既定需求形态和固定的查询模式，如大盘。特殊情况下有个别专项，实际上专项中也是把临时需求固化下来形成小的产品，并无真正意义上的临时查询。 现状从OLAP技术手段来看，我们一直采用MOLAP的实现方法，将细节数据和聚合后的数据均保存在cube中，以空间换效率，查询时效率高，但生成cube时需要时间和空间。这里针对离线和实时我们有两种实现，分别探讨如下： 离线中，我们在odps上通过编写sql来实现cube计算，然后将cube数据存入ots中，在应用中即时查询。由于前期有独立的abird_vpm表，开发效率较高，通过计算中间表的方式也能较大的优化计算效率、应对业务扩展。然而随着业务的再扩展问题凸显，首先，业务已不局限于abird_vpm表，还要零零散散的接入优土公共汇总层乃至明细层的其他源表，尽管对sql编写和cube计算层级做出一些规范和优化，但仍无法摆脱无底洞似的业务新增和变更的代码开发，任务增多日渐不好维护；另外，起始时缺乏对维表做统一管理，ots的表结构设计无法支持范围查询等等使得技术架构的灵活性大打折扣，给后续开发造成难度。 改进方案我们考虑如下几种改进方案： 支持olap的分布式存储引擎，如集团的hybirddb for mysql和ADS 进行预计算的olap引擎如kylin、druid、es（非预计算） 方案一对于方案1，尽管能够提供最佳的灵活性，但由于以上存储引擎实际上都是全量存储明细数据，暂不论如何创建索引如何聚合查询，这种存储成本实际上是我们不希望看到的，因为用户体验数据大部分都是基于日志数据、点击数据，明细数据带来的价值是极小的、量级是巨大的，没必要做全量存储。 方案二对于方案2，es毋庸置疑适用于大数据量的过滤、检索的明细查询，在应对聚合查询时无法达到秒级响应甚至是分钟级，es在平台内也有用武之地但不是olap。而Druid主要面向的是实时Timeseries数据，我们也有类似的场景如实时分钟级趋势等，但这里面向的还是数仓中按T+1的结构化表。kylin显然是我们倾向的方案，下面来看下kylin的利弊和适合与否 引入kylin能给我们带来什么？kylin能够支持的量级、性能、查询响应、灵活性是毋庸置疑的，但逐一比对我们发现，目前的技术架构除了灵活性（这里包括构建cube的配置化和查询模式的灵活性）不能比拟外，其他的并不是我们的实际痛点，同样是依赖上游调度的预计算、基于kv分布式存储的查询引擎使得性能和响应速度并不能获得太大的提升。那我们就着重来看下kylin能带来哪些灵活性的提升以及是否解决我们的实际痛点。 我们知道，Kylin的设计是基于一个星形模型，基于一个事实表和多个维表构建cube模型，生成mr job来计算每层cuboid并落地hbase。通过可视化的web ui即可做到cube的配置化实现（这里是我们想要的）。然而为此，我们仍然需要考虑： 我们能得到什么？ 对于基数计算、精确去重问题（如我们的uv计算），不像市面上其他引擎采用近似估计如hyperloglog算法，kylin目前已在1.5.3版本中实现了全类型精确去重计数的支持 配置化的cube实现 cube的剪枝优化，如必要维度、维度组、衍生维度等。我们目前只有必要维度的优化，sql的开发模式使得做这些优化不能通用所以要麻烦一些（这里的优化带来的计算资源的节省和存储空间的节省目前不是我们的瓶颈） 我们还需考虑什么？ 构建小规模的数据集市层，如事实表采用宽表，增建维表或引用已有维表（当然符合定义规范的维度也可以在runtime应用里解决，这里说的是标准做法）。所有非标准星型的数据模型，都可以通过预处理ETL先拉平，做成一个宽表（目前abird_vpm并非严格意义宽表），对于复杂指标涉及到表达式时，也可以通过提前处理解决。把表达式单独转成一列，再基于这列做聚合。问题在于，多个业务表面类似但实质不同，如何规范统一化的数据模型而不是烟囱建设数据集市是将要把控的重点。 kylin在集团内是否有对标产品？目前调研都不够理想，犹如孔明灯、夸克/FBI等产品业务方已在使用，我们基于它们建设也未尝不可，但我们需要的灵活性（cube配置化、查询模式多变）仍要调研取证。如果不基于已有产品建设，那么引进kylin的成本稍大，要和odps数据源集成，和计算引擎集成等等，要搭建至少有个小规模的kylin集群，一边还要跟社区（当然自己用可以慢慢来）。换句话说甚至不如开发一个类似kylin构建cube内核的通用mr来的实在（我们之前的兼容odps的最简方案）。 seiya应用的数据层改造。目前针对ots特定的表结构已有一套数据层查询方案，但实际上引入kylin后，查询模式可以转变为基于数据模型的sql driver查询，显而易见，当我们集市构建丰富、cube模型设计合理（维度、指标等）的情况下，能够在不改变cube构建任务的情况下、在应用层即可应对更多的业务开发。 其次需要考虑的是集群的开放、运维等问题 的确，考虑长远来说的话kylin引入的收益是日渐明显的，建立cube后相比目前SQL开发能提升效率，但引入之前的宽表构建也是我们不可避免的问题，松散的业务同样需要独立的模型——仍然要从构建宽表开始考虑一遍（如果新增的业务不能符合标准星型模型的话），等等，我们作为业务部门而不是平台部门是否能分散精力组建也是我们需要考虑的。从目前我们讲求较高的开发收益比来说（从0到1的收益远比从1到1.1大），不适合进行投入式的技术研发，或许有更具性价比的产品等待我们调研发现。长远来讲，我们希望Abird能将优土用户体验等日志从采集、到通用etl、到通用olap形成一套完备的解决方案，还是路漫漫其修远兮。在此提出一些思考与读者讨论，也欢迎读者共建。]]></content>
      <tags>
        <tag>OLAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-生产部署]]></title>
    <url>%2F2018%2F02%2F13%2FDruid-%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Druid-性能]]></title>
    <url>%2F2018%2F02%2F13%2FDruid-%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[性能Druid在多个组织、企业中都有生产集群，而且为了评估它的性能，我们选择了2014年初Metamarkets的生产集群数据，与其他数据库相比，我们还包括了TPC-H的结果。 查询性能Druid的查询性能因查询的不同而不同。例如，对高基数维度的值进行排序比在一个时间范围内的简单计数代价要昂贵得多。为了展示生产Druid集群中的平均查询延迟，我们选择了8个查询最多的数据源，如表2所示。 大约30%的查询涉及过滤和聚合不同类型的metric，60%的查询是由一个或多个维度组成，而有10%的查询是搜索查询和元数据检索查询。聚合查询中扫描的列数大致遵循指数分布。涉及单个列的查询非常频繁，涉及所有列的查询非常少见。 关于我们的结果有几点注意: 结果来自我们生产集群中的“hot” tier，大约50个数据源，数百个用户发出查询 在“hot”层有大约10.5TB的RAM，并且大约有10TB的部分被加载。总的来说，这一层有大约500亿条记录。没有显示每个数据源的结果。 该tier使用英特尔®Xeon®e5 - 2670处理器,由1302个处理线程和672核(hyperthreaded)。 使用内存映射存储引擎(机器配置为内存映射数据，而不是将数据加载到Java堆中)。 图8展示查询延迟、图9展示了每分钟的查询。在所有不同的数据源中，查询延迟大约是550毫秒，90%的查询在不到1秒内返回，在2秒内达到95%，99%的查询在不到10秒内返回。在2月19日，我们偶尔观察到延迟的峰值，因为我们最大的数据源之一在非常高的查询负载下，Memcached实例上的网络问题变得更加复杂。 TPC-H Data的查询测试我们还在TPC-H数据上展示了Druid的性能。大多数TPC-H查询并不直接应用于Druid，因此我们选择了更典型的Druid负载查询来演示查询性能。作为比较，我们还提供了使用MyISAM引擎(在我们的实验中较慢的)来查询MySQL的结果。 我们选择MySQL作为基准，因为它普遍流行。我们不选择另一个开源的列存储，因为我们没有信心可以正确地调优它以获得最佳性能。 我们的Druid使用Amazon EC2 m3.2xlarge实例类型(Intel®v2 @ 2.80 ghz Xeon®e5 - 2680)的历史节点和c3.2xlarge实例(Intel®v2 @ 2.50 ghz Xeon®e5 - 2670)对代理节点。我们的MySQL设置是一个Amazon RDS实例，它运行在同一个m3.2xlarge实例类型上。 1GB TPC-H数据集的结果如图10所示，100GB数据集的结果如图11所示。 给定时间间隔的select count(*)查询Druid的扫描速率为53,539,211行/秒/核心，select sum(float)查询的扫描速率为36,246,530行/秒/核心。 最后,我们提出扩展Druid,以满足日益增长的数据量和100GB tpc-h的数据集。 我们看到,当我们从8核心增加到48核,并不是所有类型的查询都能实现线性扩展,除了简单的聚合查询,如图12所示。并行计算系统的速度增长通常是由系统的顺序操作所需要的时间所决定的。在这种情况下，代理级别上大量需要查询工作不能并行化。 数据写入性能为了展示Druid的数据写入延迟，我们选择了几个不同维度、指标和事件的生产数据源。生产者设置由6节点,360 gb的内存和360核(12 x英特尔®Xeon®e5 - 2670)请注意，在这个设置中，还有其他几个数据源正在被处理，许多其他的Druid相关的数据写入任务在机器上同时运行。 Druid的数据写入延迟严重依赖于被写入的数据集的复杂性。数据的复杂性是由每个事件中维度的数量、每个事件中的指标数量以及我们想要在这些指标上执行的聚合类型决定的。使用最基本的数据集(只有一个时间戳列)，可以以800000个事件/秒/核心 的速率写入数据，这实际上只是一个我们可以快速反序列化事件的速度。真实世界的数据集从来没有这么简单。表3显示了数据源的选择和它们的特性。 我们可以看到，根据表3中的描述，延迟会发生显著的变化，维度和度量并不总是写入延迟的因素。我们在简单数据集上看到了一些较低的延迟，因为这是数据生产者提供数据的速率。结果如图13所示。我们将吞吐量定义为一个实时节点可以接收到的事件的数量，并且可以进行查询。如果将太多事件发送到实时节点，则这些事件将被阻塞，直到实时节点有能力接受它们。我们在生产中测量的峰值写入延迟为22914.43个事件/秒/核心，数据来源为30个维度和19个指标，运行一个Amazon cc2.8xlarge实例。 我们提出的延迟测试足以解决交互带来的问题。我们希望延迟时间的变化更少。通过增加额外的硬件来减少延迟仍然是可能的，但是我们没有选择这样做，因为基础设施成本仍然是我们的考虑因素。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-查询Api]]></title>
    <url>%2F2018%2F02%2F13%2FDruid-%E6%9F%A5%E8%AF%A2Api%2F</url>
    <content type="text"><![CDATA[查询ApiDruid有自己的查询语言，通过POST请求。Broker, historical, 和real-time nodes都共享相同的查询API。POST请求的主体是一个JSON对象，其中包含各种查询参数的键值对。一个典型的查询将包含数据源名称、结果数据的粒度、时间范围、请求的类型以及需要聚合的metric。结果也将是一个JSON对象，其中包含在时间段内的聚合指标。 大多数查询类型还支持一个过滤器集合。过滤器包括维度名和维度值的Boolean表达式，可以指定多个维和值的组合。当提供一个过滤器时，只有符合过滤器数据的子集将会被扫描。Druid有处理复杂的嵌套过滤器的能力，能够进行深度数据钻取。 具体的查询语法取决于查询类型和请求。查询一周的样本数据语法如下: { &quot;queryType&quot;: &quot;timeseries&quot;, &quot;dataSource&quot;: &quot;wikipedia&quot;, &quot;intervals&quot;: &quot;2013-01-01/2013-01-08&quot;, &quot;filter&quot;: { &quot;type&quot;: &quot;selector&quot;, &quot;dimension&quot;: &quot;page&quot;, &quot;value&quot;: &quot;Ke$ha&quot;}, &quot;granularity&quot;: &quot;day&quot;, &quot;aggregations&quot;: [{&quot;type&quot;:&quot;count&quot;, &quot;name&quot;:&quot;rows&quot;}] } 上面所示的查询将返回从2013-01-01到2013-01-08的Wikipedia数据源中行数统计，只对那些“page”维度的值等于“Ke$ha”的行进行filter计算。结果将按天进行分桶，并且将是一个JSON数组的形式: [ { &quot;timestamp&quot;: &quot;2012-01-01T00:00:00.000Z&quot;, &quot;result&quot;: {&quot;rows&quot;:393298} } { &quot;timestamp&quot;: &quot;2012-01-02T00:00:00.000Z&quot;, &quot;result&quot;: {&quot;rows&quot;:382932} } ... { &quot;timestamp&quot;: &quot;2012-01-07T00:00:00.000Z&quot;, &quot;result&quot;: {&quot;rows&quot;: 1337} } ] Druid支持多种类型的聚合，包括对浮点数和整数类型的sum、最小值、最大值和复杂聚合，如基数估计和近似分位数估计。聚合的结果可以结合在数学表达式中，形成其他的聚合。这篇文章的主旨是查询API，更多的信息可以在网上找到。 在撰写本文时，还没有实现对Druid的join查询。这是一个工程资源分配和用例决策的功能，而不是技术价值驱动的决策。事实上，Druid的存储格式将允许实现连接(这并不意味着失去了维度列)并且为了实现它我们交流了几个月。到目前为止，我们已经做出了这样的选择:实施成本不值得投资于我们的产品。做出这一决定有两方面原因。 在我们的实践中，join连接查询在分布式数据库中有不可抗的瓶颈。 添加繁重join查询功能的收益比提升高并发的问题要小。 join查询实质上是基于一组共享key的两个或多个数据流的合并。对于我们所知道的连接查询，主要基于hash策略或排序合并策略。基于哈希的策略要求，除了一个数据集之外，所有的数据集看起来都像一个散列表，然后在这个哈希表上对“主”流中的每一行执行查找操作。排序合并策略假定每个流都由连接键进行排序，从而允许流的加入。然而，每一种策略都需要以排序顺序或散列表形式实现一些具体化的流。 当连接的都是明显的大表(&gt; 10亿条记录)时，实现流join需要复杂的分布式内存管理。内存管理的复杂性会因为我们“高度并发的多租户”目标而被放大。就我们所知，这是一个积极的学术研究问题，我们愿意以可扩展的方式帮助解决这个问题。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-存储格式]]></title>
    <url>%2F2018%2F02%2F13%2FDruid-%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[存储格式Druid的数据表(称为数据源)是事件的集合，并划分为一组segment，其中每个段通常为5 - 1000万行。形式上，我们将segment定义为一组数据的集合，这些数据跨越了一定时间周期。Segments是Druid基本存储单元，并在其上进行分配和备份。 Druid需要一个时间戳列来简化数据分配策略、数据保留策略和查询优化策略。Druid根据时间间隔划分数据源，通常为一个小时或一天，并可以进一步以其他列分割以实现所需的段大小。划分segment的时间粒度是数据量和时间范围的函数。跨年的时间粒度会比天级的粒度被更好的划分，跨天的时间粒度会比小时级更优。 段是由数据源标识符、数据的时间区间和在创建段时的版本号来联合标识的（唯一）。版本字符串表示段数据的最新程度;具有较晚版本的片段(在一定时间范围内)比较老版本的片段具有更新的数据视图。此段元数据被系统用于并发控制;特定的时间范围内的读取操作总是访问最新版本的数据。 Druid段以列存储存储。考虑到Druid最适合聚合事件流(所有进入Druid的数据都必须有时间戳)，将聚合信息作为列存储而不是行存储的优势具有较好的文档记录[1]。列存储允许更高效的利用CPU，因为只需要加载和扫描所需的列的内容。在面向行的数据存储中，与行关联的所有列必须作为聚合的一部分进行扫描。额外的扫描会引起性能下降[1]。 Druid有多个列类型来表示各种数据。根据列类型不同，使用不同的压缩算法来降低内存和磁盘的存储成本。在表1给出的示例中，页面、用户、性别和城市列仅包含字符串。直接存储字符串是不必要的开销，而字符串列可以是字典编码的。字典编码是一种常用的压缩数据的方法，它已经被广泛用于其他的数据存储如PowerDrill[17]。在表1中的示例中，我们可以将每个page字段映射到一个惟一的整数标识符。 Justin Bieber -&gt; 0 Ke$ha -&gt; 1 这个映射允许我们将page列表示为一个integer数组，数组索引对应于原始数据集的行。 [0, 0, 1, 1] 所得到的Integer数组很适合压缩算法。关于编码的通用压缩算法在列存储中非常常见。Druid使用LZF [24] 压缩算法。 类似压缩方法可以应用于数值列。例如，在表1中Characters Added|Characters Removed列也可以表示为单个数组。 Characters Added -&gt; [1800, 2912, 1953, 3194] Characters Removed -&gt; [25, 42, 17, 170] 在这种情况下，我们压缩原始值而不是它们的字典表示值。 索引、过滤数据在许多现实的OLAP工作流中，通常根据某些特定维度聚合指标的数值。举例来说:“在旧金山的用户中，有多少编辑维基百科的人是男性的?”这个查询基于一个维度值的布尔表达式来过滤表1中的Wikipedia数据集合。在许多真实的数据集中，维度列通常是字符串，而度量列通常是数值。Druid为字符串列创建额外的查询索引，这样就只扫描那些与特定查询筛选相关的行。 让我们考虑前面章节表1中的page列。对于表1中的每个独一无二的page，我们可以创建一些特定页面的标示。我们可以将这些信息存储在一个二进制数组中，其中数组索引表示我们的行。如果某个特定的页面出现在某个行中，那么该数组索引将被标记为1。例如: Justin Bieber -&gt; rows [0, 1] -&gt; [1][1][0][0] Ke$ha -&gt; rows [2, 3] -&gt; [0][0][1][1] Justin Bieber在第0和第1行。这个由列值映射到行索引的映射形成了一个反向索引[39]。要知道是哪条记录包含Justin Bieber和Ke$ha，将两个数组进行OR运算。 这种在大型位图集合上执行布尔操作的方法通常用于搜索引擎。OLAP的位图索引在[32]中详细描述。位图压缩算法常用于搜索领域[2,44,42]以及运行长度编码。Druid 选择使用Concise算法[10]。图7显示了使用整数数组的Concise压缩的字节数。以上结果基于cc2.8xlarge系统，单线程、2G堆内存、512m young内存，以及每次运行之间的强制GC。该数据集是Twitter garden hose一天的数据[41]数据流收集的单日数据。数据集包含2,272,295行和12个不同基数的维度。作为一个额外的比较，我们也采用数据集行来最大化压缩。 在未排序的情况下，总的Concise大小为53,451,144字节，总整数数组大小为127,248,520字节。总的来说，Concise的压缩比整型数组小约42%。在排序的情况下，总Concise压缩大小为43,832,884个字节，总整数数组大小为127,248,520字节。有趣的是，在排序之后，全局压缩只增加了最低限度。 存储引擎Druid的持久化组件允许插入不同的存储引擎，类似Dynamo[12]。这些存储引擎可以将数据存储在一个内存中，例如JVM堆或内存映射结构。swap存储引擎的能力允许根据特定应用程序的规范配置Druid。内存存储引擎可能比内存映射的存储引擎更昂贵，但是如果性能很重要，它可能是更好的选择。默认情况下，使用内存映射存储引擎。 当使用内存映射存储引擎时，Druid依赖于操作系统来分配段的内存。考虑到段只能在内存中进行扫描，内存映射存储引擎允许最近的段保留在内存中，而从不查询的段被移出。使用内存映射存储引擎的主要缺点是，当一个查询需要将更多的段放到内存中，而不是给定节点的容量时。在这种情况下，查询性能将受到段移入和移出内存的成本的影响。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-架构]]></title>
    <url>%2F2018%2F02%2F12%2FDruid-%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[架构体系Druid集群由不同类型的节点组成，每个节点类型被设计用于执行特定的一组事情。我们认为，这种设计将关注点分离，简化了整个系统的复杂性，不同的节点类型之间相互独立，而且它们之间的交互很少。因此，集群内通信故障对数据可用性的影响最小。 为了解决复杂的数据分析问题，不同的节点类型聚在一起形成一个完整的工作系统。Druid这个名字来自于许多角色扮演游戏中的德鲁伊，它是一个变形者，能够在一个群体中扮演各种不同的角色。德鲁伊集群中数据的组成和数据流如图1所示。 Real-time NodesReal-time Nodes的功能是”数据注入”和”查询事件流”。Event通过这些节点创建索引并立即用于查询。这些节点只关心一些小的时间范围内的Event，并且周期性地将它们在这个小时间范围内收集到的”不可变数据集”并传递给Druid集群中的其他节点，这些节点专门处理一些”不可变数据集”。实时节点利用Zookeeper[19]与其他Druid节点进行协调。这些节点宣告它们的在线状态和它们在Zookeeper中服务的数据。 Real-time Nodes为所有注入的事件在内存中维护的索引缓冲区。这些索引随着事件的注入而递增，索引也可以直接查询。Druid的行为像是基于JVM堆内存的行式存储。为了避免堆溢出问题，Real-time Nodes会周期性地或在达到某个最大行限制后将内存索引持久化到磁盘。这个持久化过程将存储在内存缓冲区中的数据转换为第4节中描述的面向列存储格式。每个持久的索引都是不可变的，Real-time Nodes将持久的索引加载到堆外内存中，这样它们仍然可以被查询。这个过程在[33]中被详细描述，如图2所示。 定期地，每个Real-time Node将调度一个后台任务，搜索所有本地持久化的索引。该任务将这些索引合并在一起，并构建一个不可变的数据块，该数据块包含一个实时节点在一定时间内所接收到的所有事件。我们将此数据块称为“segment”。在转换阶段，实时节点将此段上传至永久备份存储，通常是一个分布式文件系统，如S3[12]或HDFS[36]，而Druid将其称为“deep storage”。ingest、persist、merge和handoff步骤都是流畅的;在任何过程中都没有数据丢失。 图3说明了Real-time Node的操作。节点从13:37开始，只接受当前小时或下一个小时的事件。当事件被注入时，该节点宣布它将从13:00到14:00的时间段内服务一个数据段。每10分钟(持久化周期是可配置的)，节点将刷新并将其内存缓冲区保存到磁盘。在临近结束时，节点很可能会在看到14:00到15:00之间的事件。当出现这种情况时，节点准备为下一个小时提供数据，并创建一个新的内存索引。然后，节点宣布它也从14:00到15:00服务一个段。从13:00到14:00，节点不会立即合并持久索引，而是等待从13:00到14:00的离散事件的可配置窗口期。这个窗口期最小化了在事件交付过程中数据丢失的风险。当窗口期结束，节点合并从13:00到14:00的所有索引到一个不变的段(segment)。一旦这个段在其他Druid集群中被加载并可查询时，实时节点将”下架”13:00到14:00的数据。 可用性和可伸缩性Real-time nodes是数据的使用者，需要一个对应的生产者来提供数据流。一般来说，对于数据耐用性的目的，位于生产者和实时节点之间会有一个像Kafka[21]这样的消息总线，如图4所示，实时节点通过从消息总线中读取事件来获取数据，从事件创建到事件消耗的时间通常为数百毫秒。 图4中的消息总线有两个目的。首先，消息总线充当传入事件的缓冲区。像Kafka这样的消息总线维护了位置偏移量——表明了在事件流中，一个消费者(一个实时节点)已经读取了多少。消费者可以通过编程方式更新这些偏移量。实时节点在每次将内存缓冲区保存到磁盘时更新此偏移量。在失败和恢复场景中，如果一个节点没有丢失磁盘，它可以从磁盘重新加载所有持久的索引，并从它所提交的最后偏移量中继续读取事件。从最近的提交点注入事件可以大大减少一个节点的恢复时间。在实践中，我们发现节点在几秒钟内便从这些失败场景中恢复过来。 消息总线的第二个目的是充当单个端点，多个实时节点可以从该端点读取事件。多个实时节点可以从总线上接收相同的事件集合，从而创建事件的复制。在一个场景中，一个节点commit失败并丢失磁盘，复制的流确保没有数据丢失。单个注入端点还允许对数据流进行分区，从而使多个实时节点都能接收到流的一部分。这允许无缝地添加额外的实时节点。在实践中，该模型使最大的Druid生产集群能够以大约500 MB/s(15万Event/s或2 TB/小时)的速度消耗原始数据。 Historical NodesHistorical Nodes功能是加载和服务由实时节点创建的不可变数据块(段)。在许多实际的工作流中，在Druid集群中加载的大多数数据是不可变的，因此，Historical Nodes通常是Druid集群的主要工作人员。Historical Nodes遵循无中心架构，节点之间没有独立的连接点。节点之间没有相互了解，操作上也很简单;它们只知道如何加载、删除和服务“不可变段”。 类似于实时节点，历史节点在Zookeeper声明它们的在线状态和它们服务中的数据。加载和下架段的指令被发送到Zookeeper上，其中包含该segment位于deep storage的位置信息，以及如何解压缩和处理该segment。在历史节点从深度存储中下载特定的段之前，它首先检查一个本地缓存，该缓存维护节点上已经存在的段的信息。如果缓存中不存在某个段的信息，那么历史节点将继续从深度存储中下载该段。这个过程如图5所示。一旦成功完成，该部分将在zookeeper中宣布。此时，该段是可查询的。本地缓存还允许快速更新和重新启动历史节点。在启动时，节点检查它的缓存并立即提供它找到的任何数据。 历史节点可以支持读取一致性，因为它们只处理不可变数据。不可变数据块还支持简单并行化模型:历史节点可以同时扫描和聚合不可变块，而不会阻塞。 Tiers机制历史节点可以分在不同的tiers中，其中给定tiers中的所有节点都是相同配置的。可以为每一tier设置不同的性能和容错参数。节点分层的目的是根据segment的重要性，分配高、低优先级。例如，可以创建一个“hot”级的历史节点，这些节点具有高的内核数和较大的内存容量。可以将“hot”集群配置为更频繁下载访问的数据。一个并行的“cold”集群也可以用不太强大的支持硬件来创建。“cold”集群只包含较少被访问的部分。 可用性历史节点依赖于Zookeeper的段“加载”和“下架”指令。如果Zookeeper变得不可用，那么历史节点就不再能够提供新的数据或删除过时的数据，因为查询是通过HTTP提供的，而历史节点仍然能够响应查询请求，以获取当前服务的数据。这意味着，Zookeeper中断不会影响历史节点上的当前数据可用性。 Broker NodesBroker nodes充当历史和实时节点的查询路由器。Broker节点知晓在Zookeeper中发布的关于哪些段是可查询的以及这些段所在位置的元数据。代理路由传入的查询，这样查询就会命中正确的历史节点或实时节点。Broker节点还将历史和实时节点的部分结果合并，然后将最终合并结果返回给调用者。 缓存Broker nodes包含一个LRU[31, 20]失效策略的缓存。缓存可以使用本地堆内存或外部分布式key/value存储[16]。每当一个broker节点接收到一个查询时，它首先将查询映射到一组segments。某些段的结果可能已经存在于缓存中，不需要重新计算它们。对于缓存中不存在的任何结果，broker节点将向正确的历史和实时节点转发查询。一旦历史节点返回其结果，代理将会将这些结果缓存到一个基础段中以供将来使用。这个过程如图6所示。实时数据永远不会被缓存，因此实时数据的请求将被转发到实时节点。实时数据永远在变化，缓存结果是不可靠的。 缓存还可以额外作为数据可靠性级别。在所有历史节点都失败的情况下，如果缓存中已经存在这些结果，仍然可以查询结果。 可靠性在整个Zookeeper中断的情况下，数据仍然是可查询的。如果broker节点无法与Zookeeper进行通信，它们将使用集群的最后一个memory，并继续将查询转发到实时和历史节点。broker节点假定集群的结构与中断前的结构相同。在实践中，这个可用性模型允许我们的Druid集群在诊断Zookeeper宕机的时候继续为查询服务。 Coordinator NodesDruid Coordinator Nodes（协调节点）主要负责历史节点的数据管理和分配。协调器节点告诉历史节点加载新数据、删除过时数据、复制数据，并将数据移动到负载平衡。Druid使用多版本并发控制交换协议来管理不可变段，以保持稳定的视图。如果任何不可变的segment包含被完全废弃的数据，那么过时的片段就会从集群中删除。协调节点经历了一个领导选举过程，决定运行协调功能的节点。其余的协调节点充当冗余备份。 Coordinator Nodes定期运行以确定集群的当前状态。它通过将集群的期望状态与运行时集群的实际状态进行比较，从而做出决策。与所有Druid节点一样，协调节点维护当前集群信息的Zookeeper连接。协调节点还维护与一个MySQL数据库的连接，该数据库包含额外的操作参数和配置。MySQL数据库中的关键信息之一是包含所有应该由历史节点服务的所有段的列表。这个表可以通过创建段(例如实时节点)的任何服务来更新。MySQL数据库还包含一个规则表，该规则表在集群中管理如何创建、销毁和复制段。 Rules“Rules”管理如何从集群中加载和删除历史片段。”Rules”指示如何将段分配到不同的历史节点tier，以及每个tier中应该存在多少个片段的备份。”Rules”也可能指出何时应该完全从集群中删除段。”Rules”通常设置一段时间。例如，用户可以使用规则将最近的一个月的片段加载到一个“热”集群中，将最近一年的部分划分为“冷”集群，并删除较老的部分。 coordinator nodes从MySQL数据库中的规则表加载一组规则。规则可能是特定于某个数据源的，或者可以配置一个默认的规则集。coordinator nodes将循环遍历所有可用的段，并将每个seg与应用于它的第一条规则相匹配。 Load Balancing在典型的生产环境中，查询常常会命中几十个甚至几百个段。由于每个历史节点都有有限的资源，所以必须在集群之间分配段，以确保集群负载平衡。确定最优的负载分布需要一些关于查询模式和速度的信息。一般情况下，查询涵盖了单个数据源的跨时间间隔的最近段。平均而言，访问较小段的查询速度更快。 这些查询模式建议在更高的速率上复制最近的历史segment，将大量的片段分散到不同的历史节点上，并从不同的数据源中联合定位片段。为了在集群中最优地分配和平衡segments，我们开发了一个基于成本的优化过程，该过程考虑了段数据源、距离和大小。该算法的具体细节超出了本文的范围，可以在以后的文献中讨论。 ReplicationCoordinator nodes可以告诉不同的历史节点加载同一段的副本。历史计算集群的每个tier中复制的数量是完全可配置的。需要高水平容错的设置可以配置为具有大量的副本。备份的段与原始的段相同，并遵循相同的负载分配算法。通过备份段，单个历史节点故障在Druid集群中是透明的。我们使用此属性进行软件升级。我们可以无缝地将一个历史节点脱机，更新它，将其恢复，并对集群中的每个历史节点重复这个过程。在过去的两年里，我们从来没有在我们的Druid集群中进行软件升级。 可靠性Druid协调节点将Zookeeper和MySQL作为外部依赖关系。协调节点依赖于Zookeeper来确定集群中已经存在的历史节点。如果Zookeeper是不可用的，那么协调器将不再能够发送指令来分配、平衡和删除部分。然而，这些操作并不影响数据的可用性。 对MySQL和Zookeeper失败的响应的设计原则是相同的:如果一个负责协作的外部依赖项失败，集群将维持现状。Druid使用MySQL存储操作管理信息和段元数据信息，了解哪些片段应该存在于集群中。如果MySQL宕机，则此信息将无法用于协调节点。然而，这并不意味着数据本身是不可用的。如果协调节点无法与MySQL通信，它们将停止分配新的段并删除过时的部分。在MySQL中断期间，代理、历史和实时节点仍然可以查询。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-背景]]></title>
    <url>%2F2018%2F02%2F12%2FDruid-%E8%83%8C%E6%99%AF%2F</url>
    <content type="text"><![CDATA[提出问题Druid最初的设计目的是解决围绕数据注入和探索大量事务事件(日志数据)的问题。这种timeseries形式的数据通常在OLAP中出现，而数据的性质往往非常复杂。例如，考虑表1所示的数据。表1包含在Wikipedia上发生编辑的数据。每次用户编辑Wikipedia中的页面时，都会生成一个关于”编辑”的元数据的事件。 Timestamp Page Username Gender City Characters Added Characters Removed 2011-01-01T01:00:00Z Justin Bieber Boxer Male San Francisco 1800 25 2011-01-01T01:00:00Z Justin Bieber Reach Male Waterloo 2912 42 2011-01-01T02:00:00Z Ke$ha Helz Male Calgary 1953 17 2011-01-01T02:00:00Z Ke$ha Xeno Male Taiyuan 3194 170 此元数据由3个不同的部分组成。首先，有一个时间戳列，指示何时进行编辑。接下来，有一个维度列，指示编辑器(例如编辑的页面、编辑的用户和用户的位置)的各种属性。最后，有一组度量列包含可以聚合的值(通常是数值)，例如在编辑中添加或删除的字符数。我们的目标是快速地对这些数据的下钻和聚集。我们想回答这样的问题:“在旧金山，贾斯汀·比伯在页面上做了多少编辑?”以及“在一个月的时间里，来自Calgary的人们添加的字符的平均数量是多少?”我们还希望对任意维度的任意组合进行查询，以亚秒级延迟返回。 由于现有的开源关系数据库管理系统(RDBMS)和NoSQL键/值存储无法为交互式应用程序提供低延迟的数据输入和查询平台[40]，因此需要使用Druid。在Metamarkets的早期，我们专注于构建一个托管的仪表板，允许用户任意地探索和可视化事件流。数据存储为仪表板提供了强大的支持，使其能够快速返回查询，使数据可视化能够为用户提供交互式体验。 除了查询延迟需求之外，系统还必须是多租户和高可用的。在高度并发的环境中使用了Metamarkets产品。如果一个系统在软件升级或网络故障的情况下无法使用，停机时间将会非常昂贵，而且许多企业不愿意等待。创业公司通常缺乏适当的内部运营管理，因此，宕机往往决定企业的成败。 最后，Metamarkets在早期面临的另一个挑战是允许用户和警报系统在“实时”中做出业务决策。当”一个事件被创建”到”该事件是可查询”的时候，时延就决定了跨部门的各方能够对其系统中潜在的灾难性情况作出反应。流行的开源数据仓库系统，如Hadoop，无法提供我们需要的亚秒数据注入延迟。 多个行业面临数据挖掘、注入和可用性的问题。自从Druid在2012年10月开源之后，它作为一个视频、网络监控、操作监控和在线广告分析平台在多个公司中部署。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-概述]]></title>
    <url>%2F2018%2F02%2F12%2FDruid-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Druid系列文章为笔者对Druid white paper的翻译文章，如需转载请注明出处。 概述Druid是一个用于大规模数据集的实时分析处理的数据存储系统。该系统结合了列存储、分布式的、无共享的体系结构，高效的索引能够支持10亿行数据分析查询的亚秒级响应。在本文中，我们描述了Druid的架构，以及它如何支持快速聚合，灵活的过滤器，以及低延迟的数据注入。Categories and Subject DescriptorsH.2.4 [Database Management]: Systems—Distributed databases关键字distributed; real-time; fault-tolerant; highly available; open source; analytics; column-oriented; OLAP 简介近年来，互联网技术的迅猛发展使机器生成的事件激增。单独来说，这些事件包含的有用信息非常少，而且价值很低。考虑到从大量数据中提取有价值的数据所需的时间和资源，许多公司愿意放弃这些数据。尽管已经建立了基础设施来处理基于事件的数据(例如IBM的Netezza[37]、HP的Vertica[5]和EMC的Greenplum[29])，但它们在很大程度上以高价出售，而且只针对那些能够支付得起的公司。 几年前，谷歌引入了MapReduce[11]作为其利用商品硬件的机制来索引网络和分析日志。Hadoop[36]项目很快就完成了，并且很大程度上是基于最初的MapReduce理论而形成的。Hadoop目前部署在许多组织中，以存储和分析大量的日志数据。Hadoop帮助公司将其低价值的事件流转化为高价值的聚合，用于各种应用程序，如商业智能和测试。 与许多伟大的系统一样，Hadoop已经打开了我们的视野，也让我们看到新的问题。具体来说，Hadoop擅长存储和提供大量数据，但是它不能保证数据访问的速度有多快。此外，尽管Hadoop是一个高度可用的系统，但是在大量并发负载下性能会降低。最后，尽管Hadoop可以很好地存储数据，但它并没有对数据进行优化，使数据立即可读。 在开发Metamarkets产品的早期，我们遇到了这些问题，并认识到Hadoop是一个很好的后台处理、批处理和数据仓库系统。但是，作为一个在高度一致的租户环境(1000+用户)中具有产品级保证的公司，查询性能和数据可用性方面，Hadoop并不能满足我们的需求。 我们探索了不同的解决方案，在尝试了关系数据库管理系统和NoSQL技术架构之后，我们得出了这样的结论:在开源世界中没有任何东西可以完全满足我们的需求。我们最终创建了Druid，一个开源的、分布式的、基于列的、实时的分析数据存储。在许多方面，Druid与其他OLAP系统(30、35、22)、在交互查询系统[28]、内存数据库[14]以及广为人知的分布式数据存储(7、12、23)有相似之处。在分布式和查询模型上也借鉴了当前的生成搜索基础结构[25,3,4]。本文描述了Druid的架构，探索了在创建系统的过程中所做的各种决策设计，它为托管服务提供了动力，并试图帮助任何一个面临类似问题的人。Druid被部署在几个技术公司的生产环境中中。本文的结构如下:首先第2节中描述Druid解决的问题。接下来，第3节我们将从数据流角度详细介绍系统架构。在第4节中，我们讨论如何以及为什么数据转换成二进制格式。第5节中简要描述了查询API，并在第6节中展示了性能结果。最后，我们将从第7节中运行Druid、第8节中总结经验。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序 / INTRO]]></title>
    <url>%2F2018%2F02%2F10%2F%E5%BA%8F-INTRO%2F</url>
    <content type="text"><![CDATA[我们要有最朴素的生活 与最遥远的梦想即使明日天寒地冻 路远马亡 WELCOME HUBERY’S POLARIS 这里是徐海滨的博客 主要更新一些技术文章、个人作品 请持续关注哟~~~ 比心]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ganglia原理介绍、安装使用]]></title>
    <url>%2F2018%2F02%2F10%2FGanglia%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%E3%80%81%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Ganglia简介：Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。 每台计算机都运行一个收集和发送度量数据的名为 gmond 的守护进程。接收所有度量数据的主机可以显示这些数据并且可以将这些数据的精简表单传递到层次结构中。正因为有这种层次结构模式，才使得 Ganglia 可以实现良好的扩展。gmond 带来的系统负载非常少，这使得它成为在集群中各台计算机上运行的一段代码，而不会影响用户性能。所有这些数据多次收集会影响节点性能。网络中的 “抖动”发生在大量小消息同时出现时，可以通过将节点时钟保持一致，来避免这个问题。 gmetad可以部署在集群内任一台节点或者通过网络连接到集群的独立主机，它通过单播路由的方式与gmond通信，收集区域内节点的状态信息，并以XML数据的形式，保存在数据库中。由RRDTool工具处理数据，并生成相应的的图形显示，以Web方式直观的提供给客户端。 工作原理： Ganglia包括如下几个程序，他们之间通过XDR(xml的压缩格式)或者XML格式传递监控数据，达到监控效果。集群内的节点，通过运行gmond收集发布节点状态信息，然后gmetad周期性的轮询gmond收集到的信息，然后存入rrd数据库，通过web服务器可以对其进行查询展示。 安装ganglia网上示例很多，对该部分的翻译后续再跟进。 配置ganglia默认的配置仅仅能使ganglia工作，如果了解更多的配置项，能帮助你更好的使用ganglia做监控部署。 gmod安装在每个想监控的节点上，与操作系统交互获取度量信息（cpu使用率、内存、网络以及其他可通过自定义拓展的度量）并与集群内其他节点共享。每个在集群内的gmod实例知晓所有gmond所在节点的度量值，并通过XML格式的dump对外提供访问，gmetad通过gmond的连接端口连接。 拓扑结构gmond的默认拓扑结构采用广播的方式(multicast)，意味着所有集群内节点发送并接受度量信息，并以hash table的结构保存到各自的内存数据库中，包括所有集群节点的度量信息。如下图所示： 上图图解最重要的是与gmond deamon全然不同的性质。在内部，gmond发送和接受两种行为是无关联的（正如图中垂直虚线所示）。gmond不会自我交互，它只会向网络中发送度量信息。任何本地节点的信息获取都会经过sender传输到网络中，再由receiver从网络中收集。 这种拓扑对大多数场景适用，但某些案例下，指定少数监听者比每个节点都监听集群内度量信息要更行之有效，因为每个节点都监听会浪费额外的cpu，更多细节详见第三章。 通过“聋哑”模式，如下图，可以消除大集群内的过度联系。聋人模式和哑人模式使一些gmond按特定的模式工作，如收集或者发送。哑人模式意味着节点不对外传输数据，但收集集群内其他节点的度量信息。聋人模式意味着该实例不接受网络中的度量信息，但它不是哑巴，它会持续向网络中的其他监听节点发送度量信息。 并非所有的拓扑模型都要求使用广播。当广播不是第一选择的时候，聋哑模式的拓扑可由UDP单播实现: 或者，可以混合使用“聋哑”模式和默认的拓扑创建适合你环境的系统架构。唯一的拓扑要求如下所述： 在集群内至少有一个gmond实例负责接收网络中的所有度量信息。 gmetad必须定期轮训gmond以拉去集群状态信息。 然而真实实践中，不具备广播连通性的节点不需要配置成”deaf”模式；他们可以使用127.0.0.1地址为自己发送消息，并保留自己的本地度量信息。这可以利用于节点自己发送TCP探针（XML）来排查故障。 2.配置文件可以用以下gmond命令生成默认的配置文件： user@host:$ gmond -t 配置文件由多个部分组成，用花括号包裹，这可以大致分为两个逻辑目录。第一部分包含节点与集群的配置；另一部分包括度量信息的收集和定时策略。 所有的配置属性大小不敏感，如以下例子是相等的名称： name NAME Name NaMe 有些配置部分是可选的；一些配置是必填的。有些可以定义多次，有些只能配置一次。有些配置可以包含子选项配置。 当配置复杂时，有些内部指令可以将gmond.conf分离到多个文件中，并支持类型通配。如： include (&apos;/etc/ganglia/conf.d/*.conf&apos;) 将会命令gmond加载/etc/ganglia/conf.d/路径下所有以.conf为后缀的配置文件。 PS：为了快速开始，你所需要配置的仅仅是cluster部分下的name属性，其他属性可以全部默认。 配置文件使用第三方API工具libconfuse解析，采用libconfuse正常的格式规范。值得说明的是，boolean类型的值可以是yes,true,on；与他们相对的是no,false,off。布尔类型是大小写不敏感的。 有以下八个部分配置节点自身属性: Section: globals.配置守护线程自身的通用属性，配置文件中只能够配置一次。以下是Ganglia3.3.1的默认配置：123456789101112131415globals &#123; daemonize = yes setuid = yes user = nobody debug_level = 0 max_udp_msg_len = 1472 mute = no deaf = no allow_extra_data = yes host_dmax = 86400 /*secs. Expires (removes from web interface) hosts in 1 day */ host_tmax = 20 /*secs */ cleanup_threshold = 300 /*secs */ gexec = no send_metadata_interval = 0 /*secs */&#125; daemonize (boolean)当配置为true时，gmond在后台并行运行。当配置为false时，你可以使用deamontools等守护线程管理工具运行。 setuid (boolean)当设置为true时，gmond以user属性下的有效uid运行，否则不改变有效用户。 debug_level (integer value)当设置为0时，gmond正常运行。等级越高，输出的日志信息越丰富。 max_udp_msg_len (integer value)udp单包的最大长度，不建议修改。 mute (boolean)当设置为true时，gmond不会发送数据，忽略其他的配置指令。只用于收集其他gmond信息，但仍然会响应如gmetad轮询者的请求。 deaf(boolean)当设置为ture时，gmond不会接收数据，忽略其他配置指令。当置于大规模集群中，或者HPC敏感的网格中，CPU消耗成为不得不考虑的因素时，通常节点会配置为deaf模式以确保集群之间的交互降到最低。在这种情境下，部分节点设置为mute专注于收集。如此一来，mute节点的状态信息并不用于集群总状态的评估。它们的作用只是用于收集，所以他们的节点状态会污染集群状态信息。 allow_extra_data (boolean)当设置为false时，gmond不会发送标记为EXTRA_ELEMENT和EXTRA_DATA的XML部分。如果你使用自己的前端平台，这个参数可以有效节省带宽。 host_dmax (integer_value in seconds)代表“delete max”，当设置为0时，gmond永远不会从他们的列表中删除节点，即时某些远程节点丢失报告对量。如果host_dmax 设置为比0大的自然数，gmond会在超过这个时间后刷新host列表。 host_tmax (integer_value in seconds)代表“timeout max”。gmond更新host状态的最大等待时间。因为消息会在网络中丢失，所以gmond会在该超时时间未接收到数据后判定该节点down掉。 cleanup_threshold (integer_value in seconds)gmond清理过期数据的最大时间。 gexec (boolean)当设置为true，gmond允许节点执行gexec job，这个方法要求本节点gexecd已运行并且安装了合适的键。 send_metadata_interval (integer_value in seconds)设置gmond发送或重发包含度量信息的元数据包的时间间隔。默认设置为0，这意味着gmond只有当启动时，或者别的远程节点请求时才会发送数据包。当一个新节点gmond加入集群时，需要通知自己和其他所有节点当前的支持状态。在广播模式下，这不是个问题，但是单播模式下，该时间间隔必须设置，表示两次发送数据的时间间隔。 module_dir (path; optional)指定度量收集模块所在的目录位置。如果忽略，则默认是编译时期的配置项：–with-moduledir。这个配置项，默认是Ganglia目录下的libganglia所在目录，运行如下指令生成默认gmond可发现的配置文件 #gmond -t Section: cluster.每个gmond节点向集群报告信息都通过cluster部分的配置。默认值设置为”unspecified”；默认值是系统可用的，该部分在配置文件中只能配置一次。以下是默认配置：123456cluster &#123; name = &quot;unspecified&quot; owner = &quot;unspecified&quot; latlong = &quot;unspecified&quot; url = &quot;unspecified&quot;&#125; name (text)指定集群名称。当节点轮训拉取xml描述的节点状态信息时，该名称会被插入到CLUSTER部分。gmetad会根据这个值在拉取时归并到不同的RRD文件中存储。该配置项取代了在gmetad.conf中的cluster name配置项。 owner (text)配置集群管理员。 latlong (text)指定集群在地球上的GPS经纬度坐标。 url (text)指定集群的特定URL地址访问信息，如集群目的和使用明细。 Section: host.指定运行该gmond实例的host地址。只有一个配置项：123host &#123; location = &quot;unspecified&quot;&#125; location (text)节点地址，rack,U[,blade]格式也是可用的。 Section: UDP channels.配置gmond节点与其他节点对话的UDP发送/接收渠道。集群通过UDP通道交互，这意味着，所谓集群只是gmond节点直接的发送和接收消息的通道组成。默认情况下，每个gmond节点通过UDP广播向其他节点广播度量信息，其他节点类似。这样很容易启动和维护：每个节点在集群中共享广播地址，并且新增节点自动发现。然而，当我们回顾之前的deaf and mute模式，某些情况下需要单独指定单播地址。由此，每一个gmond的发送和接收频道需要针对当前环境进行配置。每个发送通道的配置定义了一个新的发送自身度量信息的方式，每个接收通道的配置定义了从其他节点接收度量信息的方式。通道可以通过IP4-IP6进行单播或者广播。 记住，一个gmond节点不可向多个集群发送度量信息，也不要试图从其他集群节点接收度量信息。 UDP通道通过udp_(send|receive)_channel部分创建。默认发送通道如下：123456udp_send_channel &#123; #bind_hostname = yes mcast_join = 239.2.11.71 port = 8649 ttl = 1&#125; bind_hostname (boolean; optional, for multicast or unicast)配置是否通过机器名绑定。 mcast_join (IP; optional, for multicast only)当指定时，gmond将会通过创建udp连接并且加入广播组，该配置创建广播渠道并与host配置二选一。 mcast_if (text; optional, for multicast only)指定时，gmond通过指定接口发送数据。如：eth0 host (text or IP; optional, for unicast only)指定发送数据地址，与mcast_join二选一。 port (number; optional, for multicast and unicast)gmond发送数据使用端口，默认8649 ttl (number; optional, for multicast or unicast)time-to-live存活时间，该配置项对广播环境 尤其重要，用于限制度量信息的有效时间，越高的值，则容忍性越大。 如下是默认的接收通道配置：12345udp_recv_channel &#123; mcast_join = 239.2.11.71 port = 8649 bind = 239.2.11.71&#125; mcast_join (IP; optional, for multicast only)当指定时，gmond将从该IP所在的广播群组中接收广播信息，如果不指定广播属性，gmond将会通过特定端口创建UDP单播服务。 mcast_if (text; optional, for multicast only)同上； bind (IP; optional, for multicast or unicast)指定后，gmond将会和本地地址绑定。 port (number; optional, for multicast or unicast)接收端口，默认8649 family (inet4|inet6; optional, for multicast or unicast)ip版本，默认inet4。如果想监听ipv4和ipv6两个网络，需要配置两个接收渠道。 acl (ACL definition; optional, for multicast or unicast)access control list：细粒度的接受渠道控制。详见“Access control”章节。 Section: TCP Accept Channels.gmond与gmetad或者其它轮询者交互通过TCP通道。可选如下配置项，默认如下：123456789tcp_accept_channel &#123; port = 8649&#125;bind (IP; optional)port (number)family (inet4|inet6; optional)interface (text; optional)acl (ACL definition; optional) Access control. 即acl，udp_recv_channel和tcp_accept_channel 的配置项。这个配置可以指定具体地址或者地址范围来添加gmond的连接许可。如下是个ACL示例：12345678910111213acl &#123; default = &quot;deny&quot; access &#123; ip = 192.168.0.0 mask = 24 action = &quot;allow&quot; &#125;access &#123; ip = ::ff:1.2.3.0 mask = 120 action = &quot;deny&quot; &#125;&#125; 配置遵从第一优先级。mask可指定路由范围。 Optional section: sFlow. sFlow是产品级的管理高速网络交换的技术。起初设想嵌入网络硬件内，现在存在于操作系统级别，如同其他应用一样如tomcat等web容器。gmond可以配置为成为sFlow的收集器，打包sFlow的数据包并发送给gmetad。在第八章中详细介绍。该配置全部可选，以下是默认配置：12345678910#sflow &#123;# udp_port = 6343# accept_vm_metrics = yes# accept_jvm_metrics = yes# multiple_jvm_instances = no# accept_http_metrics = yes# multiple_http_instances = no# accept_memcache_metrics = yes# multiple_memcache_instances = no#&#125; udp_port (number; optional)用于接收sFlow数据的端口。 Section: modules.该配置包含了加载度量模块的必须参数。gmond可收集所有动态加载的可扩展的度量模块。详见第五章。每个模块至少包含一个module子目录。该子目录由5个属性组成。默认配置包括所有已安装的度量插件，所以除非你有新增的度量插件，不然无需更改。示例如下：123456789101112131415modules &#123; module &#123; name = &quot;example_module&quot; language = &quot;C/C++&quot; enabled = yes path = &quot;modexample.so&quot; params = &quot;An extra raw parameter&quot; param RandomMax &#123; value = 75 &#125; param ConstantValue &#123; value = 25 &#125; &#125;&#125; name (text)如果使用c/c++实现，则该参数由模块结构决定。如果使用phthon等解释性语言编写，则由源文件决定。 language (text; optional)文件源码的实现语言，默认是c/c++，目前只支持c/c++或者python。 enabled (boolean; optional)方便该度量插件的启停。默认为yes； path (text)gmond加载度量插件的路径(c/c++动态加载)，如果不是绝对路径，则在前补上globals模块下的module_path属性。 params (text; optional)加载插件时的string参数。 Section: collection_group.该目录指定gmond收集那些度量信息，以及收集和广播的频率。你可以 尽可能多的将待收集的度量信息分组。每个分组至少包含一个 metric 模块。这是根据采样间隔做的逻辑分组。在gmond.conf下的不会影响web下的分组结果，默认配置如下：1234567891011121314151617181920212223collection_group &#123; collect_once = yes time_threshold = 1200 metric &#123; name = &quot;cpu_num&quot; title = &quot;CPU Count&quot; &#125; &#125;collection_group &#123; collect_every = 20 time_threshold = 90 /* CPU status */ metric &#123; name = &quot;cpu_user&quot; value_threshold = &quot;1.0&quot; title = &quot;CPU User&quot; &#125; metric &#123; name = &quot;cpu_system&quot; value_threshold = &quot;1.0&quot; title = &quot;CPU System&quot; &#125;&#125; collect_once (boolean)某些度量信息除非重启否则不会改变，如操作系统类型，cpu核数等。这些参数只需要启动时采集一次即可。该参数和collect_every互斥。 collect_every (seconds)频繁轮训采集时间，如cpu_user，cpu_system每20秒采集一次。 time_threshold (seconds)最大等待时间，gmond发送collection_group数据到所有udp_send_channels的时间。 name (text)度量信息在度量模块中的名称。典型的，每个度量模块定义多个度量信息名称，一个可选的name可以是name_match，如果使用name_match代替name，可以匹配多个度量名称，例如：namematch = “multicpu([a-z]+)([0-9]+)” value_threshold (number)每次收集度量信息，新值会和上一次的值进行比对。如果发生变化并且当前值大于配置值，则整个收集群组会发送到udp_send_channels定义的通道内。 title (text)一个用于web展示的用户友好的度量标题。 gmetadgmetad,the Ganlia Meta Daemon，安装在运行了收集度量信息的gmond节点之上，负责度量信息的收集和聚合。默认情况下，gmetad收集并聚合度量信息存储到RRD文件，但可以配置gmetad向其他系统汇总数据，如Graphite。 gmetad监听tcp端口8651，连接远程的gmetad并提供授权节点的xml dump状态文件。通过8652tcp端口响应其他节点的请求。交互的设备接受简易的子树和xml网格状态的总览。gweb通过使用这些查询在展现不适合存储在RRD中的数据，比如操作系统版本信息。 gmetad拓扑一个最简单的拓扑结构如下图所示，只存在一个gmetad负责轮训多个gmond实例: 高可用性是通常的需求也比较容易实现。如下图所示，两个gmetad和多个gmond实例。gmetad如果从node1拉取不到，将会从node2拉取。两个gmetad也会同时工作: gmetad并未限制轮训gmond，也可以拉取gmetad以创造另一个gmetad层级。如下图： 在更大的集群中，IO成为性能瓶颈，rrdcached作为gmetad和RRD文件的中间缓存，如图: gmetad.confgmetad.conf配置文件由单行属性和相对应的值组成。名称大小写不敏感，但value不同。如下所示属性表示的是相同的名称： name NAME Name NaMe 大多数属性是可选的；另外的是必须的。有些可被定义多次，有些只能被定义一次。 The data_source attribute. data_source是gmetad的核心配置。每一行data_source描述一个gmond集群或者一个由gmetad负责收集和聚合的网络。gmetad可以自行区分是一个cluster还是一个由gmetad主导的网格，所以data_source对二者都是相等的。如果gmetad发现data_source指向一个cluster ，它从data_source将维持完成的轮训列表。否则，gmetad会认为data_source指向网格，它只保存RRD的相关概要信息。 设置scalable属性为false，会强制gmetad保持完整的RRD文件集合以用于网格数据源。 以默认配置文件为例： data_source &quot;my cluster&quot; 10 localhost my.machine.edu:8649 1.2.3.5:8655 data_source &quot;my grid&quot; 50 1.3.4.7:8655 grid.org:8651 grid-backup.org:8651 data_source &quot;another source&quot; 1.3.4.8:8655 1.3.4.8 每个data_source由三部分构成。第一部分唯一标识该数据源。第二部分指定轮训时间间隔，单位是秒。第二部分表示gmetad轮训数据的空格分割的host地址列表，可以使用IP或者DNS识别的域名。最后一部分表示tcp端口，默认8649。 gmetad将按顺序检查列表中每个Host，带着第一个节点的状态信息做出响应。所以没必要在data_source中列举集群内所有节点。两三个既能保证数据不会出错。 gmetad daemon behavior. 属性列举如下： gridname (text) 字符串类型，唯一标识一个gmetad网格。这个字符串不能与gmond集合中的名称冲突。在gmond.conf（cluster中的配置项{name=”XXX”}）用于表示哪些特定的gmond实例负责收集。而gridname属性将会用Grid标签包裹数据源的数据。可以定义为data_source的收集者。 authority (URL)网格有效的URL，用于其他gmetad实例访问当前数据源的图表信息，默认地址为：http://hostname/ganglia/ trusted_hosts (text)gmetad的信任地址，localhost是用被信任的，空格分割。 all_trusted (on|off)设置为on则重写trusted_hosts配置，任何节点都被信任。 setuid_username (UID)gmetad使用的用户id，默认是nobody。 setuid (on|off)是否禁用uid。 xml_port (number)gmetad的监听端口，默认8651. interactive_port (number)gmetad交互端口，默认8652。与上个配置项对应。 server_threads (number)连接到监听端口的最大连接数。默认为4。 case_sensitive_hostnames (1|0)在早期的版本中，RRD文件使用大小写敏感的hostname创建，但如今已不在使用。3.2版本后默认为0。 RRDtool attributes. 配置RRD文件的创建和处理。 RRAs (text)These specify custom Round Robin Archive values. The default is (with a “stepsize” of 15 seconds): “RRA:AVERAGE:0.5:1:5856” “RRA:AVERAGE:0.5:4:20160” “RRA:AVERAGE:0.5:40:52704” The full details of an RRA specification are contained in the manpage forrrdcreate(1). umask (number)Specifies the umask to apply to created RRD files and the directory structure containing them. It defaults to 022. rrd_rootdir (path)Specifies the Graphite support. 可以将gmetad收集的度量数据全部导出到Graphite，一个第三方的开源的度量信息存储和可视化展示工具，配置参数如下： carbon_server (address)hostname或者ip，远程的daemon地址。 carbon_port (number)远程端口号，默认2003。 graphite_prefix (text)Graphite使用点号分割的路径组织和引用度量信息，所以更合适写一个前缀来表示描述度量信息。如datacenter1.gmetad carbon_timeout (number)gmetad等待Graphite的响应时间，单位毫秒。这个设置十分重要，因为gmetad的sender不是线程的并且会产生阻塞。默认500。 gmetad interactive port query syntax. 正如前面所提，gmetad监听tcp端口8652用于响应请求信息。这个请求基础的功能是获取grid中他们感兴趣的xml类型的dump状态信息。 通过文本协议，如SMTP和HTTP。请求有层级结构，以（/）斜线开始。比如如下请求会返回所有度量信息： / 如果要更明确请求内容，可以指定集群名称，如： /cluster1 如果要更明确节点内容，可以指定节点名称，如： /cluster1/host1 请求也可以通过指定后缀参数设置过滤器来修正度量值，如你可以获取cluster1的概要信息： /cluster1?filter=summary gweb相比ganglia的三个部分而言，gweb是最需要配置的组件。实际上，你不需要改变任何参数，gweb即可运行完整功能的web UI。 Apache virtual host configuration 尽管gweb本身不需要配置，但一些web容器想运行gweb需要作出部分配置。每个需要支持PHP的web容器需要做以下工作，还有许多是本书中未涉及的web配置参数。Apache Web Server是常用的web容器。假设gweb安装在/var/www/html/ganglia2，域名为 myganglia.example.org，则配置如下：1234567NameVirtualHost *.80&lt;VirtualHost *:80&gt;ServerName myganglia.example.orgServerAlias mygangliaDocumentRoot /var/www/html/ganglia2# Other directives here&lt;/VirtualHost&gt; 这只是一个简单的例子，更多详见：http://httpd.apache.org/docs/2.0/vhosts/ gweb options gweb的配置通过conf.php文件。实际上这个文件覆盖了默认文件conf_default.php。该文件在web目录的根目录下。该文档已编写完善，超过80个配置项不需要一一配置，只需使用时修改其中重要的几个即可。 该文件，正如名称所讲，是由许多参数组成的PHP脚本，不像其他的配置文件，参数有多行组成。这些属性名都是以$conf这种gweb格式组成，大小写敏感，看起来像PHP数组。如下参数表示gweb使用的RRDtool所在目录： $conf[&apos;rrdtool&apos;] = &quot;/usr/bin/rrdtool&quot;; 所有的参数都是必须的，也有一些被定义多次，也有被定义一次，也有引用其他的变量值，如： $conf[&apos;rrds&apos;] = &quot;${conf[&apos;gmetad_root&apos;]}/rrds&quot;; Application settings.该目录下的参数影响gweb的基础功能，它自己的家目录，比如，RRDs或者templates。这些很少被用户修改但偶尔会被提及。 templates (path)指定gweb的templates路径。就像一个站点的皮肤样式。 graphdir (path)指定定义图表的json文件所在路径。如下章所讲，用户会用json自定义图表样式，并将其存放在该目录下，用于web的UI展示。 rrds (path)指定RRD文件的所在目录。 又如第七章所提到的，不同的Nagios的特性在gweb的conf.php文件中集成。所以Nagios可以通过请求gweb获取度量信息，而不是Nagios Service Check Acceptor(NSCA)和Nagios Remote Plugin Executor (NRPE)。 Look and feel.gweb可以配置一次显示的图表数量(max_graphs)，也可以用来指定列数量和host视图。也有一些布尔类型的配置影响UI的默认行为，如 metric_groups_initially_collapsed。 config.php文件定义了大量样式，以及自定义时间范围。 Security.该属性参数如下： auth_system (readonly|enabled|disabled) gweb做了简单的安全认证机制，允许或禁止个别用户对部分应用功能的访问。如果设置为enabled则为可用。 Advanced features.参数如下：rrdcached_socket (path)指定rddcached的socket地址，用于高并发下的缓存策略。 graph_engine (rrdtool|graphite)gweb可以使用Graphite代替RRDtool作为图形引擎。该配置要求你已安装Graphite和Graphite webapp在该服务器上。详见：sourceforge.net/apps/trac/ganglia/wiki/ganglia-web-2#UsingGraphiteasthegraphingengine 部署运行到此为止，ganglia已经安装配置完成，是时候运行一下它们了！验证它们的基础功能是否完善并确保它们之间的通讯完成。 Starting Up the Processes 虽然没必要保证启动的先后顺序，如果按此处推荐顺序启动，能避免对元数据转换成udp聚合包的等待延迟，并且用户不会再web端看到错误数据。 如果你使用UDP单播模式，先启动UDP的收集节点。这样能确保该收集器能收集到各节点第一次发送的元数据。 启动其他的gmond实例。 如果你使用了rddcached，启动它。 从最低的层级开始启动gmetad。 启动其他层级的gmetad。 启动apache web server，比gmetad后启动，如果PHP脚本不能连接gmetad，可通过端口8652监控。 Testing Your Installationgmond和gmetad都监听tcp端口，为了测试gmond，使用telnet： user@host:$ telnet localhost 8649 作为回复，gmond会输出一段xml格式的dump信息，包含其度量信息。如果gmond是deaf模式或者mute模式，会返回一个空的xml文档，仅仅含有cluster标签。测试gmetad可以使用 user@host:$ telnet localhost 8651 一个有效的gmetad会返回一段xml格式的dump度量信息。详见第六章，了解更多验证程序状态的方法。 Firewalls防火墙问题是初装ganglia最普遍的问题，我们整理问题如下： gmond默认使用广播模式，所以跨越子网的集群需要配置单播的发送者和监听者，在之前的拓扑章节有介绍。如果gmond必须穿过防火墙与其他节点交互，允许端口udp/8649。对于广播模式，路由和防火墙必须支持IGMP协议。 gmond从端口TCP 8649监听gmetad连接。如果gmetad必须穿过防火墙，则保证其gmond的tcp 8649端口畅通。 gmetad使用tcp 8651，8652。前者类似于gmond的8649端口，后者作为“交互查询端口”用于响应指定的查询请求。被gweb使用，且通常与gmetad安装在同一台机器，所以除非你有使用更多的集成特性，如Nagios集成，或者有自定义的查询gmetad的脚本，不然不需要有防火墙的ACLs。 gweb运行在web容器中，通常监听80或者443端口(如果你使用SSL，则443)。如果web服务器被防火墙隔离，那么开放Tcp80和tcp443端口。 如果ganglia集成了sFlow收集器，并且每个sFlow收集器都要与gmond交互 ，为gmond的监听开放udp 6343端口。]]></content>
      <categories>
        <category>Ganglia</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>Ganglia</tag>
      </tags>
  </entry>
</search>

<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Druid-架构]]></title>
    <url>%2F2018%2F02%2F12%2FDruid-%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[架构体系Druid集群由不同类型的节点组成，每个节点类型被设计用于执行特定的一组事情。我们认为，这种设计将关注点分离，简化了整个系统的复杂性，不同的节点类型之间相互独立，而且它们之间的交互很少。因此，集群内通信故障对数据可用性的影响最小。 为了解决复杂的数据分析问题，不同的节点类型聚在一起形成一个完整的工作系统。Druid这个名字来自于许多角色扮演游戏中的德鲁伊，它是一个变形者，能够在一个群体中扮演各种不同的角色。德鲁伊集群中数据的组成和数据流如图1所示。 Real-time NodesReal-time Nodes的功能是”数据注入”和”查询事件流”。Event通过这些节点创建索引并立即用于查询。这些节点只关心一些小的时间范围内的Event，并且周期性地将它们在这个小时间范围内收集到的”不可变数据集”并传递给Druid集群中的其他节点，这些节点专门处理一些”不可变数据集”。实时节点利用Zookeeper[19]与其他Druid节点进行协调。这些节点宣告它们的在线状态和它们在Zookeeper中服务的数据。 Real-time Nodes为所有注入的事件在内存中维护的索引缓冲区。这些索引随着事件的注入而递增，索引也可以直接查询。Druid的行为像是基于JVM堆内存的行式存储。为了避免堆溢出问题，Real-time Nodes会周期性地或在达到某个最大行限制后将内存索引持久化到磁盘。这个持久化过程将存储在内存缓冲区中的数据转换为第4节中描述的面向列存储格式。每个持久的索引都是不可变的，Real-time Nodes将持久的索引加载到堆外内存中，这样它们仍然可以被查询。这个过程在[33]中被详细描述，如图2所示。 定期地，每个Real-time Node将调度一个后台任务，搜索所有本地持久化的索引。该任务将这些索引合并在一起，并构建一个不可变的数据块，该数据块包含一个实时节点在一定时间内所接收到的所有事件。我们将此数据块称为“segment”。在转换阶段，实时节点将此段上传至永久备份存储，通常是一个分布式文件系统，如S3[12]或HDFS[36]，而Druid将其称为“deep storage”。ingest、persist、merge和handoff步骤都是流畅的;在任何过程中都没有数据丢失。 图3说明了Real-time Node的操作。节点从13:37开始，只接受当前小时或下一个小时的事件。当事件被注入时，该节点宣布它将从13:00到14:00的时间段内服务一个数据段。每10分钟(持久化周期是可配置的)，节点将刷新并将其内存缓冲区保存到磁盘。在临近结束时，节点很可能会在看到14:00到15:00之间的事件。当出现这种情况时，节点准备为下一个小时提供数据，并创建一个新的内存索引。然后，节点宣布它也从14:00到15:00服务一个段。从13:00到14:00，节点不会立即合并持久索引，而是等待从13:00到14:00的离散事件的可配置窗口期。这个窗口期最小化了在事件交付过程中数据丢失的风险。当窗口期结束，节点合并从13:00到14:00的所有索引到一个不变的段(segment)。一旦这个段在其他Druid集群中被加载并可查询时，实时节点将”下架”13:00到14:00的数据。 可用性和可伸缩性Real-time nodes是数据的使用者，需要一个对应的生产者来提供数据流。一般来说，对于数据耐用性的目的，位于生产者和实时节点之间会有一个像Kafka[21]这样的消息总线，如图4所示，实时节点通过从消息总线中读取事件来获取数据，从事件创建到事件消耗的时间通常为数百毫秒。 图4中的消息总线有两个目的。首先，消息总线充当传入事件的缓冲区。像Kafka这样的消息总线维护了位置偏移量——表明了在事件流中，一个消费者(一个实时节点)已经读取了多少。消费者可以通过编程方式更新这些偏移量。实时节点在每次将内存缓冲区保存到磁盘时更新此偏移量。在失败和恢复场景中，如果一个节点没有丢失磁盘，它可以从磁盘重新加载所有持久的索引，并从它所提交的最后偏移量中继续读取事件。从最近的提交点注入事件可以大大减少一个节点的恢复时间。在实践中，我们发现节点在几秒钟内便从这些失败场景中恢复过来。 消息总线的第二个目的是充当单个端点，多个实时节点可以从该端点读取事件。多个实时节点可以从总线上接收相同的事件集合，从而创建事件的复制。在一个场景中，一个节点commit失败并丢失磁盘，复制的流确保没有数据丢失。单个注入端点还允许对数据流进行分区，从而使多个实时节点都能接收到流的一部分。这允许无缝地添加额外的实时节点。在实践中，该模型使最大的Druid生产集群能够以大约500 MB/s(15万Event/s或2 TB/小时)的速度消耗原始数据。 Historical NodesHistorical Nodes功能是加载和服务由实时节点创建的不可变数据块(段)。在许多实际的工作流中，在Druid集群中加载的大多数数据是不可变的，因此，Historical Nodes通常是Druid集群的主要工作人员。Historical Nodes遵循无中心架构，节点之间没有独立的连接点。节点之间没有相互了解，操作上也很简单;它们只知道如何加载、删除和服务“不可变段”。 类似于实时节点，历史节点在Zookeeper声明它们的在线状态和它们服务中的数据。加载和下架段的指令被发送到Zookeeper上，其中包含该segment位于deep storage的位置信息，以及如何解压缩和处理该segment。在历史节点从深度存储中下载特定的段之前，它首先检查一个本地缓存，该缓存维护节点上已经存在的段的信息。如果缓存中不存在某个段的信息，那么历史节点将继续从深度存储中下载该段。这个过程如图5所示。一旦成功完成，该部分将在zookeeper中宣布。此时，该段是可查询的。本地缓存还允许快速更新和重新启动历史节点。在启动时，节点检查它的缓存并立即提供它找到的任何数据。 历史节点可以支持读取一致性，因为它们只处理不可变数据。不可变数据块还支持简单并行化模型:历史节点可以同时扫描和聚合不可变块，而不会阻塞。 Tiers机制历史节点可以分在不同的tiers中，其中给定tiers中的所有节点都是相同配置的。可以为每一tier设置不同的性能和容错参数。节点分层的目的是根据segment的重要性，分配高、低优先级。例如，可以创建一个“hot”级的历史节点，这些节点具有高的内核数和较大的内存容量。可以将“hot”集群配置为更频繁下载访问的数据。一个并行的“cold”集群也可以用不太强大的支持硬件来创建。“cold”集群只包含较少被访问的部分。 可用性历史节点依赖于Zookeeper的段“加载”和“下架”指令。如果Zookeeper变得不可用，那么历史节点就不再能够提供新的数据或删除过时的数据，因为查询是通过HTTP提供的，而历史节点仍然能够响应查询请求，以获取当前服务的数据。这意味着，Zookeeper中断不会影响历史节点上的当前数据可用性。 Broker NodesBroker nodes充当历史和实时节点的查询路由器。Broker节点知晓在Zookeeper中发布的关于哪些段是可查询的以及这些段所在位置的元数据。代理路由传入的查询，这样查询就会命中正确的历史节点或实时节点。Broker节点还将历史和实时节点的部分结果合并，然后将最终合并结果返回给调用者。 缓存Broker nodes包含一个LRU[31, 20]失效策略的缓存。缓存可以使用本地堆内存或外部分布式key/value存储[16]。每当一个broker节点接收到一个查询时，它首先将查询映射到一组segments。某些段的结果可能已经存在于缓存中，不需要重新计算它们。对于缓存中不存在的任何结果，broker节点将向正确的历史和实时节点转发查询。一旦历史节点返回其结果，代理将会将这些结果缓存到一个基础段中以供将来使用。这个过程如图6所示。实时数据永远不会被缓存，因此实时数据的请求将被转发到实时节点。实时数据永远在变化，缓存结果是不可靠的。 缓存还可以额外作为数据可靠性级别。在所有历史节点都失败的情况下，如果缓存中已经存在这些结果，仍然可以查询结果。 可靠性在整个Zookeeper中断的情况下，数据仍然是可查询的。如果broker节点无法与Zookeeper进行通信，它们将使用集群的最后一个memory，并继续将查询转发到实时和历史节点。broker节点假定集群的结构与中断前的结构相同。在实践中，这个可用性模型允许我们的Druid集群在诊断Zookeeper宕机的时候继续为查询服务。 Coordinator NodesDruid Coordinator Nodes（协调节点）主要负责历史节点的数据管理和分配。协调器节点告诉历史节点加载新数据、删除过时数据、复制数据，并将数据移动到负载平衡。Druid使用多版本并发控制交换协议来管理不可变段，以保持稳定的视图。如果任何不可变的segment包含被完全废弃的数据，那么过时的片段就会从集群中删除。协调节点经历了一个领导选举过程，决定运行协调功能的节点。其余的协调节点充当冗余备份。 Coordinator Nodes定期运行以确定集群的当前状态。它通过将集群的期望状态与运行时集群的实际状态进行比较，从而做出决策。与所有Druid节点一样，协调节点维护当前集群信息的Zookeeper连接。协调节点还维护与一个MySQL数据库的连接，该数据库包含额外的操作参数和配置。MySQL数据库中的关键信息之一是包含所有应该由历史节点服务的所有段的列表。这个表可以通过创建段(例如实时节点)的任何服务来更新。MySQL数据库还包含一个规则表，该规则表在集群中管理如何创建、销毁和复制段。 Rules“Rules”管理如何从集群中加载和删除历史片段。”Rules”指示如何将段分配到不同的历史节点tier，以及每个tier中应该存在多少个片段的备份。”Rules”也可能指出何时应该完全从集群中删除段。”Rules”通常设置一段时间。例如，用户可以使用规则将最近的一个月的片段加载到一个“热”集群中，将最近一年的部分划分为“冷”集群，并删除较老的部分。 coordinator nodes从MySQL数据库中的规则表加载一组规则。规则可能是特定于某个数据源的，或者可以配置一个默认的规则集。coordinator nodes将循环遍历所有可用的段，并将每个seg与应用于它的第一条规则相匹配。 Load Balancing在典型的生产环境中，查询常常会命中几十个甚至几百个段。由于每个历史节点都有有限的资源，所以必须在集群之间分配段，以确保集群负载平衡。确定最优的负载分布需要一些关于查询模式和速度的信息。一般情况下，查询涵盖了单个数据源的跨时间间隔的最近段。平均而言，访问较小段的查询速度更快。 这些查询模式建议在更高的速率上复制最近的历史segment，将大量的片段分散到不同的历史节点上，并从不同的数据源中联合定位片段。为了在集群中最优地分配和平衡segments，我们开发了一个基于成本的优化过程，该过程考虑了段数据源、距离和大小。该算法的具体细节超出了本文的范围，可以在以后的文献中讨论。 ReplicationCoordinator nodes可以告诉不同的历史节点加载同一段的副本。历史计算集群的每个tier中复制的数量是完全可配置的。需要高水平容错的设置可以配置为具有大量的副本。备份的段与原始的段相同，并遵循相同的负载分配算法。通过备份段，单个历史节点故障在Druid集群中是透明的。我们使用此属性进行软件升级。我们可以无缝地将一个历史节点脱机，更新它，将其恢复，并对集群中的每个历史节点重复这个过程。在过去的两年里，我们从来没有在我们的Druid集群中进行软件升级。 可靠性Druid协调节点将Zookeeper和MySQL作为外部依赖关系。协调节点依赖于Zookeeper来确定集群中已经存在的历史节点。如果Zookeeper是不可用的，那么协调器将不再能够发送指令来分配、平衡和删除部分。然而，这些操作并不影响数据的可用性。 对MySQL和Zookeeper失败的响应的设计原则是相同的:如果一个负责协作的外部依赖项失败，集群将维持现状。Druid使用MySQL存储操作管理信息和段元数据信息，了解哪些片段应该存在于集群中。如果MySQL宕机，则此信息将无法用于协调节点。然而，这并不意味着数据本身是不可用的。如果协调节点无法与MySQL通信，它们将停止分配新的段并删除过时的部分。在MySQL中断期间，代理、历史和实时节点仍然可以查询。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-背景]]></title>
    <url>%2F2018%2F02%2F12%2FDruid-%E8%83%8C%E6%99%AF%2F</url>
    <content type="text"><![CDATA[提出问题Druid最初的设计目的是解决围绕数据注入和探索大量事务事件(日志数据)的问题。这种timeseries形式的数据通常在OLAP中出现，而数据的性质往往非常复杂。例如，考虑表1所示的数据。表1包含在Wikipedia上发生编辑的数据。每次用户编辑Wikipedia中的页面时，都会生成一个关于”编辑”的元数据的事件。 Timestamp Page Username Gender City Characters Added Characters Removed 2011-01-01T01:00:00Z Justin Bieber Boxer Male San Francisco 1800 25 2011-01-01T01:00:00Z Justin Bieber Reach Male Waterloo 2912 42 2011-01-01T02:00:00Z Ke$ha Helz Male Calgary 1953 17 2011-01-01T02:00:00Z Ke$ha Xeno Male Taiyuan 3194 170 此元数据由3个不同的部分组成。首先，有一个时间戳列，指示何时进行编辑。接下来，有一个维度列，指示编辑器(例如编辑的页面、编辑的用户和用户的位置)的各种属性。最后，有一组度量列包含可以聚合的值(通常是数值)，例如在编辑中添加或删除的字符数。我们的目标是快速地对这些数据的下钻和聚集。我们想回答这样的问题:“在旧金山，贾斯汀·比伯在页面上做了多少编辑?”以及“在一个月的时间里，来自Calgary的人们添加的字符的平均数量是多少?”我们还希望对任意维度的任意组合进行查询，以亚秒级延迟返回。 由于现有的开源关系数据库管理系统(RDBMS)和NoSQL键/值存储无法为交互式应用程序提供低延迟的数据输入和查询平台[40]，因此需要使用Druid。在Metamarkets的早期，我们专注于构建一个托管的仪表板，允许用户任意地探索和可视化事件流。数据存储为仪表板提供了强大的支持，使其能够快速返回查询，使数据可视化能够为用户提供交互式体验。 除了查询延迟需求之外，系统还必须是多租户和高可用的。在高度并发的环境中使用了Metamarkets产品。如果一个系统在软件升级或网络故障的情况下无法使用，停机时间将会非常昂贵，而且许多企业不愿意等待。创业公司通常缺乏适当的内部运营管理，因此，宕机往往决定企业的成败。 最后，Metamarkets在早期面临的另一个挑战是允许用户和警报系统在“实时”中做出业务决策。当”一个事件被创建”到”该事件是可查询”的时候，时延就决定了跨部门的各方能够对其系统中潜在的灾难性情况作出反应。流行的开源数据仓库系统，如Hadoop，无法提供我们需要的亚秒数据注入延迟。 多个行业面临数据挖掘、注入和可用性的问题。自从Druid在2012年10月开源之后，它作为一个视频、网络监控、操作监控和在线广告分析平台在多个公司中部署。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-概述]]></title>
    <url>%2F2018%2F02%2F12%2FDruid-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[概述Druid是一个用于大规模数据集的实时分析处理的数据存储系统。该系统结合了列存储、分布式的、无共享的体系结构，高效的索引能够支持10亿行数据分析查询的亚秒级响应。在本文中，我们描述了Druid的架构，以及它如何支持快速聚合，灵活的过滤器，以及低延迟的数据注入。Categories and Subject DescriptorsH.2.4 [Database Management]: Systems—Distributed databases关键字distributed; real-time; fault-tolerant; highly available; open source; analytics; column-oriented; OLAP 简介近年来，互联网技术的迅猛发展使机器生成的事件激增。单独来说，这些事件包含的有用信息非常少，而且价值很低。考虑到从大量数据中提取有价值的数据所需的时间和资源，许多公司愿意放弃这些数据。尽管已经建立了基础设施来处理基于事件的数据(例如IBM的Netezza[37]、HP的Vertica[5]和EMC的Greenplum[29])，但它们在很大程度上以高价出售，而且只针对那些能够支付得起的公司。 几年前，谷歌引入了MapReduce[11]作为其利用商品硬件的机制来索引网络和分析日志。Hadoop[36]项目很快就完成了，并且很大程度上是基于最初的MapReduce理论而形成的。Hadoop目前部署在许多组织中，以存储和分析大量的日志数据。Hadoop帮助公司将其低价值的事件流转化为高价值的聚合，用于各种应用程序，如商业智能和测试。 与许多伟大的系统一样，Hadoop已经打开了我们的视野，也让我们看到新的问题。具体来说，Hadoop擅长存储和提供大量数据，但是它不能保证数据访问的速度有多快。此外，尽管Hadoop是一个高度可用的系统，但是在大量并发负载下性能会降低。最后，尽管Hadoop可以很好地存储数据，但它并没有对数据进行优化，使数据立即可读。 在开发Metamarkets产品的早期，我们遇到了这些问题，并认识到Hadoop是一个很好的后台处理、批处理和数据仓库系统。但是，作为一个在高度一致的租户环境(1000+用户)中具有产品级保证的公司，查询性能和数据可用性方面，Hadoop并不能满足我们的需求。 我们探索了不同的解决方案，在尝试了关系数据库管理系统和NoSQL技术架构之后，我们得出了这样的结论:在开源世界中没有任何东西可以完全满足我们的需求。我们最终创建了Druid，一个开源的、分布式的、基于列的、实时的分析数据存储。在许多方面，Druid与其他OLAP系统(30、35、22)、在交互查询系统[28]、内存数据库[14]以及广为人知的分布式数据存储(7、12、23)有相似之处。在分布式和查询模型上也借鉴了当前的生成搜索基础结构[25,3,4]。本文描述了Druid的架构，探索了在创建系统的过程中所做的各种决策设计，它为托管服务提供了动力，并试图帮助任何一个面临类似问题的人。Druid被部署在几个技术公司的生产环境中中。本文的结构如下:首先第2节中描述Druid解决的问题。接下来，第3节我们将从数据流角度详细介绍系统架构。在第4节中，我们讨论如何以及为什么数据转换成二进制格式。第5节中简要描述了查询API，并在第6节中展示了性能结果。最后，我们将从第7节中运行Druid、第8节中总结经验。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序 / INTRO]]></title>
    <url>%2F2018%2F02%2F10%2F%E5%BA%8F-INTRO%2F</url>
    <content type="text"><![CDATA[我们要有最朴素的生活 与最遥远的梦想即使明日天寒地冻 路远马亡 WELCOME HUBERY’S POLARIS 这里是徐海滨的博客 主要更新一些技术文章、个人作品 请持续关注哟~~~ 比心]]></content>
  </entry>
</search>

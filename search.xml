<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Opentsdb原理-上卷和预计算]]></title>
    <url>%2F2018%2F07%2F14%2FOpentsdb%E5%8E%9F%E7%90%86-%E4%B8%8A%E5%8D%B7%E5%92%8C%E9%A2%84%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[虽然TSDB只要空间足够会存储原始的全明细数据，但对较大时间跨度或许多标签组合的查询都会非常痛苦。此类查询可能需要很长时间才能完成，或者在最坏的情况下，会因内存不足而终止TSD。从OpenTSDB 2.4开始，一组新的API允许存储和查询较低明细度的数据，以便更快地响应这些查询。此页面将概述上卷和预聚合的内容，它们在TSDB中的工作方式以及如何最好地使用它们。请参阅API的部分以了解具体的实现细节 注意：OpenTSDB本身不会计算和存储汇总或预聚合数据。有多种方法可以计算结果，但根据比例和精度要求，它们都有其优点和缺点。请参阅生成上卷和预聚合部分，了解如何创建此数据。 示例数据为了帮助描述较低明细度的数据，让我们看一些完整的明细（也称为原始数据）示例数据。第一个表定义了带有快捷标识符的时间序列。 请注意，它们都具有相同的metric和interface tag，但host和colo不同。 接下来是以15分钟为间隔写入的一些数据： 请注意，这些数据集缺少某些数据点。让我们先看看上卷。 上卷在OpenTSDB中，“上卷”被定义为随时间聚合的时间序列。它也可以称为“基于时间的聚合”。上卷有助于解决查看宽时间跨度的问题。例如，如果您每60秒写一个数据点并查询一年的数据，则时间序列将返回超过525k个数据点。绘制许多数据点会非常混乱。相反，您可能希望查看较低明细度的数据，例如1小时数据，其中您只需绘制大约8k的值。然后，您可以识别异常并下钻更精细的明细数据。 如果您有使用OpenTSDB查询数据，您可能熟悉将每个时间序列聚合为较小或较低明细度的下采样器。上卷实质上是存储在系统中的下采样器的结果，并且随意调用。每个上卷（或下采样器）都需要两条信息： Interval ——“上卷”新值的时间间隔。例如，1h表示1小时数据，1d表示1天数据。 Aggregation Function ——对底层值执行了什么方法以获得新值。例如。 sum，max 注意：存储上卷时，最好避免使用average，median或deviation等函数。在执行进一步的下采样或分组聚合时，这些值变得毫无意义。相反，总是存储sum和count要好得多，至少可以在查询时计算平均值。有关更多信息，请参阅以下部分。上卷数据点的时间戳应定位在上卷间隔的顶部。例如。如果上卷间隔为1小时，则它包含1小时的数据，并且应该定位到小时的顶部。由于所有时间戳都是以Unix Epoch格式编写的，定义为UTC时区，这将是一小时UTC时间的开始。 示例正如上述的示例数据，我们计算count/sum并且时间间隔为1h。 请注意，无论何时出现“bucket”区间中的第一个数据点，所有时间戳都会与小时的顶部对齐。另请注意，如果间隔中缺失数据点，则计数较低。通常，您应该在存储上卷时为每个时间序列计算和存储MAX，MIN，SUM和COUNT。 上卷均值示例当启用上卷并且您从OpenTSDB请求具有avg函数的下采样器时，TSD将扫描存储以获取SUM和COUNT值。然后迭代数据准确计算平均值。count和sum值的时间戳必须匹配。然而，如果缺少SUM的COUNT值，则该SUM将从结果中踢出。从上面的示例中选择以下示例，我们现在丢失了ts2中的COUNT点。 得到的2小时下采样查询的平均值如下所示： 预聚合虽然上卷有助于广泛的时间跨度查询，但如果metric具有高基数（即给定度量标准的唯一时间序列数），则仍会遇到小范围的查询性能问题。在上面的示例中，我们有4个Web服务器。但是我们说我们有10,000台服务器。获取接口流量的总和或平均值可能相当慢。如果用户经常通过聚合计算这样的大集合来获取结果，那么直接存储聚合结果并获取少量数据是有意义的。 与上卷不同，预聚合只需要一条额外的信息： Aggregation Function ——对底层值执行了什么算术以获得新值。例如sum/max 在OpenTSDB中，与其他时间序列不同的是预聚合有特殊的tag。默认tag key是_aggregate（可通过tsd.rollups.agg_tag_key配置）。然后，用于生成数据的聚合函数以大写形式存储在tag value中。让我们看一个例子： 请注意，这些时间序列已经删除掉了host和interface。这是因为，在聚合过程中，host和interface的多个不同值已经累计在这个新时间序列中，所以将它们作为标签也没有意义了。另请注意，我们在存储的数据中注入了新的_aggregate标记。查询现在可以通过指定_aggregate值来访问此数据。 注意：启用上卷后，如果您计划使用预聚合，则可能需要通过让TSDB自动注入_aggregate = RAW来帮助区分原始数据和预聚合。只需将tsd.rollups.tag_raw设置配置为true即可。 得到如下结果集： 由于我们通过执行聚合组（按colo分组），我们从原始数据集中获取每个时间戳的值。在这种情况下，我们不会进行下采样或执行上卷。 注意：与汇总一样，在编写预聚合时，最好避免使用 average, median or deviation等函数。只需存储sum、count 上卷、预计算虽然预聚合肯定有助于高维度基数的指标，但用户可能仍希望要求广泛的时间跨度然后又会很慢。值得庆幸的是，您可以像原始数据一样上卷预聚合数据。只需生成预聚合，然后使用上面的信息将其上卷。 生成上卷、预计算目前，TSD不会为您生成上卷或预聚合数据。主要原因是OpenTSDB旨在处理大量的时间序列数据，因此各个TSD专注于尽可能快地将数据存入存储。 问题由于TSD的（基本上）无状态特性，它们可能不具有可用于执行预聚合的全套数据。例如，我们的样本ts1数据可以写入TSD_A，而ts2写入TSD_B。如果不从存储中读取数据，也不能执行适当的分组。我们也不知道我们应该在什么时候进行预聚合。我们可以等待1分钟并预聚合数据，但会错过那一分钟后发生的任何事情。或者我们可以等待一个小时，对预聚合的查询将不会包含最后一小时的数据。如果数据延迟会发生什么？ 此外，对于上卷，根据用户如何将数据写入TSD，对于ts1，我们可能会收到TSD_A上的12:15数据点，但12:30值会到达TSD_B，因此整个小时都不完整。窗口的限制仍然适用于上卷。 解决使用上卷和预聚合进行一些分析时需要在各种权衡之间进行选择。由于一些OpenTSDB用户已经有了计算此类数据的方法，我们只需提供API来存储和查询。但是，这里有一些关于如何自己计算这些的技巧。 批处理其他时间序列数据库常用的一种方法是在延迟一段时间后从数据库中读取数据，计算pre-aggs和roll-up，然后写入它们。这是解决问题的最简单方法，适用于小规模。但是仍然存在一些问题： 随着数据的增长，生成roll-up的查询也会增长到查询负载影响写入和用户查询性能的程度。即使在HBase下启用数据压缩时，OpenTSDB会遇到同样的问题。 此外，随着数据的增长，更多的数据意味着批处理时间更长，必须在多个worker之间进行分片，这可能会很难协调和排除故障。 除非采用某种跟踪方法来触发旧数据执行新批处理，否则可能无法聚合延迟或历史数据。 一些改进批处理的方法包括： 从备份系统中读取，例如如果设置HBase备份，则可以让用户查询主系统然后从备份存储读取并计算聚合结果。 从其他可选存储读取。一个示例是将所有数据镜像到另一个存储（例如HDFS），并针对该数据运行批处理作业。 Queueing on TSDs某些数据库使用的另一个选项是将进程内存中的所有数据在队列缓存，并在经过配置的时间窗口后写入结果。但由于TSD是无状态的，并且通常用户在其TSD之前放置负载均衡器，因此单个TSD可能无法全面了解要计算的汇总或预收集（如上所述）。要使此方法起作用，上游收集器必须将计算所需的所有数据路由到特定TSD。这不是一项艰巨的任务，但面临的问题包括: 有足够的RAM或磁盘空间来为每个TSD存储本地数据 如果TSD进程终止，您将丢失聚合的数据，或者必须从存储中恢复。 无论何时进行聚合计算，原始数据的总写入吞吐量都会受到影响。 您仍然有迟到/历史数据问题。 由于TSDB是基于JVM的，因此将所有数据保存在RAM中然后运行GC会受到影响。很严重。 （落磁盘更好，但是你会遇到IO问题） 总的来说，写入是使用队列缓存并聚合计算是一个bad idea。尽量避免。 流计算处理上卷和预聚合的更好方法是将数据路由到流处理系统，在那里可以近乎实时地处理数据并写入TSD。它类似于TSD上的queueing，但是使用流处理框架之一（Storm，Flink，Spark等）来处理消息路由和内存存储。然后，您只需编写一些代码来计算聚合，并在窗口通过后将数据吐出。 这是许多下一代监控解决方案所青睐的，例如雅虎的解决方案。雅虎正致力于为需要大规模监控的其他人开源他们的流处理系统，并将其整齐地插入TSDB。 虽然流处理更好，但仍然有问题需要处理： 确保流计算足够的资源 流计算worker失败需从存储重启 必须处理延迟/历史数据。 Share如果您有用于计算聚合的工作代码，请与OpenTSDB组共享。如果您的解决方案是开源的，我们可以将其合并到OpenTSDB生态系统中。 配置对于Opentsdb 2.4，roll-up由opentsdb.conf中的配置项tsd.rollups.config启用。此键的值必须是带引号转义的JSON字符串，不带换行符，或者最好是包含配置的JSON文件的路径。文件名必须以.json结尾，如rollup_config.json中所示。JSON配置应如下所示： { &quot;aggregationIds&quot;: { &quot;sum&quot;: 0, &quot;count&quot;: 1, &quot;min&quot;: 2, &quot;max&quot;: 3 }, &quot;intervals&quot;: [{ &quot;table&quot;: &quot;tsdb&quot;, &quot;preAggregationTable&quot;: &quot;tsdb-preagg&quot;, &quot;interval&quot;: &quot;1m&quot;, &quot;rowSpan&quot;: &quot;1h&quot;, &quot;defaultInterval&quot;: true }, { &quot;table&quot;: &quot;tsdb-rollup-1h&quot;, &quot;preAggregationTable&quot;: &quot;tsdb-rollup-preagg-1h&quot;, &quot;interval&quot;: &quot;1h&quot;, &quot;rowSpan&quot;: &quot;1d&quot; }] } 两个顶级字段包括： aggregationIds ——OpenTSDB聚合函数名称映射到用于压缩存储的数字标识符。 intervals ——包含表名的一个或多个时间间隔列表 aggregationIds聚合ID映射用于通过数字ID预先填充每种类型的数据而不是拼写出完整聚合函数名称来减少存储。例如。如果我们用COUNT作为每列的前缀：对于我们可以使用ID保存的每个值（或压缩列），这是6个字节。 ID必须是0到127之间的整数。这意味着我们每个时间间隔最多可以存储128个不同的聚合id。在映射中可以仅提供每个数值的一个ID，并且可以仅给出每种类型的一个聚合函数。如果函数名称未映射到OpenTSDB支持的聚合函数，则启动时将抛出异常。同样，必须至少给出一个聚合才能启动TSD。 注意：一旦开始写入数据，就无法更改聚合ID。如果更改映射，则可能会返回不正确的数据，或者查询和写入可能会失败。您可以在将来添加函数，但永远不要更改映射。 intervals每个intervals对象都定义了表路由，以便rollup和pre-aggregate时写入和查询数据。intervals有两种类型： Default ——这是定义的默认原始数据OpenTSDB表“defaultInterval”：true。对于现有安装，是tsdb表或tsd.storage.hbase.data_table中定义内容。时间间隔默认为OpenTSDB 1h ，并以给定的精度和时间戳存储数据。每个TSD只能配置一个默认值。 Rollup Interval ——“defaultInterval”默认为false。这些是汇总表，值被定义为时间间隔的边界。 应定义以下字段： 在存储中，rollup的写入类似于原始数据，因为每行都有一个基本时间戳，每个数据点都是该基准时间的偏移量。每个偏移量都是基准时间的增量，而不是实际偏移量。例如，如果一行存储1天的1小时数据，则最多可存在24个偏移。偏移0将映射到行的午夜，偏移5将映射到6AM。因为汇总偏移量是以14-bits编码的，如果一行中存储的间隔太多，则TSD启动时会引发错误。注意：将数据写入TSD后，请勿更改间隔宽度或行间距以进行汇总间隔。这可能导致垃圾数据和查询失败。]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb原理-查询细节与状态]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb%E5%8E%9F%E7%90%86-%E6%9F%A5%E8%AF%A2%E7%BB%86%E8%8A%82%E4%B8%8E%E7%8A%B6%E6%80%81%2F</url>
    <content type="text"><![CDATA[随着OpenTSDB 2.2版本的出现，我们现在可以围绕查询提供大量细节，因此我们专注于提高灵活性和性能。查询详细信息包括发出请求的人（通过headers和socket），响应是什么（HTTP状态代码和/或异常）以及TSD采取的各种进程的时间安排。 每个HTTP查询都可以包含一些这样的细节如原始查询，需使用showSummary和showQuery参数的添加计时信息。其他详细信息可以在 /api/stats/query的输出中找到，包括标题，状态和异常。并且可以通过日志配置将完整详细信息（除了实际结果数据）记录到磁盘。 查询此部分是用户请求的查询序列化。在日志和统计信息页面中，有包含完整查询的计时和全局选项。返回内容包括子查询（metric和filter）以用于标识查询内容，以及关联的结果集（例如如果您使用两个不同的聚合器请求两次相同的metric，则需要知道哪个数据集对应于哪个聚合器）。 对于字段及其含义，请参阅/ api / query。关于字段的一些内容如下: tags应具有与filters数组具有group_by条目相同的条目数。这是由于向后兼容2.1和1.0。旧样式查询将转换为筛选查询并以相同方式运行。 此处可能会显示许多额外字段，其默认值为null。 您可以将查询复制并粘贴到POST客户端以执行并找出返回的数据 示例： { &quot;start&quot;: &quot;1455531250181&quot;, &quot;end&quot;: null, &quot;timezone&quot;: null, &quot;options&quot;: null, &quot;padding&quot;: false, &quot;queries&quot;: [{ &quot;aggregator&quot;: &quot;zimsum&quot;, &quot;metric&quot;: &quot;tsd.connectionmgr.bytes.written&quot;, &quot;tsuids&quot;: null, &quot;downsample&quot;: &quot;1m-avg&quot;, &quot;rate&quot;: true, &quot;filters&quot;: [{ &quot;tagk&quot;: &quot;colo&quot;, &quot;filter&quot;: &quot;*&quot;, &quot;group_by&quot;: true, &quot;type&quot;: &quot;wildcard&quot; }, { &quot;tagk&quot;: &quot;env&quot;, &quot;filter&quot;: &quot;prod&quot;, &quot;group_by&quot;: true, &quot;type&quot;: &quot;literal_or&quot; }, { &quot;tagk&quot;: &quot;role&quot;, &quot;filter&quot;: &quot;frontend&quot;, &quot;group_by&quot;: true, &quot;type&quot;: &quot;literal_or&quot; }], &quot;rateOptions&quot;: { &quot;counter&quot;: true, &quot;counterMax&quot;: 9223372036854775807, &quot;resetValue&quot;: 1, &quot;dropResets&quot;: false }, &quot;tags&quot;: { &quot;role&quot;: &quot;literal_or(frontend)&quot;, &quot;env&quot;: &quot;literal_or(prod)&quot;, &quot;colo&quot;: &quot;wildcard(*)&quot; } }, { &quot;aggregator&quot;: &quot;zimsum&quot;, &quot;metric&quot;: &quot;tsd.hbase.rpcs.cumulative_bytes_received&quot;, &quot;tsuids&quot;: null, &quot;downsample&quot;: &quot;1m-avg&quot;, &quot;rate&quot;: true, &quot;filters&quot;: [{ &quot;tagk&quot;: &quot;colo&quot;, &quot;filter&quot;: &quot;*&quot;, &quot;group_by&quot;: true, &quot;type&quot;: &quot;wildcard&quot; }, { &quot;tagk&quot;: &quot;env&quot;, &quot;filter&quot;: &quot;prod&quot;, &quot;group_by&quot;: true, &quot;type&quot;: &quot;literal_or&quot; }, { &quot;tagk&quot;: &quot;role&quot;, &quot;filter&quot;: &quot;frontend&quot;, &quot;group_by&quot;: true, &quot;type&quot;: &quot;literal_or&quot; }], &quot;rateOptions&quot;: { &quot;counter&quot;: true, &quot;counterMax&quot;: 9223372036854775807, &quot;resetValue&quot;: 1, &quot;dropResets&quot;: false }, &quot;tags&quot;: { &quot;role&quot;: &quot;literal_or(frontend)&quot;, &quot;env&quot;: &quot;literal_or(prod)&quot;, &quot;colo&quot;: &quot;wildcard(*)&quot; } }], &quot;delete&quot;: false, &quot;noAnnotations&quot;: false, &quot;globalAnnotations&quot;: false, &quot;showTSUIDs&quot;: false, &quot;msResolution&quot;: false, &quot;showQuery&quot;: false, &quot;showStats&quot;: false, &quot;showSummary&quot;: false } 异常如果查询失败，则此字段将包括消息字符串和用于精确定位的堆栈信息的第一行。如果查询成功，则此字段将为null。示例： &quot;exception&quot;: &quot;No such name for &apos;metrics&apos;: &apos;nosuchmetric&apos; net.opentsdb.uid.UniqueId$1GetIdCB.call(UniqueId.java:315)&quot; 用户为了将来使用，此字段可用于从查询中提取用户信息，并帮助调试使用TSD的用户。修改TSD代码以从HTTP头中提取用户相当容易。 请求头这是使用HTTP请求发送的请求头内容。在未做太多安全处理时，Cookie头字段包含用户可识别或安全信息的情况下使用星号进行模糊处理。每个请求都不同，因此请在HTTP RFC或Web浏览器或客户端文档中查找头部信息。 示例： &quot;requestHeaders&quot;: { &quot;Accept-Language&quot;: &quot;en-US,en;q=0.8&quot;, &quot;Host&quot;: &quot;tsdhost:4242&quot;, &quot;Content-Length&quot;: &quot;440&quot;, &quot;Referer&quot;: &quot;http://tsdhost:8080/dashboard/db/tsdfrontend&quot;, &quot;Accept-Encoding&quot;: &quot;gzip, deflate&quot;, &quot;X-Forwarded-For&quot;: &quot;192.168.0.2&quot;, &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.109 Safari/537.36&quot;, &quot;Origin&quot;: &quot;http://tsdhost:8080&quot;, &quot;Content-Type&quot;: &quot;application/json;charset=UTF-8&quot;, &quot;Accept&quot;: &quot;application/json, text/plain, */*&quot; } HttpResponse该字段包含数字HTTP响应代码和文本内容。 示例： &quot;httpResponse&quot;: { &quot;code&quot;: 200, &quot;reasonPhrase&quot;: &quot;OK&quot; } 其他字段日志文件和统计信息页面的输出包括具有单个值的其他字段，如下所示： 状态每个查询周围都有许多统计信息，并且会随着时间的推移添加更多统计信息。测量各种级别的细节，包括： Global ——与整个查询有关的度量标准，包括每个子查询的最大和平均时间 Per-Sub Query ——与单个子查询有关的度量（可能存在多个），包括Scanner的最大和平均时间。 Per-Scanner ——每个Scanner相关的Metrics（启用Salting时很有用） 全局统计信息将打印到统计信息页面的标准日志。当showSummary启用时，查询日志和API中可以看到子查询和Scanner的详细信息。较低级别的计时统计数据汇总为较高级别的最大值和平均值。每个低级别的计数器也会在高级别进行汇总，因此您将在每个级别看到相同的计数器指标。下面显示统计和部分表格 示例: { &quot;statsSummary&quot;: { &quot;avgAggregationTime&quot;: 3.784976, &quot;avgHBaseTime&quot;: 8.530751, &quot;avgQueryScanTime&quot;: 10.964149, &quot;avgScannerTime&quot;: 8.588306, &quot;avgScannerUidToStringTime&quot;: 0.0, &quot;avgSerializationTime&quot;: 3.809661, &quot;emittedDPs&quot;: 1256, &quot;maxAggregationTime&quot;: 3.759478, &quot;maxHBaseTime&quot;: 9.904215, &quot;maxQueryScanTime&quot;: 10.320964, &quot;maxScannerUidtoStringTime&quot;: 0.0, &quot;maxSerializationTime&quot;: 3.779712, &quot;maxUidToStringTime&quot;: 0.197926, &quot;processingPreWriteTime&quot;: 20.170205, &quot;queryIdx_00&quot;: { &quot;aggregationTime&quot;: 3.784976, &quot;avgHBaseTime&quot;: 8.849337, &quot;avgScannerTime&quot;: 8.908597, &quot;avgScannerUidToStringTime&quot;: 0.0, &quot;emittedDPs&quot;: 628, &quot;groupByTime&quot;: 0.0, &quot;maxHBaseTime&quot;: 9.904215, &quot;maxScannerUidtoStringTime&quot;: 0.0, &quot;queryIndex&quot;: 0, &quot;queryScanTime&quot;: 10.964149, &quot;saltScannerMergeTime&quot;: 0.128234, &quot;scannerStats&quot;: { &quot;scannerIdx_00&quot;: { &quot;compactionTime&quot;: 0.048703, &quot;hbaseTime&quot;: 8.844783, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[0, 0, 2, 88, 86, -63, -25, -16], stop_key=[0, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.899045, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_01&quot;: { &quot;compactionTime&quot;: 0.066892, &quot;hbaseTime&quot;: 8.240165, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[1, 0, 2, 88, 86, -63, -25, -16], stop_key=[1, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.314855, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_02&quot;: { &quot;compactionTime&quot;: 0.01298, &quot;hbaseTime&quot;: 8.462203, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[2, 0, 2, 88, 86, -63, -25, -16], stop_key=[2, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.478315, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_03&quot;: { &quot;compactionTime&quot;: 0.036998, &quot;hbaseTime&quot;: 9.862741, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[3, 0, 2, 88, 86, -63, -25, -16], stop_key=[3, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.904215, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_04&quot;: { &quot;compactionTime&quot;: 0.058698, &quot;hbaseTime&quot;: 9.523481, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[4, 0, 2, 88, 86, -63, -25, -16], stop_key=[4, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.587324, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_05&quot;: { &quot;compactionTime&quot;: 0.041017, &quot;hbaseTime&quot;: 9.757787, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[5, 0, 2, 88, 86, -63, -25, -16], stop_key=[5, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.802395, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_06&quot;: { &quot;compactionTime&quot;: 0.062371, &quot;hbaseTime&quot;: 9.332585, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[6, 0, 2, 88, 86, -63, -25, -16], stop_key=[6, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.40264, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_07&quot;: { &quot;compactionTime&quot;: 0.063974, &quot;hbaseTime&quot;: 8.195105, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[7, 0, 2, 88, 86, -63, -25, -16], stop_key=[7, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.265713, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_08&quot;: { &quot;compactionTime&quot;: 0.062196, &quot;hbaseTime&quot;: 8.21871, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[8, 0, 2, 88, 86, -63, -25, -16], stop_key=[8, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.287582, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_09&quot;: { &quot;compactionTime&quot;: 0.051666, &quot;hbaseTime&quot;: 7.790636, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[9, 0, 2, 88, 86, -63, -25, -16], stop_key=[9, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 7.849597, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_10&quot;: { &quot;compactionTime&quot;: 0.036429, &quot;hbaseTime&quot;: 7.6472, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[10, 0, 2, 88, 86, -63, -25, -16], stop_key=[10, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 7.689386, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_11&quot;: { &quot;compactionTime&quot;: 0.044493, &quot;hbaseTime&quot;: 7.897932, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[11, 0, 2, 88, 86, -63, -25, -16], stop_key=[11, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 7.94793, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_12&quot;: { &quot;compactionTime&quot;: 0.025362, &quot;hbaseTime&quot;: 9.30409, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[12, 0, 2, 88, 86, -63, -25, -16], stop_key=[12, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.332411, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_13&quot;: { &quot;compactionTime&quot;: 0.057429, &quot;hbaseTime&quot;: 9.215958, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[13, 0, 2, 88, 86, -63, -25, -16], stop_key=[13, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.278104, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_14&quot;: { &quot;compactionTime&quot;: 0.102855, &quot;hbaseTime&quot;: 9.598685, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[14, 0, 2, 88, 86, -63, -25, -16], stop_key=[14, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.712258, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_15&quot;: { &quot;compactionTime&quot;: 0.0727, &quot;hbaseTime&quot;: 9.273193, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[15, 0, 2, 88, 86, -63, -25, -16], stop_key=[15, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.35403, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_16&quot;: { &quot;compactionTime&quot;: 0.025867, &quot;hbaseTime&quot;: 9.011146, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[16, 0, 2, 88, 86, -63, -25, -16], stop_key=[16, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.039663, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_17&quot;: { &quot;compactionTime&quot;: 0.066071, &quot;hbaseTime&quot;: 9.175692, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[17, 0, 2, 88, 86, -63, -25, -16], stop_key=[17, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 9.24738, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_18&quot;: { &quot;compactionTime&quot;: 0.090249, &quot;hbaseTime&quot;: 8.730833, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[18, 0, 2, 88, 86, -63, -25, -16], stop_key=[18, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.831461, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_19&quot;: { &quot;compactionTime&quot;: 0.039327, &quot;hbaseTime&quot;: 8.903825, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[19, 0, 2, 88, 86, -63, -25, -16], stop_key=[19, 0, 2, 88, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.947639, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 } }, &quot;serializationTime&quot;: 3.809661, &quot;successfulScan&quot;: 20, &quot;uidPairsResolved&quot;: 0, &quot;uidToStringTime&quot;: 0.197926 }, &quot;queryIdx_01&quot;: { &quot;aggregationTime&quot;: 3.73398, &quot;avgHBaseTime&quot;: 8.212164, &quot;avgScannerTime&quot;: 8.268015, &quot;avgScannerUidToStringTime&quot;: 0.0, &quot;emittedDPs&quot;: 628, &quot;groupByTime&quot;: 0.0, &quot;maxHBaseTime&quot;: 8.986041, &quot;maxScannerUidtoStringTime&quot;: 0.0, &quot;queryIndex&quot;: 1, &quot;queryScanTime&quot;: 9.67778, &quot;saltScannerMergeTime&quot;: 0.095797, &quot;scannerStats&quot;: { &quot;scannerIdx_00&quot;: { &quot;compactionTime&quot;: 0.054894, &quot;hbaseTime&quot;: 8.708179, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[0, 0, 2, 76, 86, -63, -25, -16], stop_key=[0, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.770252, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_01&quot;: { &quot;compactionTime&quot;: 0.055956, &quot;hbaseTime&quot;: 8.666615, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[1, 0, 2, 76, 86, -63, -25, -16], stop_key=[1, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.730629, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_02&quot;: { &quot;compactionTime&quot;: 0.011224, &quot;hbaseTime&quot;: 8.474637, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[2, 0, 2, 76, 86, -63, -25, -16], stop_key=[2, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.487582, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_03&quot;: { &quot;compactionTime&quot;: 0.081926, &quot;hbaseTime&quot;: 8.894951, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[3, 0, 2, 76, 86, -63, -25, -16], stop_key=[3, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.986041, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_04&quot;: { &quot;compactionTime&quot;: 0.01882, &quot;hbaseTime&quot;: 8.209866, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[4, 0, 2, 76, 86, -63, -25, -16], stop_key=[4, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.231502, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_05&quot;: { &quot;compactionTime&quot;: 0.056902, &quot;hbaseTime&quot;: 8.709846, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[5, 0, 2, 76, 86, -63, -25, -16], stop_key=[5, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.772216, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_06&quot;: { &quot;compactionTime&quot;: 0.131424, &quot;hbaseTime&quot;: 8.033916, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[6, 0, 2, 76, 86, -63, -25, -16], stop_key=[6, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.181117, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_07&quot;: { &quot;compactionTime&quot;: 0.022517, &quot;hbaseTime&quot;: 8.006976, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[7, 0, 2, 76, 86, -63, -25, -16], stop_key=[7, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.032073, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_08&quot;: { &quot;compactionTime&quot;: 0.011527, &quot;hbaseTime&quot;: 8.591358, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[8, 0, 2, 76, 86, -63, -25, -16], stop_key=[8, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.604491, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_09&quot;: { &quot;compactionTime&quot;: 0.162222, &quot;hbaseTime&quot;: 8.25452, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[9, 0, 2, 76, 86, -63, -25, -16], stop_key=[9, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.435525, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_10&quot;: { &quot;compactionTime&quot;: 0.033886, &quot;hbaseTime&quot;: 7.973254, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[10, 0, 2, 76, 86, -63, -25, -16], stop_key=[10, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.011236, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_11&quot;: { &quot;compactionTime&quot;: 0.039491, &quot;hbaseTime&quot;: 7.959601, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[11, 0, 2, 76, 86, -63, -25, -16], stop_key=[11, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.003249, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_12&quot;: { &quot;compactionTime&quot;: 0.107793, &quot;hbaseTime&quot;: 8.177353, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[12, 0, 2, 76, 86, -63, -25, -16], stop_key=[12, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.298284, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_13&quot;: { &quot;compactionTime&quot;: 0.020697, &quot;hbaseTime&quot;: 8.124243, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[13, 0, 2, 76, 86, -63, -25, -16], stop_key=[13, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.147879, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_14&quot;: { &quot;compactionTime&quot;: 0.033261, &quot;hbaseTime&quot;: 8.145149, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[14, 0, 2, 76, 86, -63, -25, -16], stop_key=[14, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.182331, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_15&quot;: { &quot;compactionTime&quot;: 0.057804, &quot;hbaseTime&quot;: 8.17854, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[15, 0, 2, 76, 86, -63, -25, -16], stop_key=[15, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.243458, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_16&quot;: { &quot;compactionTime&quot;: 0.01212, &quot;hbaseTime&quot;: 8.070582, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[16, 0, 2, 76, 86, -63, -25, -16], stop_key=[16, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 8.084813, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_17&quot;: { &quot;compactionTime&quot;: 0.036777, &quot;hbaseTime&quot;: 7.919167, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[17, 0, 2, 76, 86, -63, -25, -16], stop_key=[17, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 7.959645, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_18&quot;: { &quot;compactionTime&quot;: 0.048097, &quot;hbaseTime&quot;: 7.87351, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[18, 0, 2, 76, 86, -63, -25, -16], stop_key=[18, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 7.926318, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 }, &quot;scannerIdx_19&quot;: { &quot;compactionTime&quot;: 0.0, &quot;hbaseTime&quot;: 7.271033, &quot;scannerId&quot;: &quot;Scanner(table=\&quot;tsdb\&quot;, start_key=[19, 0, 2, 76, 86, -63, -25, -16], stop_key=[19, 0, 2, 76, 86, -62, 4, 16], columns={\&quot;t\&quot;}, populate_blockcache=true, max_num_rows=128, max_num_kvs=4096, region=null, filter=KeyRegexpFilter(\&quot;(?s)^.{8}(?:.{7})*\\Q\u0000\u0000\u0005\\E(?:\\Q\u0000\u0000\u00006\\E)(?:.{7})*$\&quot;, ISO-8859-1), scanner_id=0x0000000000000000)&quot;, &quot;scannerTime&quot;: 7.271664, &quot;scannerUidToStringTime&quot;: 0.0, &quot;successfulScan&quot;: 1, &quot;uidPairsResolved&quot;: 0 } }, &quot;serializationTime&quot;: 3.749764, &quot;successfulScan&quot;: 20, &quot;uidPairsResolved&quot;: 0, &quot;uidToStringTime&quot;: 0.162088 }, &quot;successfulScan&quot;: 40, &quot;uidPairsResolved&quot;: 0 } }]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb原理-查询示例]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb%E5%8E%9F%E7%90%86-%E6%9F%A5%E8%AF%A2%E7%A4%BA%E4%BE%8B%2F</url>
    <content type="text"><![CDATA[以下使用示例数据集的说明查询模式。我们将说明可能遇到的一些常见查询类型，以便您了解查询系统的工作方式。示例集中的每个时间序列仅存储一个数据点，并且UID已被截断为单个字节以使其更易于读取。示例数据集是使用HTTP API查询的所有Metric，仅显示 m= 部分。有关详细信息，请参阅 /api/ query。如果您使用的是CLI工具，则查询格式会略有不同，请阅读特定命令的文档。 示例数据 注意：这不一定是设置指标和标签的最佳方式，而是用于说明查询系统的工作原理。特别地，TS＃4和5虽然是合法的时间序列，但可能会破坏你的查询，除非你知道它们是如何工作的。通常，尝试为每个时间序列保持相同数量和类型的标记。 底层原理您可能想要了解OpenTSDB如何在此处存储时间序列数据：存储章节。否则，请记住存储中的每一行都有一个格式化的唯一键： &lt;metricID&gt;&lt;normalized_timestamp&gt;&lt;tagkID1&gt;&lt;tagvID1&gt;[...&lt;tagkIDN&gt;&lt;tagvIDN&gt;] 上面的数据表将存储为: 01&lt;ts&gt;0101 01&lt;ts&gt;01010306 01&lt;ts&gt;02040101 01&lt;ts&gt;02040102 01&lt;ts&gt;02040103 01&lt;ts&gt;02050101 01&lt;ts&gt;02050102 02&lt;ts&gt;02040101 02&lt;ts&gt;02040102 当您查询OpenTSDB时，这是在幕后发生的事情: 解析并验证查询以确保格式正确并且存在metric，tag name和tag value。如果系统中不存在该metric，tag name或tag value，则会返回错误。 然后它为底层存储系统设置Scanner 如果查询没有任何标记或标记值，那么它将获取与 匹配的任何数据行，因此如果您有特定metric的大量时间序列，则可能会有很多，很多行。 如果查询确实定义了一个或多个tag，那么它仍将扫描与 匹配的所有行，但还会执行正则表达式以仅返回包含所请求标记的行。 如果需要的话, 一旦返回了所有数据, OpenTSDB就会将其分组(group)， 如果请求下采样，则使用适当的聚合器将每个单独的时间序列下采样到较小的时间跨度中 然后使用特定聚合函数聚合每组数据 如果检测到rate标志，则将调整每个聚合值以获得速率 返回结果给调用者 查询一: 一个Metric的所有时间序列m=sum:cpu.system 这是最简单的查询。 TSDB将设置Scanner以获取和之间的metric UID为01的所有数据点。结果将是时间序列＃1到＃7汇总在一起的单个数据集。如果给定metric有数千个唯一tagkv，则它们将全部累加到一个时间序列中。 [ { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: {}, &quot;aggregated_tags&quot;: [ &quot;host&quot; ], &quot;tsuids&quot;: [ &quot;010101&quot;, &quot;0101010306&quot;, &quot;0102050101&quot;, &quot;0102040101&quot;, &quot;0102040102&quot;, &quot;0102040103&quot;, &quot;0102050102&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 130.29999923706055 } } ] 查询二: Tag过滤通常聚合metric的所有时间序列并不是特别有用。相反，我们可以通过筛选包含特定 tagk / tagv 对组合的时间序列来深入挖掘。只需将它们放在花括号中： m=sum:cpu.system{host=web01} 这将返回时间序列＃1，＃4，＃5和＃6的汇总序列，因为它们是包含host = web01的唯一时间序列。 [ { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;host&quot;: &quot;web01&quot; }, &quot;aggregated_tags&quot;: [], &quot;tsuids&quot;: [ &quot;010101&quot;, &quot;0101010306&quot;, &quot;0102040101&quot;, &quot;0102050101&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 63.59999942779541 } } ] 查询三: 指定时间序列如果你想要一个特定的时间表怎么办？您必须包含每个tag name和相应的tag value。 m=sum:cpu.system{host=web01,dc=lax} 这将仅返回时间序列＃6中的数据。 [ { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;dc&quot;: &quot;lax&quot;, &quot;host&quot;: &quot;web01&quot; }, &quot;aggregated_tags&quot;: [], &quot;tsuids&quot;: [ &quot;0102050101&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 15.199999809265137 } } ] 注意：这是tag schema的缺陷。假设您想从时间序列＃4中获取数据。使用当前系统，无法做到。如果你这样查询m=sum:cpu.system{host=web01}，以期这样仅会返回#4。然而，正如我们所见会返回#1, #4, #5 and #6的聚合值。为防止出现这种情况，您需要在＃4中添加另一个标记，以区别于该组中的其他时间序列。或者，如果您已经提交，则可以使用TSUID查询。 查询四: 指定TSUID如果您知道要检索的时间序列的确切TSUID，则可以像这样简单地查询： tsuids=sum:0102040102 结果将是您请求的时间序列: [ { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;dc&quot;: &quot;lax&quot;, &quot;host&quot;: &quot;web01&quot; }, &quot;aggregated_tags&quot;: [], &quot;tsuids&quot;: [ &quot;0102050101&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 15.199999809265137 } } ] 查询五: 多TSUID查询您还可以在同一查询中聚合多个TSUID，前提是它们有相同的metric。如果您尝试聚合多个metric，API将会报错。 tsuids=sum:0102040101,0102050101 [ { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;host&quot;: &quot;web01&quot; }, &quot;aggregated_tags&quot;: [ &quot;dc&quot; ], &quot;tsuids&quot;: [ &quot;0102040101&quot;, &quot;0102050101&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 33.19999980926514 } } ] 查询六: Groupingm=sum:cpu.system{host=*} *（星号）是一个分组运算符，它将为给定的标记名称的每个唯一值返回一个数据集。 包含给定metric和给定tag name的每个时间序列，无论其他tag name或tag value如何，都将聚合在结果中。在对各个时间序列结果进行分组后，它们将被聚合并返回。 上面的例子，我们将会三组数据返回： Group Time Series Included web01 #1, #4, #5, #6 web02 #2, #7 web03 #3 TSDB发现了7个包含“host”标签的总时间序列。该标记有3个唯一值（web01，web02和web03）。 [ { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;host&quot;: &quot;web01&quot; }, &quot;aggregated_tags&quot;: [], &quot;tsuids&quot;: [ &quot;010101&quot;, &quot;0101010306&quot;, &quot;0102040101&quot;, &quot;0102050101&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 63.59999942779541 } }, { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;host&quot;: &quot;web02&quot; }, &quot;aggregated_tags&quot;: [ &quot;dc&quot; ], &quot;tsuids&quot;: [ &quot;0102040102&quot;, &quot;0102050102&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 24.199999809265137 } }, { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;dc&quot;: &quot;dal&quot;, &quot;host&quot;: &quot;web03&quot; }, &quot;aggregated_tags&quot;: [], &quot;tsuids&quot;: [ &quot;0102040103&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 42.5 } } ] 查询七: Group and Filter注意，在示例＃2中，web01组包括时间序列＃4和＃5。我们可以通过指定第二个标记来过滤掉那些： m=sum:cpu.nice{host=*,dc=dal} 现在我们只得到＃1 - ＃3的结果，但我们过滤了dc = lax值。 [ { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;dc&quot;: &quot;dal&quot;, &quot;host&quot;: &quot;web01&quot; }, &quot;aggregated_tags&quot;: [], &quot;tsuids&quot;: [ &quot;0102040101&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 18 } }, { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;dc&quot;: &quot;dal&quot;, &quot;host&quot;: &quot;web02&quot; }, &quot;aggregated_tags&quot;: [], &quot;tsuids&quot;: [ &quot;0102040102&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 9 } }, { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;dc&quot;: &quot;dal&quot;, &quot;host&quot;: &quot;web03&quot; }, &quot;aggregated_tags&quot;: [], &quot;tsuids&quot;: [ &quot;0102040103&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 42.5 } } ] 查询八: Group With OR*运算符是贪婪的，将返回分配给标记名称的所有值。如果您只想要几个标记值，则可以使用| （管道）达成。 m=sum:cpu.nice{host=web01|web02} 这将找到包含“web01”或“web02”的“主机”值的所有时间序列，然后按值分组，类似于*运算符。 这次我们的分组看起来像这样： Group Time Series Included web01 #1, #4, #5, #6 web02 #2, #7 [ { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;host&quot;: &quot;web01&quot; }, &quot;aggregated_tags&quot;: [], &quot;tsuids&quot;: [ &quot;010101&quot;, &quot;0101010306&quot;, &quot;0102040101&quot;, &quot;0102050101&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 63.59999942779541 } }, { &quot;metric&quot;: &quot;cpu.system&quot;, &quot;tags&quot;: { &quot;host&quot;: &quot;web02&quot; }, &quot;aggregated_tags&quot;: [ &quot;dc&quot; ], &quot;tsuids&quot;: [ &quot;0102040102&quot;, &quot;0102050102&quot; ], &quot;dps&quot;: { &quot;1346846400&quot;: 24.199999809265137 } } ]]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb原理-查询性能]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb%E5%8E%9F%E7%90%86-%E6%9F%A5%E8%AF%A2%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[查询性能对任何数据库系统都至关重要。此页面列出了一些常见的OpenTSDB问题以及提高查询性能的步骤。 缓存此时，OpenTSDB没有内置缓存（除了内置GUI，将缓存PNG图像文件60秒）。因此，我们依赖于底层数据库的缓存。在HBase（最普遍的OpenTSDB后端存储）中，存在块缓存的概念，它将在写入和/或读取时在内存中存储行和列的块。一个较好的入门教程是 Nick Dimiduck’s Block Cache 101。设置缓存的一个好方法是使用BucketCache并相当大地调整L1缓存的大小，以便它充当写缓存并将大部分最近的数据保存在内存中。然后，当用户运行查询时，L2缓存可以将经常查询的数据保存在内存中。 仔细观察您所在region的服务器是否有GC暂停。用户通常在堆外模式下运行bucket缓存，但在Java和JNI在堆外内存之间读写时，仍然需要付出序列化的代价。 此外，请确保在HBase表上启用了压缩。块使用表中指定的压缩算法存储在内存中，因此您可以在缓存中容纳比未压缩更多的压缩块。 一个可优化的查询如果您通常在查询中查找高基数（即很多tag value）的时间序列，请确保使用版本2.3或更高版本并启用了explicitTags。因为查询必须列出与您要查找的数据相关联的所有tag key，我们将在HBase上启用特殊过滤器，这将有助于减少扫描的行数。See Query Filters for details. 或者，如果在metric名称中存放高基数的tag，这将大大减少在查询时扫描的数据量并提高性能。有关更多信息，请参阅《写数据》章节 高基数查询对于将多个时间序列聚合在一起的查询，提高性能的最佳方法是在启用了salting的情况下运行OpenTSDB 2.2或更高版本，并在HBase集群中运行多个region服务器。这将并行执行查询，从每个区域服务器获取数据子集并合并结果。例如，对于单个region服务器，查询可能需要10秒才能完成。当使用salting将相同数据写入5个region服务器时，相同的查询应该花费大约2秒，即最慢region服务器响应所花费的时间。合并集合通常是比重较小的时间。 大跨度的时间范围查询如果在TSD和消费应用程序（例如UI或API客户端）之间观察到瓶颈，那么宽时间跨度（例如数月或数年）的查询可以从下采样中优化。使用下采样器将减少TSD序列化并发送给用户的数据量。 但是，如果瓶颈在存储（HBase）和TSD之间，那么最好的解决方案是使用OpenTSDB 2.4或更高版本writing rolled-up汇总数据。这需要外部系统来计算基于时间的汇总并将其写入存储。或者，UI或API客户端可以针对较小的时间跨度对多个TSD执行多个查询，并将结果合并在一起。在未来，我们计划直接向TSD添加此类功能。 通用优化方法以下几项供参考： 多个读TSD运行专用于读取数据的多个TSD，并在其前置负载均衡。这是运行OpenTSDB时最常见设置，也可以完成在不关闭整个系统的情况下轮换TSD升级。 优化存储HBase有许多可以调整的参数，一般来说，OpenTSDB的大部分瓶颈都来自HBase。确保监视服务器，尤其是队列，缓存，响应时间，CPU和GC。 规范使用没有数据库系统可以免受长时间运行或资源占用查询的影响。要求用户以较小的时间范围（例如1小时）开始，并逐渐增加其时间范围。然而，让用户理解基数的概念以及如何查询high_cardinality_tag_key = * 可能并不合适]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb原理-下采样]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb%E5%8E%9F%E7%90%86-%E4%B8%8B%E9%87%87%E6%A0%B7%2F</url>
    <content type="text"><![CDATA[下采样(信号处理，小数)是降低数据采样率、分辨率的过程。例如，假设温度传感器每秒将数据发送到OpenTSDB系统。如果用户在一个小时的时间内查询数据，他们将得到3600个数据点，这可以很容易地绘制出来。然而，如果用户要求一个完整的星期的数据，他们将收到604,800个数据点，突然之间图形可能变得非常混乱。使用downsampler，单个时间序列的时间范围内的多个数据点集合在一起，并将一个数学函数集合到一个对齐时间戳的单个值中。这样我们就可以减少从604,800到168的值。 Downsamplers至少需要两个组件:Interval——时间范围(或桶)，用来聚合值。例如，我们可以聚合多个值1分钟，1小时，甚至一整天。间隔在格式如 1h代表一小时或30m代表30分钟。在2.3的时候，all间隔现在可以被向下采样，所有的结果在时间范围内到一个值。all-sum将从查询开始到结束的所有值相加。请注意，仍然需要一个数值，但它可以是零或任何其他值。Aggregation Function——一个数学函数，它决定如何在区间内合并值。聚合文档中的聚合函数用于函数。 例如，在接下来的时间序列A和b中，数据点的跨度为70秒，每10秒一次。假设我们想要将这个样本值降低到30秒，因此用户看到的是一个更宽的时间跨度图。另外，我们将这两个系列组合成一个使用sum聚合器。我们可以指定一个30-sum的downsampler，它将创建30秒的bucket，并将每个bucket中的所有数据点相加。这将给我们每个系列的三个数据点： 正如你所看到的，对于每一个时间序列，我们都会生成一个在区间边界上的时间戳的合成序列(每30秒)，这样我们就能在t0,t0 + 30和t0 + 60上有一个值。每个间隔(或bucket)将包含包含bucket timestamp(start)的数据点，并包含以下bucket的timestamp(end)。在这种情况下，第一个bucket将从t0扩展到t0 + 29.9999。使用提供的聚合器，所有的值都被合并到一个新的值中。例如，对于A系列，我们总结了t0,t0 + 10s和t0 + 20的值，以t0为20的新值。最后，查询是group - by使用sum，因此我们添加了两个合成时间序列。在此期间，OpenTSDB总是在采样后进行聚合。 Note：对于OpenTSDB的早期版本，新数据点的实际时间戳将是每个数据点在时间跨度上的平均时间戳。在2.1和之后，每个点的时间戳根据当前时间的模组和下采样间隔对齐到一个bucket的起始位置。 被采样的时间戳是基于原始数据点时间戳的其余部分进行标准化的，这些时间戳除以采样间隔的毫秒数的模量。在Java中，代码是时间戳-(timestamp % interval_ms)。例如，给定时间戳为1388550980000，或1/1/2014 04:36:20 UTC，每小时间隔等于3600000毫秒，生成的时间戳将被圆到1388548800000。所有数据点在4和5之间的UTC将会在4 AM桶中结束。如果你将一天的数据下行采样到一小时内，你将得到24个数据点(假设是这样)。当使用0all - 间隔时，结果的时间戳将是查询的开始时间。标准化工作对常见的查询非常有效，比如将一天的数据压缩到1分钟或1小时。然而，如果您尝试在一个奇怪的间隔(比如36分钟)下进行抽样，那么时间戳可能看起来有点奇怪，因为它的本质是模量计算。如果间隔为36分钟，我们上面的例子，间隔将是2160000毫秒，产生的时间戳为1388549520或04:12:00 UTC。在04:12和04:48之间的所有数据点都将在一个桶中结束。 Calendar Boundaries从OpenTSDB 2.3开始，用户可以指定基于日历的下行采样，而不是快速的模数法。这对于报告结果更有用，例如查看与人类时代相关的价值，比如几个月、几周或几天。此外，下行采样可以解释时区，白天时间和时区偏移量。]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb原理-聚合]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb%E5%8E%9F%E7%90%86-%E8%81%9A%E5%90%88%2F</url>
    <content type="text"><![CDATA[OpenTSDB被设计用于在查询执行期间有效地组合多个、不同的时间序列。这样做的原因是当用户查看他们的数据时，他们通常会从一个较高的层次开始提问，比如“我的数据中心的总吞吐量是多少?”或者“地区目前的能耗是多少?”。在查看这些高级别的值之后，一个或多个用户可能会坚持下去，深入到更细粒度的数据集，比如“在我的松散的数据中心中，主机的吞吐量是多少?”我们希望能很容易地回答这些高层次的问题，但仍然可以进行更详细的下钻。 但是如何将多个单独的时间序列合并成单一的数据系列呢?聚合函数提供了将不同时间序列合并成一个的数学方法。过滤器用于通过tag来分组结果，聚合应用于每个组。聚合类似于SQL的GROUP BY子句，用户选择一个预定义的聚合函数，将多个记录合并到一个结果中。然而在tsd中，一组记录按时间戳和组进行聚合。 每个聚合器有两个组件: Function——数学计算应用于所有的值，计算平均值或选择最高。 Interpolation——一种处理丢失值的方法，比如时间序列A在T1时的值，但是时间序列B没有值。 这个文章关注的是聚合器如何在一个环境中使用，即在将多个时间序列合并为一个时。此外，聚合器还可以用于压缩时间序列(即返回一个较低的结果集)。有关更多信息，请参见Downsampling。 聚合当将多组时间序列聚合成一组时，每个序列的时间戳都是对齐的。然后，对于每个时间戳，所有时间序列中的值聚合成一个新的数值。也就是说，聚合器将在每个时间序列的所有时间戳上工作。将原始数据看作一个矩阵或表，如下例所示，它演示了在两个时间序列(A和B)上工作的sum聚合器，从而产生一个新的时间序列输出。 对于时间戳t0，可以总结A和B的数据点，即5 + 10 = = 15。接下来，将ts1的两个值相加得到10，以此类推。在SQL中，这看起来像是SELECT SUM(value) FROM ts_table GROUP BY timestamp。 插值在上面的例子中，时间序列A和B都有数据点在每个时间戳，它们整齐排列。然而，当两个系列不一一对应时，会发生什么呢?它可能很难，有时甚至是不需要，同步时所有数据源的同时写入数据。例如，如果我们有10,000个服务器每5分钟发送100个系统指标，那么在一秒钟内就会有1000万个数据点。我们需要一个非常结实的网络和集群来容纳流量。更不用说系统会闲置4分59秒。相反，随着时间的推移，写入文件变得更有意义，这样我们平均每秒就有3333个写，以减少我们的硬件和网络需求。你如何对一个不存在的值进行求和或者求平均呢?第一种本能想到的是返回有效的数据点，并完成它。然而，如果您正在处理的是数千个数据源，其中数据点是完全不一致的，那该怎么办呢?例如，下面的图显示了一个不一致的时间序列，结果是一条让人感到困惑的锯齿状的线： Note:Missing Data：“丢失”，是指一个时间序列在给定时间戳中没有数据点。通常情况下，数据在请求的具体时间戳之前或之后偏移，但如果源或TSD遇到错误，而数据没有被记录，则可能会丢失数据。一些时间序列DBs可以允许在时间戳中存储NaN，以表示不可记录的值，但是OpenTSDB还不允许这样做。 或者，您可以简单地忽略所有在给定时间戳的时间序列数据点，任何序列都缺少数据。但是如果您有两个时间序列，并且它们仅仅是时间戳不一致的，那么您的查询将返回一个空的数据集，即使在存储中有很好的数据，所以这并不一定非常有用。 另一种方法是定义一个标量值(例如，0或Long的最大值)，以便在数据点丢失时使用。OpenTSDB 2.0之后的版本提供了一些聚合方法，可以用一个标量值替换缺少的数据点，实际上，上面的图形是使用zimsum聚合器生成的，它用一个0代替不一致的值。这种替换在处理不同的值时间序列时非常有用，例如在给定时间内的总销售额，但不适用于处理平均值或较严格的视觉图形。 OpenTSDB提供的一个方案是使用定义好的数值分析方法来插值这个时间点的值。插值使用现有数据点来计算在请求的时间戳中最好的猜测值。使用OpenTSDB的线性插值我们可以平滑我们的不一致图得到： 对于一个数值例子，看一下这两个时间序列，其中的源每20秒发出一个值，而数据有10秒的时间偏移： 当OpenTSDB计算一个聚合时，它开始于任何时间序列的第一个数据点，在本例中，它将是t0中的B的数据。我们请求A在t0处一个值，但这里没有任何数据。我们知道在t0 + 10s中有A的数据，但由于之前没有任何值，我们无法猜测它会是什么。因此，我们只需返回B的值。我们要求从时间序列B中得到t0 + 10s的值，但没有。但是B知道t0 + 20有一个值我们在t0上有一个值所以我们现在可以计算一下t0 + 10s。线性插值的公式为y = y0 +(y1 - y0)(x - x0)/(x1 - x0))，对于级数B，y0 = 10,y1 = 20,x = t0 + 10s(或10)，x0 = t0(或0)，x1 = t0 + 20(或20)。因此，我们有y = 10 +(20 - 10)(10 - 0)/(20 - 0)，这将减少到y = 10 + 10 (10 / 20)，进一步减少到y = 10 + 10 。5和y = 10 + 5。因此B在t0+10s会给我们一个估计值：15。 迭代在每一个时间戳上继续进行，每个系列的数据点都被作为查询的一部分返回。使用sum聚合器的结果系列将是这样的： 更多的例子:对于图形趋势的我们有以下的例子。在OpenTSDB中记录了一个名为m的虚构的度量。“sum of m”是顶部的蓝色线,由start = 1h - ago&amp;m = sum查询而来。红色的线由host = foo得来， 绿线由host = bar得来。 如果你把红线和绿线叠起来，你就会得到蓝线。在任何离散时间点上，蓝线都有一个值等于红线的值和当时绿线的值。如果没有插值，你就会得到一些不太直观的东西，这很难理解，而且也没有那么有用： 注意蓝色的线在18:46:48处下降到绿色的数据点。不需要数学家，也不需要上高级的数学课，就可以看到，插值是必要的，以恰当地聚合多个时间序列，并得到有意义的结果。 目前，OpenTSDB主要支持线性插值(有时缩短称为“lerp”)和一些聚合器，这些聚合器将简单地替代零或最大或最小值。对于那些想要添加其他插值方法的人来说，插件是受欢迎的。只有在查询时发现多个时间序列时，才会在查询时执行插值。许多指标收集系统在写中插值，这样你的原始值就不会被记录下来。OpenTSDB存储您的原始值，并允许您随时检索它。下面是来自邮件列表的另一个稍微复杂的例子，描述了多个时间序列是如何按平均聚合的： 三角形的粗蓝线是一个与多个时间序列的avg函数的集合，如每个查询开始= 1h - ago&amp;m = avg:duration_seconds。正如我们所看到的，产生的时间序列在所有底层时间序列的每一个时间戳上都有一个数据点，数据点是通过在那个时间戳中获取所有时间序列的平均值来计算的。这也适用于方形紫色时间序列的独立数据点，它在下一次数据点之前临时提高了平均值。 Note：聚合函数根据输入数据点返回整数或双值。如果两个源值都是整数，那么结果计算将是整数。这意味着任何由计算产生的分数值将会被砍掉，不会发生四舍五入。如果两个数据点都是浮点值，那么结果就是浮点数。但是，如果启用了downsampling或rates，结果将始终是浮点数。 下采样如上所述，插值是处理丢失数据的一种方法。但一些用户讨厌线性插值是对数据欺骗的这种方法，因为它产生了虚值。相反，另一种处理不一致值的方法是通过downsampling。例如，如果消息源每分钟都报告一个值，但它们在那一分钟内发生了倾斜，那么就为所有数据源数据的每个查询提供1分钟的下行采样。这将导致在每个时间序列中对相同时间戳的值进行快照，以避免插值。当一个采样桶丢失一个值时，仍然会发生插值。有关避免插值的细节和示例，请参阅downsampling。Note：一般来说，将多个时间序列合并到一个查询中，这是一个很好的方法。 支持的聚合器下面是OpenTSDB中可用的聚合函数的描述。注意，有些应该只用于分组，有些则用于下采样。 *查看维基百科上的文章。对于高基数计算，使用估计的百分位数可能更有效。 Avg计算整个下行取样桶或跨多个时间序列的所有值的平均值。这个函数将在整个时间序列中执行线性插值。它对于查看度量指标很有用。 Note： 即使计算通常会导致浮点值，如果数据点被记录为整数，一个整数将被返回失去一些精度。 Count返回存储在系列或范围中的数据点的数量。当用于聚合多个系列时，将会替换零。当使用下采样时，它将反映每个下样桶中数据点的数量。当在组中使用聚合时，反映在给定时间内的时间序列的数量。Dev计算一个bucket或时间序列的标准偏差。这个函数将在整个时间序列中执行线性插值。它对于查看度量指标很有用。Note：即使计算通常会得到浮点值，如果数据点被记录为整数，整数返回失去一些精度。Estimated Percentiles使用选择算法计算各种百分比。这些对于有许多数据点的系列来说是有用的，因为一些数据可能被排除在计算之外。当用于聚合多个系列时，函数将执行线性插值。有关详细信息,请参阅维基百科。实现是通过Apache数学库实现的。First &amp; Last这些聚合器将在向下采样间隔中返回第一个或最后一个数据点。例如，如果一个下样桶由系列2、6、1、7组成，那么第一个聚合器将返回1，最后将返回7。请注意，这个聚合器只对downsamplers有用。 警告： 当作为集合聚合器使用时，结果不确定，因为从存储中检索的时间序列和在内存中保存的时间序列不一致。 Max返回所有时间序列中最大的数据点，或者在一个时间跨度内。这个函数将在整个时间序列中执行线性插值。它对于查看度量指标的上界很有用。MimMin“maximum if missing minimum” 函数将只返回所有时间序列中或在时间跨度内最小的数据点。该函数不会执行插值，相反，它将返回值为所缺少的数据类型的最大值。Long.MaxValue为整数值最大值或Double.MaxValue为浮点型最大值。有关详细信息，请参见原始数据类型。它对于查看度量指标的下界是有用的。MimMax“minimum if missing maximum”函数将只返回最大的数据点，从所有的时间序列，或者在时间跨度内返回。该函数不会执行插值，相反，它将返回值为缺失值的数据类型的最小值。Long.MinValue用于整数值最小值或Double.MinValue表示浮点类型最小值。有关详细信息，请参见原始数据类型。它对于查看度量指标的上界很有用。Min只返回所有时间序列中或时间段内最小的数据点。这个函数将在整个时间序列中执行线性插值。它对于查看度量指标的下界是有用的。None跳过group by聚合。这个聚合器对于从存储中获取原始数据非常有用，因为它将为每次匹配过滤器的序列返回一个结果集。注意，如果使用downsampler，查询将抛出异常。Percentiles计算不同的百分位数。当用于聚合多个系列时，函数将执行线性插值。实现是通过Apache数学库实现的。Sum计算所有时间序列，或者在抽样的时间范围内的所有数据点的和。这是GUI的默认聚合函数，因为它通常在组合多个时间序列(比如测量或计数器)时是最有用的。当数据点未能对齐时，它执行线性插值。如果你有一组不同的值，你想要求和，你不需要插值，看看zimsumZimSum计算所有数据点在指定时间戳的所有时间序列或在时间范围内的总和。这个函数不执行插值，而是用一个0代替丢失的数据点。这在处理离散值时很有用。 Listing Aggregators使用TSD上运行的HTTP API，用户可以查询 /api/aggregators 来获得在TSD上实现的聚合器列表。]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb原理-查询过滤器]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb%E5%8E%9F%E7%90%86-%E6%9F%A5%E8%AF%A2%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[任何数据库系统的一个关键功能是使用某种形式的过滤启用完整数据集的抓取子集。OpenTSDB自版本1.x 以来提供了过滤，从2.2开始具有扩展的功能。过滤器目前只能在标签值上操作。这意味着任何指标(Metric)和标记键(tag name)都必须精确地指定，因为它们在获取数据时出现在数据库中。 示例数据：在下面解释每个过滤器时，使用了以下数据。它由一个单一的度量标准组成，多个时间序列定义在不同的标签上。以T1为例，只给出一个数据点： TS# Metric Tags Value @ T1 1 sys.cpu.system dc=dal host=web01 3 2 sys.cpu.system dc=dal host=web02 2 3 sys.cpu.system dc=dal host=web03 10 4 sys.cpu.system host=web01 1 5 sys.cpu.system host=web01 owner=jdoe 4 6 sys.cpu.system dc=lax host=web01 8 7 sys.cpu.system dc=lax host=web02 4 分组分组，也称为group - by，是将多个时间序列使用所需的聚合函数和过滤器组合成一个的过程。默认情况下，OpenTSDB以Metric的形式对所有内容进行分组，如果查询返回10个时间序列，并使用一个sum的聚合器，那么所有10个序列将会随着时间的推移一起添加到一个值。有关时间序列如何合并的详细信息，请参见聚合篇章。为了避免在没有任何聚合的情况下对每个底层时间序列进行分组和获取，请使用版本2.2中none聚合器。或者，您可以在OpenTSDB 2.2之后的版本每个过滤器基础上禁用分组。请参阅API文档，了解如何做到这一点。 OpenTSDB 1.x - 2.1在最初的OpenTSDB发行版和最多2.1版本中，只有两种类型的过滤器可用，并且它们被隐式配置成分组。允许的两个操作符是: * - 星号(或通配符)将返回一个单独的结果，以检测每个唯一的标记值。如果标签密钥主机与web01和web02配对，那么将会有两个组返回，一个在web01上，一个在web02上。 | - 管道符(或文字) 将只返回指定的标记值的单独结果。也就是说，它将仅匹配具有给定标记值的时间序列和每个匹配的组。 每个查询可以提供多个过滤器，结果使用And合并。这些过滤器仍然可以在2.x及之后的版本使用。 例子下面的示例使用v1 HTTP URI语法，其中的m参数由聚合器、冒号组成，然后用等号分隔的方括号标记过滤器： Example 1: http://host:4242/q?start=1h-ago&amp;m=sum:sys.cpu.system{host=web01} Time Series Included Tags Aggregated Tags Value @ T1 1, 4, 5, 6 host=web01 16 在这种情况下，集合的Aggregated集将是空的，因为时间序列4和5都有与整个集合不相同的Tag。 Example 2: http://host:4242/q?start=1h-ago&amp;m=sum:sys.cpu.system{host=web01,dc=dal} Time Series Included Tags Aggregated Tags Value @ T1 1 host=web01,dc=dal 3 Example 3: http://host:4242/q?start=1h-ago&amp;m=sum:sys.cpu.system{host=*,dc=dal} Time Series Included Tags Aggregated Tags Value @ T1 1 host=web01,dc=dal 3 2 host=web02,dc=dal 2 3 host=web03,dc=dal 10 这次我们为主机提供了*，并为dc提供了显式匹配。这将在host这个tag name上进行分组，并对每个host返回一个value，在本例中为3个序列。 Example 4: http://host:4242/q?start=1h-ago&amp;m=sum:sys.cpu.system{dc=dal|lax} Time Series Included Tags Aggregated Tags Value @ T1 1, 2, 3 dc=dal host 15 6, 7 dc=lax host 12 这里使用的|操作符只匹配查询中提供的dc标记键的值。因此TSD将与这些值一起组合任何时间序列。host 标签被移动到聚合标签列表，因为集合中的每一个时间序列都有一个host tag，并且有多个值。 警告：因为这些过滤器是有限的，如果用户编写了# 1、# 4和# 5这样的时间序列，那么可能返回预料之外的结果，因为这些时间序列可能有一个普通的标记，但是会有不同的附加标记。这个问题有点用2.3版本的Explicit Tags来解决。 OpenTSDB 2.2在OpenTSDB2.2中，引入了一个更加灵活的过滤器框架，能够禁用grouping并且添加额外的过滤器类型如正则表达式和模糊匹配。过滤框架可以被插入到外部系统中，例如资产管理或供应系统。 当处理时，相同的tag key上允许多个过滤器，它们通过AND连接。例如，如果我们有两个过滤器host = literal_or(web01) AND host = literal_or(web02)，查询将总是空的。如果两个或更多的过滤器包含在相同的标记键上，并且一个已经被启用了，而另一个则没有，那么gourp by将作用于这个标记键上的所有过滤器。 警告：某些类型的筛选器可能会导致查询执行得慢一些，尤其是regexp、通配符查询和大小写不敏感的过滤器。在从存储中获取数据之前，过滤器被处理以创建一个基于uid的数据库过滤器，因此使用区分大小写的文本或过滤器总是比regexp更快，因为我们可以将字符串解析为uid，并将这些字符串发送到存储系统进行过滤。相反，如果您要使用pre、post或infix过滤的regex或通配符，则TSD必须使用标记键UID从存储中检索所有行，然后针对每个惟一行，将UID解析回字符串，然后在结果上运行过滤器。此外，带有大量文字的筛选器将被处理后存储，以避免为备份存储创建一个巨大的过滤器。这个限制默认值为4096，可以通过tsd.query.filter.expansion_limit配置。 Explicit Tags在2.3和之后，如果您知道给定的metric的所有tag name可以通过使用explicitTags特性得到极大的提高查询效率。这种标签有两个好处：1、对于具有高基数的度量，后端可以切换到更有效的查询，以从存储中获取较小的数据子集。(尤其是2.4)2、对于带有不同标记的度量，可以使用它来避免在最终结果中聚合不需要的时间序列。显式标记将设计一个底层存储查询，它只使用给定的标记键来获取这些行。这样，数据库就可以跳过无关的行，在更短的时间内响应请求。 例子下面的例子使用了v2 HTTP URI语法，其中的m参数由聚合器、冒号、explicit_tags URI标志组成，然后在括号中以相等的符号分隔的度量和标记过滤器。 Example 1: http://host:4242/q?start=1h-ago&amp;m=sum:explicit_tags:sys.cpu.system{host=web01} Time Series Included Tags Aggregated Tags Value @ T1 4 host=web01 1 这解决了标签键不一致的问题，让我们只能选择时间序列# 4。 Example 2: http://host:4242/q?start=1h-ago&amp;m=sum:explicit_tags:sys.cpu.system{host=*}{dc=*} Time Series Included Tags Aggregated Tags Value @ T1 1, 6 host=web01 dc 11 2, 7 host=web02 dc 6 3 host=web03,dc=dal 10 该查询使用v2 URI语法，以避免在dc标记键上分组，将其放入第二组大括号中。这允许我们过滤出来都有host和dc标签的时间序列，并在host标签上分组，跳过了第4、5条记录。 Note：在使用HBase(0.98和更高版本)或Bigtable时，请确保tsd.query.enable_fuzzy_filter(默认启用)启用了。将一个特殊的过滤器交给后端，它可以跳过我们需要查询的行，而不是遍历每个行键并比较正则表达式。 Note：在2.4中，TSDB将会发出多个get请求，而不是使用扫描器。这可以通过多种因素减少查询时间，特别是在高基数时间序列中。但是过滤器必须只包含literal_or`’s。 Built-in 2.x Filters下面的列表是OpenTSDB内置的过滤器。附加的过滤器可以作为插件加载。每个标题都是在URI或JSON查询中使用的过滤器类型。在编写URI查询时，过滤器名称是在标记键的等号并将过滤器值放置在括号中之后，将过滤器名称放置在此。如主机= { regexp(web[0 - 9]+ .lax.mysite.com)}。该JSON中，简单地使用筛选器名称作为type参数，filte作为r过滤器值参数。{ “type”: “regexp”, “filter”: “web[0-9]+.lax.mysite.com”, “tagk”: “host”, “groupBy”: false} 下面的例子使用了URI语法。literal_or取一个文本值或|管道分隔值列表，匹配区分大小写，并返回任何时间序列。这是一个非常高效的过滤器，它可以将字符串解析为uid，并将其发送到存储层进行预过滤。在SQL中，这类似于In或=谓词。 Examples ● host=literal_or(web01|web02|web03) In SQL: WHERE host IN (&apos;web01&apos;, &apos;web02&apos;, &apos;web03&apos;) ● host=literal_or(web01) In SQL: WHERE host = &apos;web01&apos; iteral_or与literal_or相同，但不区分大小写。请注意，这并不像literal or那样高效，它必须从存储中处理所有的行。 not_literal_or区分大小写的literal_or,但返回不符合给定值列表的系列。它可以通过存储预先处理。 wildcard提供大小写敏感的postfix，前缀，infix和multi - infix过滤。通配符是星号(星号)*。可以使用多个通配符。如果只给出了星号，那么过滤器可以有效地返回任何包含标记键的时间序列(并且是一个可以预先处理的高效过滤器)。在SQL land中，类似于LIKE谓词，具有更大的灵活性。 Examples ● host=wildcard(*mysite.com) In SQL: WHERE host=&apos;%mysite.com&apos; ● host=wildcard(web*) ● host=wildcard(web*mysite.com) ● host=wildcard(web*mysite*) ● host=wildcard(*) This is equivalent to the v1 basic group by operator and is efficient. iwildcard与wildcard相同，但不区分大小写。 regexp使用POSIX兼容的正则表达式从存储中获取的过滤器。该过滤器使用Java内置的正则表达式操作。根据使用的查询方法，要小心避开特殊字符。 Examples ● regexp(web.*) In SQL: WHERE host REGEXP &apos;web.*&apos; ● regexp(web[0-9].mysite.com) Loaded Filters要在OpenTSDB 2.2中显示加载的过滤器，然后调用HTTP / api / config / filters接口。这将列出加载的插件以及描述和示例用法。 Plugins随着开发人员添加插件，我们将在这里列出它们。要开发一个插件,只需扩展net.opentsdb.query.filter.TagVFilter类，在插件文档中创建JAR，并将其放入插件目录。开始时，TSD将搜索插件并加载它。如果执行时出现了错误，则TSD将不会启动并记录异常。]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb原理-日期和时间]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb%E5%8E%9F%E7%90%86-%E6%97%A5%E6%9C%9F%E5%92%8C%E6%97%B6%E9%97%B4%2F</url>
    <content type="text"><![CDATA[OpenTSDB在查询数据时支持一些日期和时间格式。支持通过GUI、CliQuery工具或HTTP API提交查询。每个查询都需要一个 start time 和一个可选的end time。如果未指定结束时间，则使用TSD正在运行的系统的当前时间。 相对时间如果您不知道请求的确切时间戳您可以提交过去相对于TSD正在运行的系统的时间。相对时间按照格式 -ago 格式 &lt;amount&gt; 是时间的数量单位； &lt;time unit&gt;是时间的单位,比如小时,天,等。 例如,如果我们提供1h-ago而不指定结束时间。查询将返回数据从1小时前到当前时间。可能的时间单位包括: ● ms - Milliseconds ● s - Seconds ● m - Minutes ● h - Hours ● d - Days (24 hours) ● w - Weeks (7 days) ● n - Months (30 days) ● y - Years (365 days)Note：相对时间不考虑闰秒、闰年或时区。他们只是简单地计算过去从当前时间的秒数。 绝对的Unix时间在内部，所有数据都与Unix(或POSIX)样式时间戳关联。Unix时间定义为自1970年1月1日UTC时间以来的秒数。时间戳被表示为一个正整数，如1364410924，代表ISO 8601:2013 - 03 - 27t19:04 . 4z。 由于调用OpenTSDB中的数据需要一个Unix时间戳，所以在查询中支持此格式是有意义的。因此，您可以为查询的开始或结束时间提供一个整数。使用Unix时间戳的查询也可以简单地追加三位数来支持毫秒精度。例如，提供1364410924000的起始时间和1364410924250的结束时间将返回250毫秒的窗口内的数据。毫秒时间戳也可以提供一个周期，将秒从毫秒分隔为1364410924.250。任何带有13(或14)字符的整数都将被当作毫秒级的时间戳。任何少于等于10个字符的表示秒。毫秒只能提供3位数的精度。如果你的工具输出超过3位小数，你必须截断或绕过数值。 绝对的格式时间由于在您的头脑中计算Unix时间非常困难，OpenTSDB还支持人类可读的绝对日期和时间。支持的格式包括： ● yyyy/MM/dd-HH:mm:ss ● yyyy/MM/dd HH:mm:ss ● yyyy/MM/dd-HH:mm ● yyyy/MM/dd HH:mm ● yyyy/MM/ddyyyy代表的是一个四位数的年份，例如2013年。MM代表了从01到12月的一个月。dd表示从当前月份的第几天，从01开始。HH表示24小时格式的一天，从00到23。mm代表从00到59的分钟，ss代表从00到59的秒。每个月、日、小时、分钟和秒数都必须以0表示，例如本月的第5天必须为05。在没有时间的情况下提供数据时，系统将假定给定的一天的午夜。例如2013 / 01 / 23 - 12:50:42 或 2013 / 01 / 23。格式化时间由主机的默认时区转换为UTC。HTTP API查询可以接受用户提供的时区来覆盖本地区域。Note：在使用CliQuery工具时，您必须使用将日期与时间分隔开来的格式。这是因为命令行在空格上被分割，所以如果您在时间戳中放置一个空格，那么它将不能正确地解析执行。 时区当转换人类可读的时间戳时，OpenTSDB将从在TSD运行的系统中配置的时区转换为UTC。虽然许多服务器配置为UTC，但我们建议运行OpenTSDB的所有系统都使用UTC，但有时也使用一个本地时区。通过查询字符串到HTTP API的查询可以指定一个带有时区标识字符串的tz参数，格式适用于运行TSD的系统的本地化设置。例如，我们可以指定tz = America / Los_Angeles将时间戳从洛杉矶本地时间转换为UTC。或者，如果您无法更改系统时区，则可以通过配置文件配置 tsd.core.timezone。]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb-Metric和时间]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb%E5%8E%9F%E7%90%86-Metric%E5%92%8C%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97%2F</url>
    <content type="text"><![CDATA[OpenTSDB是一个时间序列数据库。时间序列是随时间变化的某种特定度量的一系列数据点。每一个时间序列由一个度量加上一个或多个与这个度量相关的tag (我们将在一个部分中涉及tag)。度量指标是您希望随时间追踪的任何特定数据(例如对Apache托管文件的点击)。 OpenTSDB也是一个数据绘图系统。OpenTSDB与其他系统有一点不同。我们将在下面更详细地讨论绘图，但现在很重要的一点是，对于OpenTSDB，任何给定的图的基础都是Metric。它需要这个Metric找到所有的时间序列，根据你选择的时间范围，将这些时间序列聚合在一起(例如，把它们相加)，并绘制结果。这个绘图机制是非常灵活和强大的，你可以做更多的事情，但是现在我们来讨论一下时间序列的关键，这就是Metric。 在OpenTSDB中，一个metric以字符串命名，如http.hits. 为了能够将所有不同的值存储在这个Metric的所有地方，当您将数据发送到TSD时，您可以用一个或多个标记标记数据。TSD存储时间戳、值和标签。当您想要检索这些数据时，TSD将检索您提供的时间跨度的所有值，可选地使用标签过滤器，将所有这些值聚合在一起，并根据时间绘制这个值的图表。 这里有一堆东西我们已经介绍过了。为了帮助您理解Opentsdb是如何工作的，我将从一个典型的例子开始。 Web服务器访问、负载示例假设您有一堆web服务器，您希望跟踪两件事情：对web服务器的访问和系统的负载平均值。让我们用度量名称来表示这个。对于平均负载，我们叫它proc.loadavg.1min(因为在Linux上，您可以通过查看 /proc/loadavg轻松获得这些数据)。对于许多web服务器来说，有方法可以向web服务器请求一个计数器，它可以在服务器启动时向服务器显示命中次数。这是一个方便的计数器，这个Metric我们将命名为 http.hits。我之所以选择这两个例子有两个原因。首先，我们将了解OpenTSDB如何轻松地处理这两个计数器(随时间增加的值，除非它们在重新启动/重新启动或溢出时被重新设置)，以及它如何处理上下波动的正常值，比如负载平均值。OpenTSDB的一个巨大优势是，您不需要对计数器进行任何速率计算，它已为你做了该工作。第二个原因是，我们还可以向您展示如何在相同的范围内划分两个不同的度量标准。 第一个数据点如果不深入了解收集器如何向TSD发送数据，那么您就编写一个收集器，定期向TSD发送这些数据的当前值。因此，TSD可以聚合来自多个主机的数据，用“host”标签标记每个值。如果你有web服务器A,B,C，等等，它们会周期性地向TSD发送类似的东西： put http.hits 1234567890 34877 host=A put proc.loadavg.1min 1234567890 1.35 host=A 这里的“1234567890”是当前的时间(date+ % s)秒。下一个数字是这个时候的度规的值。这是来自主机A的数据，所以它被标记为host = A。主机B的数据将被标记为host = B，等等。随着时间的推移，您将获得一系列存储在OpenTSDB中的时间序列。 第一个图表现在，让我们回顾一下我们刚开始讨论的内容。时间序列是一些特定度量(及其标记)的一系列数据。对于这个示例，每个主机向TSD发送两个时间序列。如果你有3个盒子，每一个发送这两个时间序列，TSD将收集和存储6个时间序列。现在有了数据，让我们开始绘图。要绘制HTTP点击，您只需访问UI并输入http.hits按您的Metric名称命中，并输入时间范围。点击“Rate”按钮，因为这个特定的度量是一个速率计数器。随着时间的推移，您的web服务器的HTTP点击率有一个图表。 聚合UI的默认值是将每个主机的每个时间序列加在一起(sum)。这意味着，TSD将使用这个度量(主机= A、B、C)的三次时间序列，并将它们的值加在一起，以在给定时间内得到所有web服务器的总命中值。 注意，你不需要为此请求三次，TSD将会计算出来。所以，如果每一个主机在某个时间点上每秒钟服务1000次，那么图就会显示3000次。如果您想要显示每个web服务器所服务的点击量是多少呢？两种方式。如果您只关心每个web服务器所服务的平均值，只需将聚合器方法从sum改为avg，您也可以尝试其他(max,min)来查看最大值或最小值。更多的聚合函数在works中(percentiles，等等)。这是在每个时间间隔的基础上完成的，如果在某个时间点你的 qps 50，而其他的则是100，后来一个不同的网络服务器是每秒50和100，这两点的Min值为50。换句话说，它没有计算出哪个时间序列是最小值，只展示了主机比较的值。另一种方法是查看每个web服务器的服务数量，正是我们要查找的标签字段（tag）。 下采样为了减少返回的数据池数量，可以指定一个下采样间隔和方法，如1h - avg或1d - sum。这也很有用(比如在使用max和min时)，在给定的时间内找到最佳和最坏的数据。down采样最有效的方法是使图形化阶段变得不那么密集，可读性更强，特别是在绘制比屏幕像素更多的数据时。 Tag过滤器在UI中，您将看到TSD已经填充了一个或多个“Tags”，第一个是host。TSD在这里说的是，在这个时间范围内，它看到数据被标记为一个host标记。您可以对图表进行筛选，使其仅仅绘制一个host的值。如果您在host行中填写A，则只需在host=A的时间内绘制值。如果您想要给出一个主机列表，请填写由管道符号分隔的主机列表，例如：A | B。这将为您提供两个图，而不是一个，一个为A，一个为b。最后，您还可以指定特殊字符 *，这意味着为每个主机绘制一条线。 添加更多Metric所以，现在你有了你的网络点击量。这与平均负荷有什么关系?在这张图上，点击“+”按钮，为这个已有的图形添加一个新的Metric。输入proc.loadavg.1min 为Metric，点击“Right Axis”，Y轴分别按比例缩放，右侧为其标签。确保“Rate”是不能点击的，因为负载平均不是一个计数器度量。你瞧，现在您可以看到web点击率的变化如何影响系统的负载平均值。 更有趣的内容假设您的服务器实际上运行两个web服务器，比如静态内容和动态内容。与其创建另一个度量，不如为http.hits这个Metric创建Tag。你的收集器发送的东西像： put http.hits 1234567890 34877 host=A webserver=static put http.hits 1234567890 4357 host=A webserver=dynamic put proc.loadavg.1min 1234567890 1.35 host=A 为什么要这样做而不是创建另一个度量?那么，如果有时候你关心的是使用HTTP的总点击量，有时你关心的是如何比较静态和动态命中的关系呢？有了标签，很容易。使用这个新的标记，您将看到在绘制这个度量时，UI中出现了一个webserver标签。你可以把它空着将聚合两个值到一个图中(根据你的聚合器设置),你可以看到总hits，或者你可以做网络服务器= 列出每个静态和动态实例。您甚至可以更深入地指定webserver = 和host = *来查看完整的故障。 何时创建Metric?现在，您不能将两个指标组合成一条主线。这意味着您Metric是最大的聚合点。如果您想下钻某个度量Metric，使用标签。 Tags vs. Metrics度量标准应该是一个特定的东西，比如“Ethernet Packets”，但不能被再分解成一个特定的实例。一般来说，您不希望收集像net.bytes.eth0，net.bytes.eth1等等这样的Metric。而是 net.bytes和标签eth0如：iface=eth0。不要麻烦地创建单独的“in”和“out”度量。添加标签 direction = in或 direction= out。这样就可以很容易地在一个框中看到的总网络活动，而不必绘制一堆度量。而且这样可以让您灵活地进行下钻，并显示特定iface的活动，或者只是一个特定的direction。 Counters and Rates如果某Metric是一个计数器，或者是一个自然的速率，在发送到TSD之前，不要把它转换成速率。有两个主要原因。首先，执行自己的速率计算、重置/溢出等处理是愚蠢的，因为TSD可以为您做。您也不必担心根据一个稍微不准确或更改的样本时间间隔来得到单位秒计算的正确性。第二，如果你丢失了一个数据点或者更多，如果你发送的是当前的计数器值，那么你就不会丢失数据。TSD中的黄金法则是，如果源数据是一个计数器(某些计数器是 / proc或SNMP)，那么保持这种状态。不要转换它。如果您正在编写自己的收集器(比方说，计算一个对特定的错误消息使用 tail- f 的频率)，不要每次都重置您的计数器。让TSD为你做这项工作。 Tags are your Friend在任何小环境中，您可能都有集群或一组机器做同样的事情，随着时间的推移发生变化也没关系。当您将数据发送到TSD以传递这个集群信息时，只需使用一个标记。将cluster = webserver 添加到所有web服务器发送的数据点中，并为所有数据库添加cluster = db等等。 现在，当您为webserver集群设置CPU活动时，您会看到它们都聚合成一个图表。然后，假设您添加了一个webserver，甚至将它从webserver更改为数据库。您所要做的就是确保当它的角色发生变化时，发送正确的标签，而现在这个box中的CPU活跃度指向正确的集群。更重要的是，你所有的历史数据都是正确的!这是OpenTSDB的真正力量。您不仅不会像基于rrd的系统一样，在时间上不丢失数据的分辨率，而且历史数据不会随着您的box改变而丢失。也不必将集群分开，不会在dashborad中感知到该事件。 Precisions on Metrics and Tags在数据点上允许标签的最大数量是由一个常量(const . max_numtags)默认定义是8。度量名称、标记名称和标记值必须由字母数字字符、破折号“-”、下划线“”、句号”。“以及斜杠”/“。]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb-读数据]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb-%E8%AF%BB%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[OpenTSDB提供了许多方法来提取、操作和分析数据。数据可以通过CLI工具(HTTP API)查询，并被展示为GnuPlot图。诸如Grafana和Bosun这样的开源工具也可以访问TSDB数据。使用OpenTSDB的基于tag的系统查询可能有点复杂，所以请阅读本文并检查以下页面以了解更深的信息。此章节上的示例查询遵循HTTP API格式。 ● Understanding Metrics and Time Series ● Dates and Times ● Query Filters ● Aggregation ● Downsampling ● Query Performance ● Query Examples ● Query Details and Stats这个页面提供了典型的查询组件的概览。有关每个组件的详细信息，请参见文本中引用的页面或上面的内容表。 查询组件OpenTSDB提供了许多工具和端点，允许各种随时间而变化的查询规范。最初的语法允许进行简单的筛选、聚合和下行采样。后来的版本增加了对函数和表达式的支持。一般来说，每个查询都有以下组件： Time(时间)绝对时间戳以格式化的或Unix整数时间戳支持。相对时间可用于刷新图表。目前，所有查询都能够覆盖单个时间跨度。在未来，我们希望提供一个偏移查询参数，允许在不同的时间段内聚合或绘制一个度量，比如比较上个星期到一年前。查看日期和时间篇章了解详细的内容。虽然OpenTSDB可以用毫秒级的精确度存储数据，但大多数查询将返回具有秒级别的数据，以提供对现有工具的向后兼容性。除非使用查询指定了一个向下的采样算法，否则数据将以查询中指定的相同聚合函数自动为1秒采样。这样，如果将多个数据点存储为一个给定的秒，它们将被聚合并以正常的查询返回。若要以毫秒级别提取数据，请使用/ api /query 并指定msResolution(ms也可以，但不推荐)JSON参数或查询字符串标记，它将绕过采样(除非指定)，并在Unix纪元毫秒解析中返回所有时间戳。此外，scan 命令行将返回写入存储的时间戳。 Filter(过滤器)每个时间序列由一个度量和一个或多个tag name/value对组成。在OpenTSDB中，过滤器被应用于tag value(此时TSDB不提供对metric或tag name的过滤)。因为过滤器在查询中是可选的，如果您只请求度量的名称，那么将在聚合的结果中返回每个数值或tag value。过滤器类似于SQL中的WHERE子句。例如，如果我们有一个存储的数据集: sys.cpu.user host=webserver01,cpu=0 1356998400 1 sys.cpu.user host=webserver01,cpu=1 1356998400 4 sys.cpu.user host=webserver02,cpu=0 1356998400 2 sys.cpu.user host=webserver02,cpu=1 1356998400 1 并制作一个简单的查询，其中包含必须的条件start time、aggregator和metric，例如:start = 1356998400&amp;m = sum:sys.cpu.user，我们将在1356998400上得到一个累加值8和所有4个时间序列的值。如果我们想要下钻一个特定的时间序列，我们可以使用过滤器。例如，我们可以在主机标签上通过:start = 1356998400&amp;m = sum:sys.cpu.user { host = webserver01 }。该查询将返回一个值5，只包含时间序列，其中主机= webserver01。要深入到特定的时间序列，您必须包含该系列的所有标记，例如查询start = 1356998400&amp;m = sum:sys.cpu。用户{主机= webserver01 cpu = 0 }将返回1。Note：不一致的标记在查询时可能导致意外的结果。详情请参阅write data章节。还可以看显示标记。有关详细信息，请阅读查询过滤器文档。 Aggregation(聚合)OpenTSDB的一个强大功能是能够在一组数据点上执行多个时间序列的动态聚合。原始数据总是可以在存储中使用，但我们可以以有意义的方式快速提取数据。聚合函数是将两个或多个数据点合并为一个单一值的方法。Note：OpenTSDB在默认情况下聚合数据，并要求每个查询都有一个聚合运算符。每个聚合器都必须处理不同时间戳的丢失或数据点。这是通过插值执行的，如果用户不知道TSDB正在做什么，就会在查询时导致意外的结果。有关详细信息,请参阅Aggregation Downsampling（下采样）OpenTSDB可以输入大量的数据，甚至是一个时间序列中每秒钟一个数据点。因此，查询可能返回大量的数据点。访问带有大量API点的查询结果可能占用带宽。高频率的数据很容易压垮Javascript图形库，因此选择使用GnuPlot。GUI创建的图形很难读取，导致粗线，如下图所示： 下采样可以在查询时使用，以减少返回的数据点的数量，这样您就可以从图表中提取出更好的信息，或者在连接上传输更少的数据。下采样需要一个聚合函数和一个时间间隔。聚合函数用于在指定的区间内用适当的数学函数计算新的数据点。例如，如果使用聚合sum，那么将在区间内的所有数据点汇总成一个单独的值。如果选择avg，则返回间隔内所有数据点的平均值。使用下采样，我们可以清除之前的图表，以获得更有用的东西： For details, see Downsampling. Rate(速率)一些数据源返回值作为不断递增的计数器，一个例子是一个网站点击计数器。当您启动web服务器时，它可能有一个0的计数器。五分钟后，价值可能是1024。再过五分钟，可能是2048。计数器的图形将是一条直线，向右倾斜，并不总是很有用。OpenTSDB提供了一个速率转换函数，它可以计算随时间变化的值的速率。这将把计数器转换成带尖刺的行，以便在活动发生时显示您，并且可以更有用。 速率是值的一阶导数。它被定义为(v2 - v1)/(t2 - t1)，时间为秒。因此，你会得到每秒变化的速率。目前，毫秒值的变化速率默认为每秒计算。OpenTSDB 2.0为特殊单调递增的计数器数据处理提供了支持，包括设置“滚动”值并抑制异常波动的能力。当在一个查询中指定了counterMax时，如果一个数据点接近这个值，并且在小于之前的点上，最大的值将被用来计算一个准确的速率给出两个点。例如，如果我们将一个整数计数器记录为2个字节，那么最大值将是65,535。如果t0的值是64000，而t1的值是1000，那么每秒生成的速率将被计算为- 63000。但是我们知道，它很有可能是翻转过来的，所以我们可以将最大值设置为65535，现在计算将是65535 - t0 + t1给我们2535。 在计数器中跟踪数据的系统在重新启动时通常会恢复到0，当这种情况发生时，当使用最大计数器特性时，我们会得到一个虚假的结果。例如，如果计数器在t0上达到2000，有人重新启动服务器，那么下一个值可能是t1的500。如果我们把最大值设为65535，结果就是65535 - 2000 + 500，给我们64035。如果正常速率是每秒几点，那么这个特殊的峰值，在30秒之间，会产生一个2134.5的速率峰值!为了避免这种情况，我们可以设置resetValue，当速率超过这个值时，返回0的数据点，以避免在任意方向上出现峰值。 操作顺序理解操作的顺序是很重要的。当返回查询结果时，以下是处理的顺序： Filtering Grouping Downsampling Interpolation Aggregation Rate Conversion Functions Expressions 原文地址：http://opentsdb.net/docs/build/html/user_guide/query/index.html]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb-写数据]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb-%E5%86%99%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[你可能立刻想写一批数据到TSD中，但并不能充分发挥OpenTSDB的优势和灵活性。你要想停下来思考一下如何命名schame，然后，通过telnet或者http apis或者其它存在的工具如“tcollector”向OpenTSDB推送数据。 命名schame许多metrics的管理者为其时间序列提供单一命名。举个例子:系统管理员习惯于RRD风格的系统可能会这样为metric命名webserver01.sys.cpu.0.user。这个名字告诉我们，这是一系列在webserver1主机上的0cpu的用户使用记录。如果你只接收指定web server的cpu用户使用时间，那么这么写是没问题的。但是如果该webserver有64颗cpu并且你想计算它们的平均使用时间呢？有些系统允许你使用诸如webserver01.sys.cpu..user的通配符来读取64个不同的文件并且聚合其结果。或者你可以记录一个新的度量名为webserver01.sys.cpu.user.all来代表同样的聚合计算，但这意味着你必须记录“64+1”次记录。如果你有上千个web server并且你想计算他们的cpu平均使用时间呢？你可以使用如 .sys.cpu.*.user并且系统打开64,000个文件，聚合计算并返回结果。或者你可以起一个进程预处理这些数据并存入webservers.sys.cpu.user.all. Opentsdb引入标签来实现不同的处理方式，每个系列的数据都有一个metric名称，但它更通用，有些东西可以在时间序列上共享，换句话说，标签name/value对的结合使用似的查询更灵活，聚合计算高效。 注意：在Opentsdb中每个时间序列必须有至少一个标签。用刚才得metric webserver01.sys.cpu.0.user举例，在OpenTSDB中，可能为会变成：sys.cpu.user host=webserver01, cpu=0。现在，如果我们想查询特定的cpu核，我们可以这样写查询：sum:sys.cpu.user{host=webserver01,cpu=42}.如果想查询所有核，我们可以简单的移除掉cpu标签：sum:sys.cpu.user{host=webserver01}，如果我们想计算所有服务器的所有cpu核，我们可以简单的写成：sum:sys.cpu.user。这些数据会一个接一个的全部存储为sys.cpu.user的时间序列，所以聚合起来非常高效。opentsdb为尽可能快的聚合查询所设计，使用户可以从高层入手并深度挖掘数据详细信息。 聚合尽管标签模式非常灵活，但如果你不明白Opentsdb的查询方式还是会出现一些问题，所以需要一些提前考虑。举个例子我们要查询这个：sum:sys.cpu.user{host=webserver01}.我们记录了webserver01的64个时间序列，每一个时间序列代表一个cpu核。当我们发起这个查询，所有64个sys.cpu.user的时间序列都被查询出来，求平均，然后返回一系列数据。比如说对于这个1356998400这个时间戳是个50的结果，如果我们迁移到另一个系统预先计算了sys.cpu.user host=webserver01这个值并将其写入了Opentsdb，如果我们在执行相同的查询，那么我们会得到1356998400时间处为100的这个结果。发生了什么？Opentsdb聚合了所有64个时间序列，存储如下： sys.cpu.user host=webserver01 1356998400 50 sys.cpu.user host=webserver01,cpu=0 1356998400 1 sys.cpu.user host=webserver01,cpu=1 1356998400 0 sys.cpu.user host=webserver01,cpu=2 1356998400 2 sys.cpu.user host=webserver01,cpu=3 1356998400 0 ... sys.cpu.user host=webserver01,cpu=63 1356998400 1 Opentsdb在查询时如果不指定标签，则自动聚合所有的时间序列，如果指定一个或多个标签，则由“聚合所有”变为匹配当前标签，聚合并忽略其他标签。如果查询 sum:sys.cpu.user{host=webserver01} 我们可能包括sys.cpu.user host=webserver01,cpu=0以及sys.cpu.user host=webserver01,cpu=0,manufacturer=Intel,sys.cpu.user host=webserver01,foo=bar和sys.cpu.user host=webserver01,cpu=0,datacenter=lax,department=ops.这个示例的目的是想说明：小心命名schame。 时间序列基数命名schame一个关键的方面就是时间序列的基数。基数是指独一无二的item的集合数量。在opentsdb中，表示为与metric相关联的items的数量，比如，所有可能的标签名称和标签值组合。以及独一无二的metric的名称。基数之所有重要有以下两个原因：限制的IDs（UIDs）对于metric，tag name，tag value的指定是有数量限制的。默认地，每个类型有2^24个。举例来说，如果你运行一个十分火爆的web服务，并且向追踪客户端的ip作为标签，web.app.hits clientip=38.26.34.10，那么你很快就会到达瓶颈。此外，这种方式会创建很稀松的时间序列，而且用户可能只是偶尔访问你的应用甚至某些特定的ip再也不会访问了。其实UID的限制并不是一个问题，然而，一个标签值的UID可能会与标签名分离。如果你使用数字类型标识标签值，该数字可以只指定一个UID但是多个标签名会指向它。举例来说，如果你指定UID表示一个数字2，我们可以存储这些标签对：cpu=2, interface=2, hdd=2 and fan=2仅消耗了一个标签值的UID和4个标签名的UID如果你认为UID的限制会影响你，首先考虑你想执行的查询。如果我们观察上述web.app.hits的例子，你可能会只关心hits的总数而少有关心个别ip的hits。在这个例子中，你可以将ip地址存储一个注解。这样你既能优化基数又能满足需求，你需要使用另外的脚本来查询个别ip的hits值。（注意：未来版本会支持）如果你非要超过这个限制，你可以增加UID的字节数，从3bytes增加到8bytes。这个需要修改源码重新编译，部署你自定义的TSD并获取数据。并且保持自定义部分与未来的版本合并。警告：如果需要增加这个值，必须确保新建TSD，之前3bytes的与新的并不相容。 查询速度基数同样影响查询速度，所以思考你比较频繁的查询场景并为其优化你的schame。OpenTSDB每小时的时间序列会创建一行。如果我们有一个一个cpu的主机，并且每秒写入sys.cpu.user host=webserver01,cpu=0，写入一天，这会产生24行数据一共86400个点位数据。如果我们有8核cpu那就是192行和691,200个点位数据。这对于类似 start=1d-ago&amp;m=avg:sys.cpu.user{host=webserver01} 这种求cpu使用率的平均值和sum值看起来很不错，这个查询会遍历192行并且会将其聚合到一个时间序列里。然而如果我们有20,000台主机，每一个有8核cpu呢？这样我们由于host基数较大每天就有3，800，000行和1，728，000，00个点位数据。查询webserver01的cpu平均使用效率就会变低，因为需要从3800000行中筛选出192行（对于OpenTsdb2.2版本，你可以指定标签cpu=*或者模糊过滤来快速跳过不必要的行）这种命名好处是你的数据会有很深的粒度，可以存储每个cpu的metric。你可以简单的创建一个查询，查询所有机器所有cpu的平均使用率 start=1d-ago&amp;m=avg:sys.cpu.user 然而查询效率会随着过滤的行数增加而增加。这并不是Opentsdb才有的问题，而是所有数据库的通病。有以下几种处理基数的方式： 预聚合——在上面sys.cpu.user的例子中，你通常只关心每台主机的平均cpu使用，而不是每核cpu的使用。尽管数据采集器可以在发送时使用标签分割开的每核cpu的值，但也可以发送额外的点位数据如：sys.cpu.user.avg host=webserver01。这样一个完整的分割序列仅需要24行，对于20000台主机也仅需扫描48万行。查询响应速度会大大提高并且你仍然保留了数据的原始粒度。 转换成Metric——如果你只需要关注个别主机而不需要聚合所有主机值呢？在这种情境下，你可以转换hostname为metric，我们前面的例子可以变成sys.cpu.user.websvr01 cpu=0.使用这种schema查询每天有192行，效率更高。然而这样对host做聚合查询需要查询多次。 结论:当你设计你的schema时，牢记一下几点： 让你的命名尽可能一致以减少重复，如使用相同的metric，tag names以及tag values 对每个metric使用相同的数值和标签类型。不要保存my.metric host=foo and my.metric datacenter=lga. 根据常用查询优化schema 思考你会如何查询 不要使用太多标签，尽可能使用小数值，通常最多4,5个（Opentsdb默认支持最多8个tag） 数据模型每个时间序列需要以下数据：metric——一个时间序列通用的名字如: sys.cpu.user, stock.quote or env.probe.temp.timestamp——一个Unix时间戳，每秒或者每毫秒的数据标识value——存储某个时间点上具体的数值，可以是int类型或者float类型tag(s)——一个key/value对的标签标识。每个metric至少有一个 时间戳数据可以每秒或者每毫秒写入Opentsdb中。时间戳必须是整数类型并且不能超过13位数（见下面第一个Note）。毫秒值必须是这种格式：1364410924250并且最后三位表示毫秒值。应用如果产生超过13位数的时间戳（如比毫秒的粒度更细）必须约入13位数否则会发生错误。如果是秒级的时间戳则保存为2bytes，如果是毫秒级的时间戳则保存为4bytes。因此如果你不需要毫秒级的解决方案，建议你提交时使用10位数的时间戳以节省空间。并且注意避免混用两种时间戳，这样做会使你查询效率降低。Note:当通过telnet接口写数据时，timestamp格式会写成：1364410924.250，最后三位小数代表毫秒值。时间戳如果通过/api/put的HTTP端口发送则必须是整数类型不能存在小数。毫秒级的时间戳只能通过/api/query提取或者客户端命令行工具。详见query/index。Note:提供毫秒级的解决方案是没必要的。Opentsdb能支持到秒级的写入，而且，毫秒级的写入在长期运行的应用中，会产生大量的数据 Metrics and Tagsmetric和tag values遵循以下规则：１. 字符串大小写敏感，如”Sys.Cpu.User”和”sys.cpu.user”是不同的。２. 不支持空格３. 仅支持一下字符：a-z,A-Z,0-9,’-_./‘或者unicode字符metric和tags并未限制长度，但应尽可能短 整数值如果put命令未解析到（.）小数点，则会认为是整数。整数存储为可变长度，所以大小可能是1bytes到8bytes。这意味着整数类型的大小范围是：-9,223,372,036,854,775,808到9,223,372,036,854,775,807。整数类型不能出现逗号或者其他字符类型。比如，要存储最大的数值，必须是9223372036854775807。 浮点值如果put命令解析到（.）小数点，则会认为是浮点类型。目前在2.4以及之后的版本，所有的浮点类型都存储为单精度4bytes，双精度8bytes。无穷大或者非数值类型会发生错误。Note:虽然opentsdb支持浮点类型，但并不适合存储高精度类型比如货币，如存储15.2会返回15.199999809265137. 顺序不像其他的解决方案，opentsdb会按照你想要的顺序写入数据。这给写数据到TSD提供了很高的灵活性，允许系统自行处理当前数据然后导入历史值。 数据重复向Opentsdb中写数据，在小时区间内是幂等的。这就是说你在1356998400这个时间写入值42，稍后再重复写入也没什么不妥。但如果已写入的数据进行过压缩，当查询该行时会抛出异常。如果你试图在同一个时间戳处写入两个不同的值，那么查询时将会抛出一个数据重复的异常。这是因为整数类型的1,2,4,8bytes长度编码不同造成的，以及浮点类型。如果第一次写入整数类型第二次写入浮点类型， 将会抛出一个数据重复异常。如果两次都是整数类型或者浮点类型并且长度相同时，并不会出现任何问题而只是覆盖，前提是该行未进行压缩。在大多数情况下，如果发生重复数据写入通常是进程重启或者脚本出现bug。Opentsdb将在查询时出现重复数据而抛出异常。Opentsdb2.1之后你可以通过设置last-write-wins启用，设置tsd.storage.fix_duplicates为true。如果启用这个开关，在查询时，最近的数据记录将会被返回而非抛出异常。并在日志中记录警告信息。如果启用压缩，则原始的数据将会被最近的值重写。 Input方法目前有三种方法向Opentsdb导入数据：telnet api，http api以及从文件批量导入。你可以使用opentsdb提供的工具，如果你可以承担风险，也可以使用jar包。警告：不要试图直接向Hbase中写入数据，会造成数据混乱。Note：如果tad.mode被设置成ro（read only）而不是rw（read write）。那么TSD将不能接受RPC调用。telnet风格的调用会抛出异常，http调用会返回404。然而java api仍然可以写入。 Telnet最简单的方法开始opentsdb就是使用命令行的telnet，连接TSD并且使用put命令进入。如果你正在写程序，打开一个socket，打印一下字符串，格式如下：put 比如：put sys.cpu.user 1356998400 42.5 host=webserver01 cpu=0每一个put仅能发送一条数据。不要忘记在结尾输入换行符 “\n” Http API2.0之后的版本，数据可以通过HTTP支持的 序列化 插件发送。多数情况下，不相关的数据可以通过同一个HTTP post提交。详见../api_http/put。 批量导入如果你需要从别的系统中或者需要回填历史数据，可以使用import工具。详见cli/import。 写入性能Opentsdb支持每秒百万级别的写入，搭载在常用的服务器上。然而，如果使用单机版的HBase虚机，结果可能会大失所望，每秒只能写入几百的数据。如果你需要扩大集群规模需要做以下几件事： UID分配第一个关键点是“UID”的分配。不论是metric还是tag name/tag value都要在存储之前分配UID。举例来说，metric sys.cpu.user可能会被指定UID为000001在第一次写入TSD时。这种分配会耗费大量的时间，因为它要获取可用的UID，生成UID到name的映射，生成name到UID的映射，以及写入数据点的rowkey中。UID将会存储在TSD的缓存中以便下次查询时快速查找。因此，我们推荐你为metric/tag name/ tag value预分配UID。如果你按上面提到的命名schema的方式，你会知道许多可以分配的UID。你可以使用CLI工具cli/mkmetric, cli/uid或者HTTP API ../api_http/uid/index来执行预分配。每次你要创建批量metric或者tags时，都要考虑预定义，否则将会陷入很大的性能困境。Note：每次启动TSD时，都要加载所有UID到缓存中，所以启动会慢。 随机Metric UID分配从2.2版本之后，你可以指定随机UIDs以避免region server的热点问题。因为metric的UID是作为rowkey的起始位置的。如果你个新的数据集创建，没采用这种方式时，至少region分裂，都要集中到一台机器上。通过修改tsd.core.uid.random_metrics的值你可以随时指定是否使用随机metric UID。然而还是推荐你通过全部的metric空间预分裂TSDB的表。举例来说，如果你使用默认的metric UID长度为3bytes，因此你会有16,777,215个值。如果你已经在TSDB中存在数据了，还想选择使用随机uids，那么你可能向创建一个新的region。当创建随机UID是，TSDB会尝试十次避免碰撞。因此，随着分配的指标的数量增加，碰撞的次数和覆盖的几率会因重试而降低。如果您启用了随机id并继续添加更多的度量标准，那么您可能希望增加度量uid上的字节数。请注意，UID更改并不是向后兼容的，因此您必须创建一个新表并迁移您的旧数据。 加盐在2.2中，支持大幅增加跨区域服务器的写分布。当启用时，配置的字节数被预置到每个rowkey中。然后将标签的每一个度量和组合都放入一个“桶”中，其中的ID被写入到salt字节中。随着时间序列在已配置的bucket计数中被分割，从而路由到不同的区域和不同的服务器，特别是对于高基数度量(具有大量标记组合的那些)，分布得到了改进。例如，如果没有salting，一个包含100万系列的度量标准将被写入单个服务器上的单个区域。在启用了salting和一个20个桶大小的情况下，这个系列将在20个区域(如果集群有这么多主机)上拆分，每个区域有50,000个系列。警告：因为salting修改了存储格式，所以不能随意地启用或禁用salting。如果已有数据，则必须启动一个新的数据表，并将数据从旧表迁移到新的数据表。不能从以前版本的OpenTSDB读取盐的数据。要启用salting，您必须修改配置文件参数tsd.storage.salt.width和可选地tsd.storage.salt.buckets。我们建议将盐的宽度设置为1，并根据集群中区域服务器数量的因素确定桶数。注意，在查询时，TSD将检查tsd.storage.salt.buckets个桶数的扫描仪来获取数据。适当数量的桶必须通过实验来确定，因为在某些时候查询性能可能会受到影响，因为有太多的扫描器打开并对结果进行了排序。在未来，盐的宽度和桶可能是可配置的，但我们不想让人们在事故和丢失数据的情况下改变设置。 追加在2.2版本中，现在支持通过appends向HBase列进行写入。这可以提高读和写的性能，因为tsd将不再在每个小时的末尾维护一个行记录队列来进行压缩，从而防止在HBase中进行大规模的读取和重写操作。但是由于在HBase中应用程序的方式，在region servers上增加了CPU利用率、存储文件大小和HDFS流量。确保密切监视HBase服务器。在读取时，每一行只返回一个列，类似于post - tsd - compaction行。但是请注意，如果是tsd.storage.repair_appends在启用时，当一个列具有重复或非顺序数据时，它将被重新写入HBase。同样，具有许多重复或排序问题的列可能会减缓查询效率，因为它们必须在响应调用者之前解决。可以在任何时候启用和禁用Appends。但是，在2.2之前的OpenTSDB版本将跳过这个值。 预分裂HBase Regions如果你采用预分裂region的方式，不论你是在一个独立的服务器上进行测试或者运行一个完整的集群，那么您将看到更好的性能。HBase Region处理一个行键范围，本质上是一个文件。当您第一次创建tsdb表并开始编写数据时，所有这些数据点都被发送到一个服务器上的这个文件。当一个区域被填满时，HBase会自动将其分割成不同的文件，并将其移动到集群中的其他服务器，但当这种情况发生时，tsd不能写入该区域，并且必须缓冲数据点。因此，如果您可以在开始编写之前预先分配多个region，则tsd可以将数据发送到多个文件或服务器，您可以立即利用线性可伸缩性。预分割您的tsdb表区域的最简单方法是估计你将记录的度量名称的数量。如果您已经设计了一个命名模式，那么您应该有一个很好的想法。假设我们在系统中跟踪4000个指标，这并不是说有4000个时间序列，因为我们还没有计算标签，只是一些度量名称，比如“sys.cpu.user”。数据点是用行键写的，其中metric的UID包括第一个字节，默认情况下是3个字节。第一个metric将被分配一个UID为000001作为十六进制编码值。第4000个度量将在十六进制中有一个UID为000FA0。您可以在HBase Book的脚本中使用这些作为开始和结束键，将您的表分割成多个区域。根据每个指标的时间序列，256个区域可能是个好起点。TODO -包括用于预分解的脚本。上面的简单拆分方法假定每个度量(即相当一致的基数)的时间序列大致相等。例如，UID为000001的度规可能有200个时间序列，而000FA0有大约150个。如果你有一个很宽的时间序列，例如000001有10,000个时间序列，而000FA0只有2个，你可能需要开发一个更复杂的分割算法。但别太担心分裂。如上所述，HBase将为您自动分割区域，因此随着时间的推移，数据将相当均匀地分布。 分布式HBaseHBase将以独立模式运行，在此模式下将使用本地文件系统来存储文件。它仍将使用多个region，底层磁盘或raid阵列仍然能支持它。您肯定希望在HBase下有一个RAID阵列，这样如果驱动器失败，您可以在不丢失数据的情况下替换它。这种设置适合于测试或非常小的安装，您应该能够写入每秒数千个数据点。然而，如果您想要真正的吞吐量和可伸缩性，您就必须设置一个Hadoop和HBase集群，并拥有多个服务器。在分布式的安装中，HDFS管理Region文件，自动将副本分发到不同的服务器以进行容错。HBase将Region分配给不同的服务器，OpenTSDB的客户端将把数据点发送到存储它们的特定服务器。现在，您可以在多个服务器之间扩展操作，提高性能和存储空间。如果您需要更多的吞吐量或存储，只需添加节点或磁盘。有很多方法可以设置Hadoop / HBase集群，以及大量的各种调优调整，因此可以在谷歌搜索并询问用户组的建议。一般性的建议包括： 为name node指定一对高内存、低磁盘空间服务器。使用像心率和起搏器这样的高可用性。 在至少3个服务器上设置Zookeeper，用于容错。他们必须有大量的RAM和一个相当快的磁盘来写日志。在小集群上，这些可以在namenode服务器上运行。 对于HDFS数据节点做JBOD（Just a Bunch Of Disks） HBase regon服务器可以与HDFS datanode搭配 服务器之间至少有1个Gbps链接，10 Gbps(千兆比特每秒)更可取。 将集群保存在一个数据中心中 多TSD单个TSD每秒可以处理数千个写入。但是，如果您有许多资源，最好通过运行多个tsd，并使用负载平衡器(如清漆或DNS轮询)来分配写操作。当集群专注于OpenTSDB时，许多用户在HBase region服务器上使用colocate TSDs。 持久连接在tsd中启用keep- alives，并确保您所使用的任何应用程序都可以打开它们的连接，而不是打开和关闭每个写入。有关详细信息请参阅configuration。 禁用元数据和实时发布OpenTSDB 2.0引入了元数据来跟踪系统中的数据类型。在启用跟踪时，每次写入的每个数据点一个计数器递增，以及新的uid或时间序列都将生成元数据。数据可能被推到搜索引擎或通过树生成代码。这些过程在TSD中需要更大的内存，并可能影响吞吐量。默认情况下，跟踪是禁用的，所以在启用该特性之前进行测试。2.0还引入了一个实时发布插件，在那里输入的数据点可以在排队等待存储后立即发送到另一个目的地。这是默认禁用的，所以在部署之前测试您感兴趣的插件。 原文地址：http://opentsdb.net/docs/build/html/user_guide/writing/index.html]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb-快速入门]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb-%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[当你通过第一章完成安装后，你可以在以下内容中学到如何向opentsdb存储数据并在GUI查询它们，以及生成图表。 创建你第一个度量信息MetricsMetrics需要在你开始存储数据之前注册，以避免不想要的数据以及输入错误。你可以通过配置文件配置Metrics自动创建。创建一个或多个Metrics，使用如下命令：mkmetric ./tsdb mkmetric mysql.bytes_received mysql.bytes_sent 这会创建两个metrics，mysql.bytes_received和mysql.bytes_sent对于新的标签，会在你第一次使用时自动注册。目前为止，opentsdb允许最多2^24=16,777,216个不同的Metrics，16,777,216个不同的tag name和16,777,216个不同的tag value。因为它们的每一个都是一个3bytes的UID，Metric names, tag names 和tag values有它们自己的UID空间，所以你每一个只能有16,777,216个。虽然这个值可以配置但目前并未开放，所以需要记住，对于大的站点的user id或者event id，这种方案是不可行的。 开始收集数据目前为止我们有了两个Metric，我们可以向TSD发送数据。我们写一个收集mysql数据的脚本并向TSD发送数据。 cat &gt;mysql-collector.sh &lt;&lt;\EOF #!/bin/bash set -e while true; do mysql -u USER -pPASS --batch -N --execute &quot;SHOW STATUS LIKE &apos;bytes%&apos;&quot; \ | awk -F&quot;\t&quot; -v now=`date +%s` -v host=`hostname` \ &apos;{ print &quot;put mysql.&quot; tolower($1) &quot; &quot; now &quot; &quot; $2 &quot; host=&quot; host }&apos; sleep 15 done | nc -w 30 host.name.of.tsd PORT EOF chmod +x mysql-collector.sh nohup ./mysql-collector.sh &amp; 每隔15秒，脚本从Mysql收集两条数据并发送给TSD。你可以设置更小的时间间隔获得更大的量级。这个脚本做了什么？如果你不是shell和awk的忠实粉，可能不太明白脚本的工作原理。但是这很简单：set -e命令简单的要求bash执行遇到错误时退出。简化了错误处理。然后脚本进入无限循环。在一个循环里，我们查询mysql并接受两条记录： $ mysql -u USER -pPASS --execute &quot;SHOW STATUS LIKE &apos;bytes%&apos;&quot; +----------------+-- -----+ | Variable_name | Value | +----------------+-------+ | Bytes_received | 133 | | Bytes_sent | 190 | +----------------+-- -----+ –batch -N标签要求Mysql移除人机友好的标识所以我们不用手动过滤它。然后通过管道输出到awk，并告知使用tab分隔符 -F”\t”，因为–batch会使用该分隔符。然后我们创建两个变量，一个名为now，并通过当前时间戳初始化，另一个名为host，设置为当前机器的hostname，并且对于每一行，我们打印一条 put “mysql.”，然后跟一个小写的第一个值，然后是当前时间戳，然后是第二个值，另一个空格，并以host=hostname结尾。每15秒重复一次。nc -w 30为连接TSD设置了超时时间。注意这只是一个实例，实际实践时你可以使用Mysql收集器。 如果你没有mysql_server用来监控，可以使用以下脚本收集linux server的负载信息。 cat &gt;loadavg-collector.sh &lt;&lt;\EOF #!/bin/bash set -e while true; do awk -v now=`date +%s` -v host=`hostname` \ &apos;{ print &quot;put proc.loadavg.1m &quot; now &quot; &quot; $1 &quot; host=&quot; host; print &quot;put proc.loadavg.5m &quot; now &quot; &quot; $2 &quot; host=&quot; host }&apos; /proc/loadavg sleep 15 done | nc -w 30 host.name.of.tsd PORT EOF chmod +x loadavg-collector.sh nohup ./loadavg-collector.sh &amp; 这会存储从你服务器读取到的1分钟和5分钟的平均负载。 put proc.loadavg.1m 1288946927 0.36 host=foo put proc.loadavg.5m 1288946927 0.62 host=foo put proc.loadavg.1m 1288946942 0.43 host=foo put proc.loadavg.5m 1288946942 0.62 host=foo 批量导入想想一下，你有一个cron job收集每天或者每小时千兆字节的应用日志或者提取的数据。举个例子，你可能会某个时间请求应用的平均次数，你的cron job会计算30秒窗口的平均值，那么你的crontab输出可能如下所示： 1288900000 42 foo 1288900000 51 bar 1288900000 69 other 1288900030 40 foo 1288900030 59 bar 1288900030 80 other 第一列是一个时间戳，第二列是30秒窗口的平均延迟，第三列是请求的类型。如果你运行自己的cron job，一天就会有8640行记录。为了将其导入opentsdb，你需要将其输出调整为如下结构： myservice.latency.avg 1288900000 42 reqtype=foo myservice.latency.avg 1288900000 51 reqtype=bar myservice.latency.avg 1288900000 69 reqtype=other myservice.latency.avg 1288900030 40 reqtype=foo myservice.latency.avg 1288900030 59 reqtype=bar myservice.latency.avg 1288900030 80 reqtype=other 注意我们使用一个名为myservice.latency.avg的metric关联数据，并使用一个request type标签代表请求类型。你也可以添加一个主机名标签：host=foo。这样，就可以将每个服务器的数据划分。此外，为了批量导入你可以执行以下命令： ./tsdb import your-file 如果你的数据过大，可以考虑先gzip压缩，可以通过管道命令gzip -9 &gt;output.gz代替直接写到文件。导入命令支持gzip文件并提高批量导入的性能。 自我监控每一个TSD都可以通过一个简单的命令导出当前状态。你可以收集这些状态并将其定时返回给TSD。第一步，创建必要的metric： echo stats | nc -w 1 localhost 4242 \ | awk &apos;{ print $1 }&apos; | sort -u \ | xargs ./tsdb mkmetric 这请求TSD的状态（确保在本地运行并监听4242端口），从状态中提取metric的名称并指定UID，然后你可以使用如下这个简单的脚本收集状态并存储到TSD中。 #!/bin/bash INTERVAL=15 while :; do echo stats || exit sleep $INTERVAL done | nc -w 30 localhost $1 \ | sed &apos;s/^/put /&apos; \ | nc -w 30 localhost $1 这样，你可以每15秒收集一次。 创建图表一旦你使用如上方法向TSD中写入了数据，你可以使用这些数据创建图表。用浏览器打开GUI界面。如果你的TSD在本地运行，那么访问：http://localhost:4242第一步，选中一个指标并且将其放入metric box中，举个例子，proc.loadavg.1m。如你所见，该行会自动凸起并可以点击它们。然后点击from按钮，一个日期插件会显示出来，从昨天选择任意时间然后点击另一个按钮。到此为止，你会看到正在加载图表，在图表下方有简要说明。如果图表为空，可能未找到最近数据，点击 now刷新图表。这个初始化图表会聚合你所选中metric的一系列时间，试着点击左侧tags标签添加标签以筛选不同的host标签，从右侧添加值。在另一处点击你会看到图表重新绘制。 原文地址：http://opentsdb.net/docs/build/html/user_guide/quickstart.html]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb-配置]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb-%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[配置OpenTSDB可以通过本地系统的文件进行配置，通过命令行参数或组合，或者两者兼而有之。配置文件配置文件符合Java属性规范。配置名称是小写的，带点字符串，没有空格。每个名称后面跟着一个等号，然后是属性的值。所有OpenTSDB属性从tsd开始。#注释。例如: # List of Zookeeper hosts that manage the HBase cluster tsd.storage.hbase.zk_quorum = 192.168.1.100 将配置TSD连接到192.168.1.100的Zookeeper。当组合配置文件和命令行参数时，处理顺序如下: 加载默认值加载配置文件值 配置文件参数加载，覆盖默认值 命令行参数加载，覆盖配置文件参数和默认参数文件路径可以使用- config命令行参数指定配置文件的完整路径。否则，如果没有指定，OpenTSDB和一些命令行工具将尝试在以下位置搜索一个有效的配置文件:● ./opentsdb.conf● /etc/opentsdb.conf● /etc/opentsdb/opentsdb.conf● /opt/opentsdb/opentsdb.conf如果无法找到有效的配置文件，且不设置所需的属性，则TSD将不会启动。请参阅下面的属性表，以获得所需配置设置的列表。参数下面是所有工具的配置选项表。在应用时，将提供相应的命令行覆盖。请注意，单个命令行工具可能有它们自己的值，所以请参阅它们的文档以了解详细信息。 http://opentsdb.net/docs/build/html/user_guide/configuration.html]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Opentsdb-安装]]></title>
    <url>%2F2018%2F07%2F13%2FOpentsdb-%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[环境OpenTSDB可从源码编译或者包安装，详见github运行要求Linux系统java1.6以上Hbase 0.92以上GnuPlot4.2以上 安装第一步，启动Hbase。如果你刚接触HBase和OpenTSDB，建议使用单机模式启动运行以方便测试。启动OpenTSDB之前，确保zookeeper可用，可以通过telnet localhost 2181命令查看状态。如果不能连接zookeeper，检查Ips或者host name以解决hbase对zookeeper的强制依赖。如果hbase启动，可以选择从包安装，或者使用GIT或者从tar包安装。 从源码编译：Compilation requirements include: ● A Linux system ● Java Development Kit 1.6 or later ● GnuPlot 4.2 or later ● Autotools ● Make ● Python ● Git ● An Internet connection命令： git clone git://github.com/OpenTSDB/opentsdb.git cd opentsdb ./build.sh make install 创建表安装完成后你可以在hbase实例之上运行opentsdb，第一步你要在hbase上创建必要的表。有一个简单的脚本能完成这件事并可以指定是否压缩： env COMPRESSION=NONE HBASE_HOME=path/to/hbase-0.94.X ./src/create_table.sh 此处，COMPRESSION有如下选项：none，lzo，gzip，snappy，该脚本创建如下表：tsdb，tsdb-uid，tsdb-tree，和tsdb-meta，如果你只是测试opentsdb，暂时无需考虑压缩。生产环境下，确保使用可用压缩以避免存储灾难。 启动TSDopentsdb2.3版本维护一个配置文件，由daemon和命令行工具共享。如果你从源码编译，需要copy ./src/opentsdb.conf到指定目录，并且做如下配置，其中必要的是： ● tsd.http.cachedir - 临时文件路径 ● tsd.http.staticroot - 静态GUI文件路径 ./build/staticroot ● tsd.storage.hbase.zk_quorum - 如果HBase和zookeeper不是运行在同一台机，在此处指定地址和端口号。 配置好以后，可通过如下命令启动： ./build/tsdb tsd 或者，你可以用如下命令创建临时目录并在一行命令中指定： tsdtmp=${TMPDIR-&apos;/tmp&apos;}/tsd # For best performance, make sure mkdir -p &quot;$tsdtmp&quot; # your temporary directory uses tmpfs ./build/tsdb tsd --port=4242 --staticroot=build/staticroot --cachedir=&quot;$tsdtmp&quot; --zkquorum=myhost:2181 之后，你可以通过 http://127.0.0.1:4242 访问TSD的web界面注意：当你通过GUI生成一个图形时，cache directory用来存放生成的临时文件。这些文件需要定期清理。opentsdb并未做这件事，但提供了一天清理一次的脚本 tools/clean_cache.sh 原文地址：http://opentsdb.net/docs/build/html/installation.html]]></content>
      <categories>
        <category>Opentsdb</category>
      </categories>
      <tags>
        <tag>外文翻译</tag>
        <tag>Opentsdb</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Abird平台OLAP技术架构的一些思考(2)]]></title>
    <url>%2F2018%2F02%2F26%2FAbird%E5%B9%B3%E5%8F%B0OLAP%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83-2%2F</url>
    <content type="text"><![CDATA[承接上文，讨论一下Abird平台OLAP技术架构之——实时 的一些思考 背景简单描述下实时的业务背景：对于部分用户体验数据以及其他数据，业务方有实时查询的需求，并且也要结合多维分析来定位异常问题。如实时vv/成功率/卡顿率/秒开率等指标，而且需要省份、运营商、网络制式、app版本等维度辅助定位陡升、陡降问题力求第一时间监控报警，还有诸如心跳等实时业务。 现状上文说了，我们采用MOLAP的实现方法，实时的技术架构也不外如是。我们目前采用TT+Blink+Streaming+OTS的方式，注意：Blink依据数据量级以及ETL复杂度选择使用。在Streaming中我们实现了配置化cube构建引擎，以降低后续流计算的接入成本。由于主要集中于业务，并未为此付出过多精力进行数据膨胀的优化。导致部分业务完全立方体的构建极易造成数据膨胀。其次，streaming集群跨数据中心io瓶颈、任务不稳定等随着业务增加任务增加，日渐增加我们的人肉运维成本。 改进方案我们考虑如下几种改进方案： SQL On Blink streaming通用化OLAP的cube构建引擎基于Blink重构，并完善优化 基于Druid实时OLAP引擎——对Druid的介绍详见笔者Druid专栏 方案一对于方案1，集团Bayes平台已足够成熟，能够支持工程开发以及运维优化，但是随着业务增加，需要接入的TT流更多，业务SQL增多会面临离线一样的问题，尽管这样看似离线和实时在某些公共的业务上能整合计算逻辑，但实际上编写OLAP多维度的复杂SQL同样不易维护，而且会持续增加开发、运维成本。针对独立性强的、处理逻辑简明的可以采取该种方案。 方案二对于方案2，能形成自主研发的基于Flink的实时OLAP引擎，从长远来看意义重大，在已有的Streaming通用化方案上迁移容易，但难点在其三： 由于前期cube构建采用完全立方体构建，实际上是对n个维度做了2^n条记录的分发再聚合，并未考虑后续的优化方案（这里只有必要维度的优化，对于维度组、衍生维度等比较实用的优化没有），而如果进行后续优化，实际上是对cube算法的重构，我们可以采用”By Layer”算法实现清晰易维护的MR流计算逻辑或者采用“Fast Cubing Algorithm”算法。具体不在此展开，参考kylin cube构建算法：http://kylin.apache.org/blog/2015/08/15/fast-cubing/但不可否认的是，前者需要存储中间结果后者也需要做一定研发工作，真实实践仍需对其On Blink做性能评估，长远来看是有价值的也是值得一试的。 由于目前使用Blink只能依赖Bayes平台，暂不提供jar任务提交方式。——硬伤，必须解，解不了此方案可pass。 最终cube结果存储。如果考虑该种方案，要对存储立方体数据做出一套规划，如采取kv分布式存储Hbase等如何面对非rowkey格式查询，以及一些runtime聚合操作的实现（如衍生维度在查询时聚合、如时间维度上的下行采样等场景）。可以借鉴kylin/opentsdb等对HBase的应用，有同学会问，kylin2.0以上版本正在支持实时cube构建，能够达到亚秒级响应，为何不用？具体原因参见上一篇，说白了还是平台兼容性，我们作为业务部门balabala… 方案三对于方案3，是笔者在前段考虑实时通用化时重点考虑的方案。在此重点研讨优缺点如下： 我们能得到什么？ Druid对事件数据聚合，此类数据有较为明显的三类数据列构成：时间戳列、维度数据列（字符串数据，可用来过滤）、指标数据列（druid在导入数据时会根据配置对原始数据的数值列进行roll up得到指标数据列）。这种做法实际上是将Cube的计算逻辑后移（这种说法不严谨，实际上并无cube计算，而是基于最细粒度聚合的再计算），我们只需梳理事实表即可。这里说的最细粒度聚合实际上就是group by A,B,C,……所有一开始指定的维度的聚合计算。由此，我们可以避免cube计算以及优化等一系列“故事”。 避免了cube计算也就避免了上文提到的streaming稳定性的问题，更进一步，Druid这种方式更具查询灵活性，最细粒度聚合值有了，在其上所有需要的聚合层级都可得，也能支持秒级查询响应。 具备了上述灵活性，查询受用面积更广，实际上更进一步避免了“实时热更新”的问题。不在此展开（这个问题曾一度困扰笔者，解决思路如新版Flink的CEP和cloudera的streaming+drools），附链接：http://blog.cloudera.com/blog/2015/11/how-to-build-a-complex-event-processing-app-on-apache-spark-and-drools/ 我们还需考虑什么？ Druid目前不支持join查询，这意味着我们许多指标如成功率/卡顿率/秒开率等需要将相关因子在ETL时打平并落入一个datasource，这里Blink完全能够胜任不足为虑。 集团内Druid集群服务。据目前了解，只有个别团队提供Druid对外服务，但其产品未对外开放，运维成本能降低（自行搭建的Druid角色较多运维难度较大），但运维门槛会加大（毕竟未开放想让人家提供无偿贴心服务么）。而且资源受限，是否能够支撑重点业务，数据ttl等问题都需要根据业务进行评估。 综合考虑之，还是建议用Blink+Druid实现实时OLAP，这是目前综合困扰我们的稳定性和灵活性而言较好的方案。 以上两篇文章的思考皆是立足于现实，如果读者完全拥抱开源无需多虑选择合适的方案即可，这也是为什么笔者在两个重点技术中用的是“我们能得到什么？”和“我们还需考虑什么？”的原因，是笔者项目实际情况所致而非技术严格意义上的优缺点。当然，即使是开源的技术应用于生产时也是有很多路要走。思考良多发现技术没有一劳永逸的，哪怕根据实际情况选择了最合适的架构，你也要为之付出许多的研发、运维成本，所以不要被“我们还需考虑什么？”吓坏，经过团队研讨后确定了的走下去就不怕错，至少比面对挫折犹豫不决踌躇不前要好，就当技术储备也能获益良多。]]></content>
      <tags>
        <tag>OLAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Abird平台OLAP技术架构的一些思考(1)]]></title>
    <url>%2F2018%2F02%2F26%2FAbird%E5%B9%B3%E5%8F%B0OLAP%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83-1%2F</url>
    <content type="text"><![CDATA[背景本文重点讲述abird平台下vpm项目的需求梳理和架构思考。vpm项目主要是优酷“用户体验数据”的多维度分析产品，旨在辅助业务方收集、展示、监控和处理用户体验问题，反应用户体验的一些通用指标诸如vv/uv/成功率/卡顿率/秒开率此类，此外，还有错误码分布、异常日志分析等功能。 平台上OLAP场景大致分为两类： 临时查询：指用户通过手写SQL来完成一些临时的数据分析需求。这类需求的SQL形式多变、逻辑复杂，对响应时间没有严格的要求。 固化查询：指对一些固化下来的取数、看数的需求，通过数据产品的形式提供给用户，从而提高数据分析和运营的效率。这类需求的SQL有固定的模式，对响应时间有比较高的要求。 多数情况下，vpm项目下的用户体验多维分析属于固化查询，有既定需求形态和固定的查询模式，如大盘。特殊情况下有个别专项，实际上专项中也是把临时需求固化下来形成小的产品，并无真正意义上的临时查询。 现状从OLAP技术手段来看，我们一直采用MOLAP的实现方法，将细节数据和聚合后的数据均保存在cube中，以空间换效率，查询时效率高，但生成cube时需要时间和空间。这里针对离线和实时我们有两种实现，分别探讨如下： 离线中，我们在odps上通过编写sql来实现cube计算，然后将cube数据存入ots中，在应用中即时查询。由于前期有独立的abird_vpm表，开发效率较高，通过计算中间表的方式也能较大的优化计算效率、应对业务扩展。然而随着业务的再扩展问题凸显，首先，业务已不局限于abird_vpm表，还要零零散散的接入优土公共汇总层乃至明细层的其他源表，尽管对sql编写和cube计算层级做出一些规范和优化，但仍无法摆脱无底洞似的业务新增和变更的代码开发，任务增多日渐不好维护；另外，起始时缺乏对维表做统一管理，ots的表结构设计无法支持范围查询等等使得技术架构的灵活性大打折扣，给后续开发造成难度。 改进方案我们考虑如下几种改进方案： 支持olap的分布式存储引擎，如集团的hybirddb for mysql和ADS 进行预计算的olap引擎如kylin、druid、es（非预计算） 方案一对于方案1，尽管能够提供最佳的灵活性，但由于以上存储引擎实际上都是全量存储明细数据，暂不论如何创建索引如何聚合查询，这种存储成本实际上是我们不希望看到的，因为用户体验数据大部分都是基于日志数据、点击数据，明细数据带来的价值是极小的、量级是巨大的，没必要做全量存储。 方案二对于方案2，es毋庸置疑适用于大数据量的过滤、检索的明细查询，在应对聚合查询时无法达到秒级响应甚至是分钟级，es在平台内也有用武之地但不是olap。而Druid主要面向的是实时Timeseries数据，我们也有类似的场景如实时分钟级趋势等，但这里面向的还是数仓中按T+1的结构化表。kylin显然是我们倾向的方案，下面来看下kylin的利弊和适合与否 引入kylin能给我们带来什么？kylin能够支持的量级、性能、查询响应、灵活性是毋庸置疑的，但逐一比对我们发现，目前的技术架构除了灵活性（这里包括构建cube的配置化和查询模式的灵活性）不能比拟外，其他的并不是我们的实际痛点，同样是依赖上游调度的预计算、基于kv分布式存储的查询引擎使得性能和响应速度并不能获得太大的提升。那我们就着重来看下kylin能带来哪些灵活性的提升以及是否解决我们的实际痛点。 我们知道，Kylin的设计是基于一个星形模型，基于一个事实表和多个维表构建cube模型，生成mr job来计算每层cuboid并落地hbase。通过可视化的web ui即可做到cube的配置化实现（这里是我们想要的）。然而为此，我们仍然需要考虑： 我们能得到什么？ 对于基数计算、精确去重问题（如我们的uv计算），不像市面上其他引擎采用近似估计如hyperloglog算法，kylin目前已在1.5.3版本中实现了全类型精确去重计数的支持 配置化的cube实现 cube的剪枝优化，如必要维度、维度组、衍生维度等。我们目前只有必要维度的优化，sql的开发模式使得做这些优化不能通用所以要麻烦一些（这里的优化带来的计算资源的节省和存储空间的节省目前不是我们的瓶颈） 我们还需考虑什么？ 构建小规模的数据集市层，如事实表采用宽表，增建维表或引用已有维表（当然符合定义规范的维度也可以在runtime应用里解决，这里说的是标准做法）。所有非标准星型的数据模型，都可以通过预处理ETL先拉平，做成一个宽表（目前abird_vpm并非严格意义宽表），对于复杂指标涉及到表达式时，也可以通过提前处理解决。把表达式单独转成一列，再基于这列做聚合。问题在于，多个业务表面类似但实质不同，如何规范统一化的数据模型而不是烟囱建设数据集市是将要把控的重点。 kylin在集团内是否有对标产品？目前调研都不够理想，犹如孔明灯、夸克/FBI等产品业务方已在使用，我们基于它们建设也未尝不可，但我们需要的灵活性（cube配置化、查询模式多变）仍要调研取证。如果不基于已有产品建设，那么引进kylin的成本稍大，要和odps数据源集成，和计算引擎集成等等，要搭建至少有个小规模的kylin集群，一边还要跟社区（当然自己用可以慢慢来）。换句话说甚至不如开发一个类似kylin构建cube内核的通用mr来的实在（我们之前的兼容odps的最简方案）。 seiya应用的数据层改造。目前针对ots特定的表结构已有一套数据层查询方案，但实际上引入kylin后，查询模式可以转变为基于数据模型的sql driver查询，显而易见，当我们集市构建丰富、cube模型设计合理（维度、指标等）的情况下，能够在不改变cube构建任务的情况下、在应用层即可应对更多的业务开发。 其次需要考虑的是集群的开放、运维等问题 的确，考虑长远来说的话kylin引入的收益是日渐明显的，建立cube后相比目前SQL开发能提升效率，但引入之前的宽表构建也是我们不可避免的问题，松散的业务同样需要独立的模型——仍然要从构建宽表开始考虑一遍（如果新增的业务不能符合标准星型模型的话），等等，我们作为业务部门而不是平台部门是否能分散精力组建也是我们需要考虑的。从目前我们讲求较高的开发收益比来说（从0到1的收益远比从1到1.1大），不适合进行投入式的技术研发，或许有更具性价比的产品等待我们调研发现。长远来讲，我们希望Abird能将优土用户体验等日志从采集、到通用etl、到通用olap形成一套完备的解决方案，还是路漫漫其修远兮。在此提出一些思考与读者讨论，也欢迎读者共建。]]></content>
      <tags>
        <tag>OLAP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-生产部署]]></title>
    <url>%2F2018%2F02%2F13%2FDruid-%E7%94%9F%E4%BA%A7%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Druid-性能]]></title>
    <url>%2F2018%2F02%2F13%2FDruid-%E6%80%A7%E8%83%BD%2F</url>
    <content type="text"><![CDATA[性能Druid在多个组织、企业中都有生产集群，而且为了评估它的性能，我们选择了2014年初Metamarkets的生产集群数据，与其他数据库相比，我们还包括了TPC-H的结果。 查询性能Druid的查询性能因查询的不同而不同。例如，对高基数维度的值进行排序比在一个时间范围内的简单计数代价要昂贵得多。为了展示生产Druid集群中的平均查询延迟，我们选择了8个查询最多的数据源，如表2所示。 大约30%的查询涉及过滤和聚合不同类型的metric，60%的查询是由一个或多个维度组成，而有10%的查询是搜索查询和元数据检索查询。聚合查询中扫描的列数大致遵循指数分布。涉及单个列的查询非常频繁，涉及所有列的查询非常少见。 关于我们的结果有几点注意: 结果来自我们生产集群中的“hot” tier，大约50个数据源，数百个用户发出查询 在“hot”层有大约10.5TB的RAM，并且大约有10TB的部分被加载。总的来说，这一层有大约500亿条记录。没有显示每个数据源的结果。 该tier使用英特尔®Xeon®e5 - 2670处理器,由1302个处理线程和672核(hyperthreaded)。 使用内存映射存储引擎(机器配置为内存映射数据，而不是将数据加载到Java堆中)。 图8展示查询延迟、图9展示了每分钟的查询。在所有不同的数据源中，查询延迟大约是550毫秒，90%的查询在不到1秒内返回，在2秒内达到95%，99%的查询在不到10秒内返回。在2月19日，我们偶尔观察到延迟的峰值，因为我们最大的数据源之一在非常高的查询负载下，Memcached实例上的网络问题变得更加复杂。 TPC-H Data的查询测试我们还在TPC-H数据上展示了Druid的性能。大多数TPC-H查询并不直接应用于Druid，因此我们选择了更典型的Druid负载查询来演示查询性能。作为比较，我们还提供了使用MyISAM引擎(在我们的实验中较慢的)来查询MySQL的结果。 我们选择MySQL作为基准，因为它普遍流行。我们不选择另一个开源的列存储，因为我们没有信心可以正确地调优它以获得最佳性能。 我们的Druid使用Amazon EC2 m3.2xlarge实例类型(Intel®v2 @ 2.80 ghz Xeon®e5 - 2680)的历史节点和c3.2xlarge实例(Intel®v2 @ 2.50 ghz Xeon®e5 - 2670)对代理节点。我们的MySQL设置是一个Amazon RDS实例，它运行在同一个m3.2xlarge实例类型上。 1GB TPC-H数据集的结果如图10所示，100GB数据集的结果如图11所示。 给定时间间隔的select count(*)查询Druid的扫描速率为53,539,211行/秒/核心，select sum(float)查询的扫描速率为36,246,530行/秒/核心。 最后,我们提出扩展Druid,以满足日益增长的数据量和100GB tpc-h的数据集。 我们看到,当我们从8核心增加到48核,并不是所有类型的查询都能实现线性扩展,除了简单的聚合查询,如图12所示。并行计算系统的速度增长通常是由系统的顺序操作所需要的时间所决定的。在这种情况下，代理级别上大量需要查询工作不能并行化。 数据写入性能为了展示Druid的数据写入延迟，我们选择了几个不同维度、指标和事件的生产数据源。生产者设置由6节点,360 gb的内存和360核(12 x英特尔®Xeon®e5 - 2670)请注意，在这个设置中，还有其他几个数据源正在被处理，许多其他的Druid相关的数据写入任务在机器上同时运行。 Druid的数据写入延迟严重依赖于被写入的数据集的复杂性。数据的复杂性是由每个事件中维度的数量、每个事件中的指标数量以及我们想要在这些指标上执行的聚合类型决定的。使用最基本的数据集(只有一个时间戳列)，可以以800000个事件/秒/核心 的速率写入数据，这实际上只是一个我们可以快速反序列化事件的速度。真实世界的数据集从来没有这么简单。表3显示了数据源的选择和它们的特性。 我们可以看到，根据表3中的描述，延迟会发生显著的变化，维度和度量并不总是写入延迟的因素。我们在简单数据集上看到了一些较低的延迟，因为这是数据生产者提供数据的速率。结果如图13所示。我们将吞吐量定义为一个实时节点可以接收到的事件的数量，并且可以进行查询。如果将太多事件发送到实时节点，则这些事件将被阻塞，直到实时节点有能力接受它们。我们在生产中测量的峰值写入延迟为22914.43个事件/秒/核心，数据来源为30个维度和19个指标，运行一个Amazon cc2.8xlarge实例。 我们提出的延迟测试足以解决交互带来的问题。我们希望延迟时间的变化更少。通过增加额外的硬件来减少延迟仍然是可能的，但是我们没有选择这样做，因为基础设施成本仍然是我们的考虑因素。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
        <tag>外文翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-查询Api]]></title>
    <url>%2F2018%2F02%2F13%2FDruid-%E6%9F%A5%E8%AF%A2Api%2F</url>
    <content type="text"><![CDATA[查询ApiDruid有自己的查询语言，通过POST请求。Broker, historical, 和real-time nodes都共享相同的查询API。POST请求的主体是一个JSON对象，其中包含各种查询参数的键值对。一个典型的查询将包含数据源名称、结果数据的粒度、时间范围、请求的类型以及需要聚合的metric。结果也将是一个JSON对象，其中包含在时间段内的聚合指标。 大多数查询类型还支持一个过滤器集合。过滤器包括维度名和维度值的Boolean表达式，可以指定多个维和值的组合。当提供一个过滤器时，只有符合过滤器数据的子集将会被扫描。Druid有处理复杂的嵌套过滤器的能力，能够进行深度数据钻取。 具体的查询语法取决于查询类型和请求。查询一周的样本数据语法如下: { &quot;queryType&quot;: &quot;timeseries&quot;, &quot;dataSource&quot;: &quot;wikipedia&quot;, &quot;intervals&quot;: &quot;2013-01-01/2013-01-08&quot;, &quot;filter&quot;: { &quot;type&quot;: &quot;selector&quot;, &quot;dimension&quot;: &quot;page&quot;, &quot;value&quot;: &quot;Ke$ha&quot;}, &quot;granularity&quot;: &quot;day&quot;, &quot;aggregations&quot;: [{&quot;type&quot;:&quot;count&quot;, &quot;name&quot;:&quot;rows&quot;}] } 上面所示的查询将返回从2013-01-01到2013-01-08的Wikipedia数据源中行数统计，只对那些“page”维度的值等于“Ke$ha”的行进行filter计算。结果将按天进行分桶，并且将是一个JSON数组的形式: [ { &quot;timestamp&quot;: &quot;2012-01-01T00:00:00.000Z&quot;, &quot;result&quot;: {&quot;rows&quot;:393298} } { &quot;timestamp&quot;: &quot;2012-01-02T00:00:00.000Z&quot;, &quot;result&quot;: {&quot;rows&quot;:382932} } ... { &quot;timestamp&quot;: &quot;2012-01-07T00:00:00.000Z&quot;, &quot;result&quot;: {&quot;rows&quot;: 1337} } ] Druid支持多种类型的聚合，包括对浮点数和整数类型的sum、最小值、最大值和复杂聚合，如基数估计和近似分位数估计。聚合的结果可以结合在数学表达式中，形成其他的聚合。这篇文章的主旨是查询API，更多的信息可以在网上找到。 在撰写本文时，还没有实现对Druid的join查询。这是一个工程资源分配和用例决策的功能，而不是技术价值驱动的决策。事实上，Druid的存储格式将允许实现连接(这并不意味着失去了维度列)并且为了实现它我们交流了几个月。到目前为止，我们已经做出了这样的选择:实施成本不值得投资于我们的产品。做出这一决定有两方面原因。 在我们的实践中，join连接查询在分布式数据库中有不可抗的瓶颈。 添加繁重join查询功能的收益比提升高并发的问题要小。 join查询实质上是基于一组共享key的两个或多个数据流的合并。对于我们所知道的连接查询，主要基于hash策略或排序合并策略。基于哈希的策略要求，除了一个数据集之外，所有的数据集看起来都像一个散列表，然后在这个哈希表上对“主”流中的每一行执行查找操作。排序合并策略假定每个流都由连接键进行排序，从而允许流的加入。然而，每一种策略都需要以排序顺序或散列表形式实现一些具体化的流。 当连接的都是明显的大表(&gt; 10亿条记录)时，实现流join需要复杂的分布式内存管理。内存管理的复杂性会因为我们“高度并发的多租户”目标而被放大。就我们所知，这是一个积极的学术研究问题，我们愿意以可扩展的方式帮助解决这个问题。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
        <tag>外文翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-存储格式]]></title>
    <url>%2F2018%2F02%2F13%2FDruid-%E5%AD%98%E5%82%A8%E6%A0%BC%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[存储格式Druid的数据表(称为数据源)是事件的集合，并划分为一组segment，其中每个段通常为5 - 1000万行。形式上，我们将segment定义为一组数据的集合，这些数据跨越了一定时间周期。Segments是Druid基本存储单元，并在其上进行分配和备份。 Druid需要一个时间戳列来简化数据分配策略、数据保留策略和查询优化策略。Druid根据时间间隔划分数据源，通常为一个小时或一天，并可以进一步以其他列分割以实现所需的段大小。划分segment的时间粒度是数据量和时间范围的函数。跨年的时间粒度会比天级的粒度被更好的划分，跨天的时间粒度会比小时级更优。 段是由数据源标识符、数据的时间区间和在创建段时的版本号来联合标识的（唯一）。版本字符串表示段数据的最新程度;具有较晚版本的片段(在一定时间范围内)比较老版本的片段具有更新的数据视图。此段元数据被系统用于并发控制;特定的时间范围内的读取操作总是访问最新版本的数据。 Druid段以列存储存储。考虑到Druid最适合聚合事件流(所有进入Druid的数据都必须有时间戳)，将聚合信息作为列存储而不是行存储的优势具有较好的文档记录[1]。列存储允许更高效的利用CPU，因为只需要加载和扫描所需的列的内容。在面向行的数据存储中，与行关联的所有列必须作为聚合的一部分进行扫描。额外的扫描会引起性能下降[1]。 Druid有多个列类型来表示各种数据。根据列类型不同，使用不同的压缩算法来降低内存和磁盘的存储成本。在表1给出的示例中，页面、用户、性别和城市列仅包含字符串。直接存储字符串是不必要的开销，而字符串列可以是字典编码的。字典编码是一种常用的压缩数据的方法，它已经被广泛用于其他的数据存储如PowerDrill[17]。在表1中的示例中，我们可以将每个page字段映射到一个惟一的整数标识符。 Justin Bieber -&gt; 0 Ke$ha -&gt; 1 这个映射允许我们将page列表示为一个integer数组，数组索引对应于原始数据集的行。 [0, 0, 1, 1] 所得到的Integer数组很适合压缩算法。关于编码的通用压缩算法在列存储中非常常见。Druid使用LZF [24] 压缩算法。 类似压缩方法可以应用于数值列。例如，在表1中Characters Added|Characters Removed列也可以表示为单个数组。 Characters Added -&gt; [1800, 2912, 1953, 3194] Characters Removed -&gt; [25, 42, 17, 170] 在这种情况下，我们压缩原始值而不是它们的字典表示值。 索引、过滤数据在许多现实的OLAP工作流中，通常根据某些特定维度聚合指标的数值。举例来说:“在旧金山的用户中，有多少编辑维基百科的人是男性的?”这个查询基于一个维度值的布尔表达式来过滤表1中的Wikipedia数据集合。在许多真实的数据集中，维度列通常是字符串，而度量列通常是数值。Druid为字符串列创建额外的查询索引，这样就只扫描那些与特定查询筛选相关的行。 让我们考虑前面章节表1中的page列。对于表1中的每个独一无二的page，我们可以创建一些特定页面的标示。我们可以将这些信息存储在一个二进制数组中，其中数组索引表示我们的行。如果某个特定的页面出现在某个行中，那么该数组索引将被标记为1。例如: Justin Bieber -&gt; rows [0, 1] -&gt; [1][1][0][0] Ke$ha -&gt; rows [2, 3] -&gt; [0][0][1][1] Justin Bieber在第0和第1行。这个由列值映射到行索引的映射形成了一个反向索引[39]。要知道是哪条记录包含Justin Bieber和Ke$ha，将两个数组进行OR运算。 这种在大型位图集合上执行布尔操作的方法通常用于搜索引擎。OLAP的位图索引在[32]中详细描述。位图压缩算法常用于搜索领域[2,44,42]以及运行长度编码。Druid 选择使用Concise算法[10]。图7显示了使用整数数组的Concise压缩的字节数。以上结果基于cc2.8xlarge系统，单线程、2G堆内存、512m young内存，以及每次运行之间的强制GC。该数据集是Twitter garden hose一天的数据[41]数据流收集的单日数据。数据集包含2,272,295行和12个不同基数的维度。作为一个额外的比较，我们也采用数据集行来最大化压缩。 在未排序的情况下，总的Concise大小为53,451,144字节，总整数数组大小为127,248,520字节。总的来说，Concise的压缩比整型数组小约42%。在排序的情况下，总Concise压缩大小为43,832,884个字节，总整数数组大小为127,248,520字节。有趣的是，在排序之后，全局压缩只增加了最低限度。 存储引擎Druid的持久化组件允许插入不同的存储引擎，类似Dynamo[12]。这些存储引擎可以将数据存储在一个内存中，例如JVM堆或内存映射结构。swap存储引擎的能力允许根据特定应用程序的规范配置Druid。内存存储引擎可能比内存映射的存储引擎更昂贵，但是如果性能很重要，它可能是更好的选择。默认情况下，使用内存映射存储引擎。 当使用内存映射存储引擎时，Druid依赖于操作系统来分配段的内存。考虑到段只能在内存中进行扫描，内存映射存储引擎允许最近的段保留在内存中，而从不查询的段被移出。使用内存映射存储引擎的主要缺点是，当一个查询需要将更多的段放到内存中，而不是给定节点的容量时。在这种情况下，查询性能将受到段移入和移出内存的成本的影响。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
        <tag>外文翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-架构]]></title>
    <url>%2F2018%2F02%2F12%2FDruid-%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[架构体系Druid集群由不同类型的节点组成，每个节点类型被设计用于执行特定的一组事情。我们认为，这种设计将关注点分离，简化了整个系统的复杂性，不同的节点类型之间相互独立，而且它们之间的交互很少。因此，集群内通信故障对数据可用性的影响最小。 为了解决复杂的数据分析问题，不同的节点类型聚在一起形成一个完整的工作系统。Druid这个名字来自于许多角色扮演游戏中的德鲁伊，它是一个变形者，能够在一个群体中扮演各种不同的角色。德鲁伊集群中数据的组成和数据流如图1所示。 Real-time NodesReal-time Nodes的功能是”数据注入”和”查询事件流”。Event通过这些节点创建索引并立即用于查询。这些节点只关心一些小的时间范围内的Event，并且周期性地将它们在这个小时间范围内收集到的”不可变数据集”并传递给Druid集群中的其他节点，这些节点专门处理一些”不可变数据集”。实时节点利用Zookeeper[19]与其他Druid节点进行协调。这些节点宣告它们的在线状态和它们在Zookeeper中服务的数据。 Real-time Nodes为所有注入的事件在内存中维护的索引缓冲区。这些索引随着事件的注入而递增，索引也可以直接查询。Druid的行为像是基于JVM堆内存的行式存储。为了避免堆溢出问题，Real-time Nodes会周期性地或在达到某个最大行限制后将内存索引持久化到磁盘。这个持久化过程将存储在内存缓冲区中的数据转换为第4节中描述的面向列存储格式。每个持久的索引都是不可变的，Real-time Nodes将持久的索引加载到堆外内存中，这样它们仍然可以被查询。这个过程在[33]中被详细描述，如图2所示。 定期地，每个Real-time Node将调度一个后台任务，搜索所有本地持久化的索引。该任务将这些索引合并在一起，并构建一个不可变的数据块，该数据块包含一个实时节点在一定时间内所接收到的所有事件。我们将此数据块称为“segment”。在转换阶段，实时节点将此段上传至永久备份存储，通常是一个分布式文件系统，如S3[12]或HDFS[36]，而Druid将其称为“deep storage”。ingest、persist、merge和handoff步骤都是流畅的;在任何过程中都没有数据丢失。 图3说明了Real-time Node的操作。节点从13:37开始，只接受当前小时或下一个小时的事件。当事件被注入时，该节点宣布它将从13:00到14:00的时间段内服务一个数据段。每10分钟(持久化周期是可配置的)，节点将刷新并将其内存缓冲区保存到磁盘。在临近结束时，节点很可能会在看到14:00到15:00之间的事件。当出现这种情况时，节点准备为下一个小时提供数据，并创建一个新的内存索引。然后，节点宣布它也从14:00到15:00服务一个段。从13:00到14:00，节点不会立即合并持久索引，而是等待从13:00到14:00的离散事件的可配置窗口期。这个窗口期最小化了在事件交付过程中数据丢失的风险。当窗口期结束，节点合并从13:00到14:00的所有索引到一个不变的段(segment)。一旦这个段在其他Druid集群中被加载并可查询时，实时节点将”下架”13:00到14:00的数据。 可用性和可伸缩性Real-time nodes是数据的使用者，需要一个对应的生产者来提供数据流。一般来说，对于数据耐用性的目的，位于生产者和实时节点之间会有一个像Kafka[21]这样的消息总线，如图4所示，实时节点通过从消息总线中读取事件来获取数据，从事件创建到事件消耗的时间通常为数百毫秒。 图4中的消息总线有两个目的。首先，消息总线充当传入事件的缓冲区。像Kafka这样的消息总线维护了位置偏移量——表明了在事件流中，一个消费者(一个实时节点)已经读取了多少。消费者可以通过编程方式更新这些偏移量。实时节点在每次将内存缓冲区保存到磁盘时更新此偏移量。在失败和恢复场景中，如果一个节点没有丢失磁盘，它可以从磁盘重新加载所有持久的索引，并从它所提交的最后偏移量中继续读取事件。从最近的提交点注入事件可以大大减少一个节点的恢复时间。在实践中，我们发现节点在几秒钟内便从这些失败场景中恢复过来。 消息总线的第二个目的是充当单个端点，多个实时节点可以从该端点读取事件。多个实时节点可以从总线上接收相同的事件集合，从而创建事件的复制。在一个场景中，一个节点commit失败并丢失磁盘，复制的流确保没有数据丢失。单个注入端点还允许对数据流进行分区，从而使多个实时节点都能接收到流的一部分。这允许无缝地添加额外的实时节点。在实践中，该模型使最大的Druid生产集群能够以大约500 MB/s(15万Event/s或2 TB/小时)的速度消耗原始数据。 Historical NodesHistorical Nodes功能是加载和服务由实时节点创建的不可变数据块(段)。在许多实际的工作流中，在Druid集群中加载的大多数数据是不可变的，因此，Historical Nodes通常是Druid集群的主要工作人员。Historical Nodes遵循无中心架构，节点之间没有独立的连接点。节点之间没有相互了解，操作上也很简单;它们只知道如何加载、删除和服务“不可变段”。 类似于实时节点，历史节点在Zookeeper声明它们的在线状态和它们服务中的数据。加载和下架段的指令被发送到Zookeeper上，其中包含该segment位于deep storage的位置信息，以及如何解压缩和处理该segment。在历史节点从深度存储中下载特定的段之前，它首先检查一个本地缓存，该缓存维护节点上已经存在的段的信息。如果缓存中不存在某个段的信息，那么历史节点将继续从深度存储中下载该段。这个过程如图5所示。一旦成功完成，该部分将在zookeeper中宣布。此时，该段是可查询的。本地缓存还允许快速更新和重新启动历史节点。在启动时，节点检查它的缓存并立即提供它找到的任何数据。 历史节点可以支持读取一致性，因为它们只处理不可变数据。不可变数据块还支持简单并行化模型:历史节点可以同时扫描和聚合不可变块，而不会阻塞。 Tiers机制历史节点可以分在不同的tiers中，其中给定tiers中的所有节点都是相同配置的。可以为每一tier设置不同的性能和容错参数。节点分层的目的是根据segment的重要性，分配高、低优先级。例如，可以创建一个“hot”级的历史节点，这些节点具有高的内核数和较大的内存容量。可以将“hot”集群配置为更频繁下载访问的数据。一个并行的“cold”集群也可以用不太强大的支持硬件来创建。“cold”集群只包含较少被访问的部分。 可用性历史节点依赖于Zookeeper的段“加载”和“下架”指令。如果Zookeeper变得不可用，那么历史节点就不再能够提供新的数据或删除过时的数据，因为查询是通过HTTP提供的，而历史节点仍然能够响应查询请求，以获取当前服务的数据。这意味着，Zookeeper中断不会影响历史节点上的当前数据可用性。 Broker NodesBroker nodes充当历史和实时节点的查询路由器。Broker节点知晓在Zookeeper中发布的关于哪些段是可查询的以及这些段所在位置的元数据。代理路由传入的查询，这样查询就会命中正确的历史节点或实时节点。Broker节点还将历史和实时节点的部分结果合并，然后将最终合并结果返回给调用者。 缓存Broker nodes包含一个LRU[31, 20]失效策略的缓存。缓存可以使用本地堆内存或外部分布式key/value存储[16]。每当一个broker节点接收到一个查询时，它首先将查询映射到一组segments。某些段的结果可能已经存在于缓存中，不需要重新计算它们。对于缓存中不存在的任何结果，broker节点将向正确的历史和实时节点转发查询。一旦历史节点返回其结果，代理将会将这些结果缓存到一个基础段中以供将来使用。这个过程如图6所示。实时数据永远不会被缓存，因此实时数据的请求将被转发到实时节点。实时数据永远在变化，缓存结果是不可靠的。 缓存还可以额外作为数据可靠性级别。在所有历史节点都失败的情况下，如果缓存中已经存在这些结果，仍然可以查询结果。 可靠性在整个Zookeeper中断的情况下，数据仍然是可查询的。如果broker节点无法与Zookeeper进行通信，它们将使用集群的最后一个memory，并继续将查询转发到实时和历史节点。broker节点假定集群的结构与中断前的结构相同。在实践中，这个可用性模型允许我们的Druid集群在诊断Zookeeper宕机的时候继续为查询服务。 Coordinator NodesDruid Coordinator Nodes（协调节点）主要负责历史节点的数据管理和分配。协调器节点告诉历史节点加载新数据、删除过时数据、复制数据，并将数据移动到负载平衡。Druid使用多版本并发控制交换协议来管理不可变段，以保持稳定的视图。如果任何不可变的segment包含被完全废弃的数据，那么过时的片段就会从集群中删除。协调节点经历了一个领导选举过程，决定运行协调功能的节点。其余的协调节点充当冗余备份。 Coordinator Nodes定期运行以确定集群的当前状态。它通过将集群的期望状态与运行时集群的实际状态进行比较，从而做出决策。与所有Druid节点一样，协调节点维护当前集群信息的Zookeeper连接。协调节点还维护与一个MySQL数据库的连接，该数据库包含额外的操作参数和配置。MySQL数据库中的关键信息之一是包含所有应该由历史节点服务的所有段的列表。这个表可以通过创建段(例如实时节点)的任何服务来更新。MySQL数据库还包含一个规则表，该规则表在集群中管理如何创建、销毁和复制段。 Rules“Rules”管理如何从集群中加载和删除历史片段。”Rules”指示如何将段分配到不同的历史节点tier，以及每个tier中应该存在多少个片段的备份。”Rules”也可能指出何时应该完全从集群中删除段。”Rules”通常设置一段时间。例如，用户可以使用规则将最近的一个月的片段加载到一个“热”集群中，将最近一年的部分划分为“冷”集群，并删除较老的部分。 coordinator nodes从MySQL数据库中的规则表加载一组规则。规则可能是特定于某个数据源的，或者可以配置一个默认的规则集。coordinator nodes将循环遍历所有可用的段，并将每个seg与应用于它的第一条规则相匹配。 Load Balancing在典型的生产环境中，查询常常会命中几十个甚至几百个段。由于每个历史节点都有有限的资源，所以必须在集群之间分配段，以确保集群负载平衡。确定最优的负载分布需要一些关于查询模式和速度的信息。一般情况下，查询涵盖了单个数据源的跨时间间隔的最近段。平均而言，访问较小段的查询速度更快。 这些查询模式建议在更高的速率上复制最近的历史segment，将大量的片段分散到不同的历史节点上，并从不同的数据源中联合定位片段。为了在集群中最优地分配和平衡segments，我们开发了一个基于成本的优化过程，该过程考虑了段数据源、距离和大小。该算法的具体细节超出了本文的范围，可以在以后的文献中讨论。 ReplicationCoordinator nodes可以告诉不同的历史节点加载同一段的副本。历史计算集群的每个tier中复制的数量是完全可配置的。需要高水平容错的设置可以配置为具有大量的副本。备份的段与原始的段相同，并遵循相同的负载分配算法。通过备份段，单个历史节点故障在Druid集群中是透明的。我们使用此属性进行软件升级。我们可以无缝地将一个历史节点脱机，更新它，将其恢复，并对集群中的每个历史节点重复这个过程。在过去的两年里，我们从来没有在我们的Druid集群中进行软件升级。 可靠性Druid协调节点将Zookeeper和MySQL作为外部依赖关系。协调节点依赖于Zookeeper来确定集群中已经存在的历史节点。如果Zookeeper是不可用的，那么协调器将不再能够发送指令来分配、平衡和删除部分。然而，这些操作并不影响数据的可用性。 对MySQL和Zookeeper失败的响应的设计原则是相同的:如果一个负责协作的外部依赖项失败，集群将维持现状。Druid使用MySQL存储操作管理信息和段元数据信息，了解哪些片段应该存在于集群中。如果MySQL宕机，则此信息将无法用于协调节点。然而，这并不意味着数据本身是不可用的。如果协调节点无法与MySQL通信，它们将停止分配新的段并删除过时的部分。在MySQL中断期间，代理、历史和实时节点仍然可以查询。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
        <tag>外文翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-背景]]></title>
    <url>%2F2018%2F02%2F12%2FDruid-%E8%83%8C%E6%99%AF%2F</url>
    <content type="text"><![CDATA[提出问题Druid最初的设计目的是解决围绕数据注入和探索大量事务事件(日志数据)的问题。这种timeseries形式的数据通常在OLAP中出现，而数据的性质往往非常复杂。例如，考虑表1所示的数据。表1包含在Wikipedia上发生编辑的数据。每次用户编辑Wikipedia中的页面时，都会生成一个关于”编辑”的元数据的事件。 Timestamp Page Username Gender City Characters Added Characters Removed 2011-01-01T01:00:00Z Justin Bieber Boxer Male San Francisco 1800 25 2011-01-01T01:00:00Z Justin Bieber Reach Male Waterloo 2912 42 2011-01-01T02:00:00Z Ke$ha Helz Male Calgary 1953 17 2011-01-01T02:00:00Z Ke$ha Xeno Male Taiyuan 3194 170 此元数据由3个不同的部分组成。首先，有一个时间戳列，指示何时进行编辑。接下来，有一个维度列，指示编辑器(例如编辑的页面、编辑的用户和用户的位置)的各种属性。最后，有一组度量列包含可以聚合的值(通常是数值)，例如在编辑中添加或删除的字符数。我们的目标是快速地对这些数据的下钻和聚集。我们想回答这样的问题:“在旧金山，贾斯汀·比伯在页面上做了多少编辑?”以及“在一个月的时间里，来自Calgary的人们添加的字符的平均数量是多少?”我们还希望对任意维度的任意组合进行查询，以亚秒级延迟返回。 由于现有的开源关系数据库管理系统(RDBMS)和NoSQL键/值存储无法为交互式应用程序提供低延迟的数据输入和查询平台[40]，因此需要使用Druid。在Metamarkets的早期，我们专注于构建一个托管的仪表板，允许用户任意地探索和可视化事件流。数据存储为仪表板提供了强大的支持，使其能够快速返回查询，使数据可视化能够为用户提供交互式体验。 除了查询延迟需求之外，系统还必须是多租户和高可用的。在高度并发的环境中使用了Metamarkets产品。如果一个系统在软件升级或网络故障的情况下无法使用，停机时间将会非常昂贵，而且许多企业不愿意等待。创业公司通常缺乏适当的内部运营管理，因此，宕机往往决定企业的成败。 最后，Metamarkets在早期面临的另一个挑战是允许用户和警报系统在“实时”中做出业务决策。当”一个事件被创建”到”该事件是可查询”的时候，时延就决定了跨部门的各方能够对其系统中潜在的灾难性情况作出反应。流行的开源数据仓库系统，如Hadoop，无法提供我们需要的亚秒数据注入延迟。 多个行业面临数据挖掘、注入和可用性的问题。自从Druid在2012年10月开源之后，它作为一个视频、网络监控、操作监控和在线广告分析平台在多个公司中部署。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
        <tag>外文翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Druid-概述]]></title>
    <url>%2F2018%2F02%2F12%2FDruid-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[Druid系列文章为笔者对Druid white paper的翻译文章，如需转载请注明出处。 概述Druid是一个用于大规模数据集的实时分析处理的数据存储系统。该系统结合了列存储、分布式的、无共享的体系结构，高效的索引能够支持10亿行数据分析查询的亚秒级响应。在本文中，我们描述了Druid的架构，以及它如何支持快速聚合，灵活的过滤器，以及低延迟的数据注入。Categories and Subject DescriptorsH.2.4 [Database Management]: Systems—Distributed databases关键字distributed; real-time; fault-tolerant; highly available; open source; analytics; column-oriented; OLAP 简介近年来，互联网技术的迅猛发展使机器生成的事件激增。单独来说，这些事件包含的有用信息非常少，而且价值很低。考虑到从大量数据中提取有价值的数据所需的时间和资源，许多公司愿意放弃这些数据。尽管已经建立了基础设施来处理基于事件的数据(例如IBM的Netezza[37]、HP的Vertica[5]和EMC的Greenplum[29])，但它们在很大程度上以高价出售，而且只针对那些能够支付得起的公司。 几年前，谷歌引入了MapReduce[11]作为其利用商品硬件的机制来索引网络和分析日志。Hadoop[36]项目很快就完成了，并且很大程度上是基于最初的MapReduce理论而形成的。Hadoop目前部署在许多组织中，以存储和分析大量的日志数据。Hadoop帮助公司将其低价值的事件流转化为高价值的聚合，用于各种应用程序，如商业智能和测试。 与许多伟大的系统一样，Hadoop已经打开了我们的视野，也让我们看到新的问题。具体来说，Hadoop擅长存储和提供大量数据，但是它不能保证数据访问的速度有多快。此外，尽管Hadoop是一个高度可用的系统，但是在大量并发负载下性能会降低。最后，尽管Hadoop可以很好地存储数据，但它并没有对数据进行优化，使数据立即可读。 在开发Metamarkets产品的早期，我们遇到了这些问题，并认识到Hadoop是一个很好的后台处理、批处理和数据仓库系统。但是，作为一个在高度一致的租户环境(1000+用户)中具有产品级保证的公司，查询性能和数据可用性方面，Hadoop并不能满足我们的需求。 我们探索了不同的解决方案，在尝试了关系数据库管理系统和NoSQL技术架构之后，我们得出了这样的结论:在开源世界中没有任何东西可以完全满足我们的需求。我们最终创建了Druid，一个开源的、分布式的、基于列的、实时的分析数据存储。在许多方面，Druid与其他OLAP系统(30、35、22)、在交互查询系统[28]、内存数据库[14]以及广为人知的分布式数据存储(7、12、23)有相似之处。在分布式和查询模型上也借鉴了当前的生成搜索基础结构[25,3,4]。本文描述了Druid的架构，探索了在创建系统的过程中所做的各种决策设计，它为托管服务提供了动力，并试图帮助任何一个面临类似问题的人。Druid被部署在几个技术公司的生产环境中中。本文的结构如下:首先第2节中描述Druid解决的问题。接下来，第3节我们将从数据流角度详细介绍系统架构。在第4节中，我们讨论如何以及为什么数据转换成二进制格式。第5节中简要描述了查询API，并在第6节中展示了性能结果。最后，我们将从第7节中运行Druid、第8节中总结经验。]]></content>
      <categories>
        <category>Druid</category>
      </categories>
      <tags>
        <tag>OLAP</tag>
        <tag>Druid</tag>
        <tag>外文翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[序 / INTRO]]></title>
    <url>%2F2018%2F02%2F10%2F%E5%BA%8F-INTRO%2F</url>
    <content type="text"><![CDATA[我们要有最朴素的生活 与最遥远的梦想即使明日天寒地冻 路远马亡 WELCOME HUBERY’S POLARIS 这里是徐海滨的博客 主要更新一些技术文章、个人作品 请持续关注哟~~~ 比心]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ganglia原理介绍、安装使用]]></title>
    <url>%2F2018%2F02%2F10%2FGanglia%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D%E3%80%81%E5%AE%89%E8%A3%85%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Ganglia简介：Ganglia是UC Berkeley发起的一个开源集群监视项目，设计用于测量数以千计的节点。Ganglia的核心包含gmond、gmetad以及一个Web前端。主要是用来监控系统性能，如：cpu 、mem、硬盘利用率， I/O负载、网络流量情况等，通过曲线很容易见到每个节点的工作状态，对合理调整、分配系统资源，提高系统整体性能起到重要作用。 每台计算机都运行一个收集和发送度量数据的名为 gmond 的守护进程。接收所有度量数据的主机可以显示这些数据并且可以将这些数据的精简表单传递到层次结构中。正因为有这种层次结构模式，才使得 Ganglia 可以实现良好的扩展。gmond 带来的系统负载非常少，这使得它成为在集群中各台计算机上运行的一段代码，而不会影响用户性能。所有这些数据多次收集会影响节点性能。网络中的 “抖动”发生在大量小消息同时出现时，可以通过将节点时钟保持一致，来避免这个问题。 gmetad可以部署在集群内任一台节点或者通过网络连接到集群的独立主机，它通过单播路由的方式与gmond通信，收集区域内节点的状态信息，并以XML数据的形式，保存在数据库中。由RRDTool工具处理数据，并生成相应的的图形显示，以Web方式直观的提供给客户端。 工作原理： Ganglia包括如下几个程序，他们之间通过XDR(xml的压缩格式)或者XML格式传递监控数据，达到监控效果。集群内的节点，通过运行gmond收集发布节点状态信息，然后gmetad周期性的轮询gmond收集到的信息，然后存入rrd数据库，通过web服务器可以对其进行查询展示。 安装ganglia网上示例很多，对该部分的翻译后续再跟进。 配置ganglia默认的配置仅仅能使ganglia工作，如果了解更多的配置项，能帮助你更好的使用ganglia做监控部署。 gmod安装在每个想监控的节点上，与操作系统交互获取度量信息（cpu使用率、内存、网络以及其他可通过自定义拓展的度量）并与集群内其他节点共享。每个在集群内的gmod实例知晓所有gmond所在节点的度量值，并通过XML格式的dump对外提供访问，gmetad通过gmond的连接端口连接。 拓扑结构gmond的默认拓扑结构采用广播的方式(multicast)，意味着所有集群内节点发送并接受度量信息，并以hash table的结构保存到各自的内存数据库中，包括所有集群节点的度量信息。如下图所示： 上图图解最重要的是与gmond deamon全然不同的性质。在内部，gmond发送和接受两种行为是无关联的（正如图中垂直虚线所示）。gmond不会自我交互，它只会向网络中发送度量信息。任何本地节点的信息获取都会经过sender传输到网络中，再由receiver从网络中收集。 这种拓扑对大多数场景适用，但某些案例下，指定少数监听者比每个节点都监听集群内度量信息要更行之有效，因为每个节点都监听会浪费额外的cpu，更多细节详见第三章。 通过“聋哑”模式，如下图，可以消除大集群内的过度联系。聋人模式和哑人模式使一些gmond按特定的模式工作，如收集或者发送。哑人模式意味着节点不对外传输数据，但收集集群内其他节点的度量信息。聋人模式意味着该实例不接受网络中的度量信息，但它不是哑巴，它会持续向网络中的其他监听节点发送度量信息。 并非所有的拓扑模型都要求使用广播。当广播不是第一选择的时候，聋哑模式的拓扑可由UDP单播实现: 或者，可以混合使用“聋哑”模式和默认的拓扑创建适合你环境的系统架构。唯一的拓扑要求如下所述： 在集群内至少有一个gmond实例负责接收网络中的所有度量信息。 gmetad必须定期轮训gmond以拉去集群状态信息。 然而真实实践中，不具备广播连通性的节点不需要配置成”deaf”模式；他们可以使用127.0.0.1地址为自己发送消息，并保留自己的本地度量信息。这可以利用于节点自己发送TCP探针（XML）来排查故障。 2.配置文件可以用以下gmond命令生成默认的配置文件： user@host:$ gmond -t 配置文件由多个部分组成，用花括号包裹，这可以大致分为两个逻辑目录。第一部分包含节点与集群的配置；另一部分包括度量信息的收集和定时策略。 所有的配置属性大小不敏感，如以下例子是相等的名称： name NAME Name NaMe 有些配置部分是可选的；一些配置是必填的。有些可以定义多次，有些只能配置一次。有些配置可以包含子选项配置。 当配置复杂时，有些内部指令可以将gmond.conf分离到多个文件中，并支持类型通配。如： include (&apos;/etc/ganglia/conf.d/*.conf&apos;) 将会命令gmond加载/etc/ganglia/conf.d/路径下所有以.conf为后缀的配置文件。 PS：为了快速开始，你所需要配置的仅仅是cluster部分下的name属性，其他属性可以全部默认。 配置文件使用第三方API工具libconfuse解析，采用libconfuse正常的格式规范。值得说明的是，boolean类型的值可以是yes,true,on；与他们相对的是no,false,off。布尔类型是大小写不敏感的。 有以下八个部分配置节点自身属性: Section: globals.配置守护线程自身的通用属性，配置文件中只能够配置一次。以下是Ganglia3.3.1的默认配置：123456789101112131415globals &#123; daemonize = yes setuid = yes user = nobody debug_level = 0 max_udp_msg_len = 1472 mute = no deaf = no allow_extra_data = yes host_dmax = 86400 /*secs. Expires (removes from web interface) hosts in 1 day */ host_tmax = 20 /*secs */ cleanup_threshold = 300 /*secs */ gexec = no send_metadata_interval = 0 /*secs */&#125; daemonize (boolean)当配置为true时，gmond在后台并行运行。当配置为false时，你可以使用deamontools等守护线程管理工具运行。 setuid (boolean)当设置为true时，gmond以user属性下的有效uid运行，否则不改变有效用户。 debug_level (integer value)当设置为0时，gmond正常运行。等级越高，输出的日志信息越丰富。 max_udp_msg_len (integer value)udp单包的最大长度，不建议修改。 mute (boolean)当设置为true时，gmond不会发送数据，忽略其他的配置指令。只用于收集其他gmond信息，但仍然会响应如gmetad轮询者的请求。 deaf(boolean)当设置为ture时，gmond不会接收数据，忽略其他配置指令。当置于大规模集群中，或者HPC敏感的网格中，CPU消耗成为不得不考虑的因素时，通常节点会配置为deaf模式以确保集群之间的交互降到最低。在这种情境下，部分节点设置为mute专注于收集。如此一来，mute节点的状态信息并不用于集群总状态的评估。它们的作用只是用于收集，所以他们的节点状态会污染集群状态信息。 allow_extra_data (boolean)当设置为false时，gmond不会发送标记为EXTRA_ELEMENT和EXTRA_DATA的XML部分。如果你使用自己的前端平台，这个参数可以有效节省带宽。 host_dmax (integer_value in seconds)代表“delete max”，当设置为0时，gmond永远不会从他们的列表中删除节点，即时某些远程节点丢失报告对量。如果host_dmax 设置为比0大的自然数，gmond会在超过这个时间后刷新host列表。 host_tmax (integer_value in seconds)代表“timeout max”。gmond更新host状态的最大等待时间。因为消息会在网络中丢失，所以gmond会在该超时时间未接收到数据后判定该节点down掉。 cleanup_threshold (integer_value in seconds)gmond清理过期数据的最大时间。 gexec (boolean)当设置为true，gmond允许节点执行gexec job，这个方法要求本节点gexecd已运行并且安装了合适的键。 send_metadata_interval (integer_value in seconds)设置gmond发送或重发包含度量信息的元数据包的时间间隔。默认设置为0，这意味着gmond只有当启动时，或者别的远程节点请求时才会发送数据包。当一个新节点gmond加入集群时，需要通知自己和其他所有节点当前的支持状态。在广播模式下，这不是个问题，但是单播模式下，该时间间隔必须设置，表示两次发送数据的时间间隔。 module_dir (path; optional)指定度量收集模块所在的目录位置。如果忽略，则默认是编译时期的配置项：–with-moduledir。这个配置项，默认是Ganglia目录下的libganglia所在目录，运行如下指令生成默认gmond可发现的配置文件 #gmond -t Section: cluster.每个gmond节点向集群报告信息都通过cluster部分的配置。默认值设置为”unspecified”；默认值是系统可用的，该部分在配置文件中只能配置一次。以下是默认配置：123456cluster &#123; name = &quot;unspecified&quot; owner = &quot;unspecified&quot; latlong = &quot;unspecified&quot; url = &quot;unspecified&quot;&#125; name (text)指定集群名称。当节点轮训拉取xml描述的节点状态信息时，该名称会被插入到CLUSTER部分。gmetad会根据这个值在拉取时归并到不同的RRD文件中存储。该配置项取代了在gmetad.conf中的cluster name配置项。 owner (text)配置集群管理员。 latlong (text)指定集群在地球上的GPS经纬度坐标。 url (text)指定集群的特定URL地址访问信息，如集群目的和使用明细。 Section: host.指定运行该gmond实例的host地址。只有一个配置项：123host &#123; location = &quot;unspecified&quot;&#125; location (text)节点地址，rack,U[,blade]格式也是可用的。 Section: UDP channels.配置gmond节点与其他节点对话的UDP发送/接收渠道。集群通过UDP通道交互，这意味着，所谓集群只是gmond节点直接的发送和接收消息的通道组成。默认情况下，每个gmond节点通过UDP广播向其他节点广播度量信息，其他节点类似。这样很容易启动和维护：每个节点在集群中共享广播地址，并且新增节点自动发现。然而，当我们回顾之前的deaf and mute模式，某些情况下需要单独指定单播地址。由此，每一个gmond的发送和接收频道需要针对当前环境进行配置。每个发送通道的配置定义了一个新的发送自身度量信息的方式，每个接收通道的配置定义了从其他节点接收度量信息的方式。通道可以通过IP4-IP6进行单播或者广播。 记住，一个gmond节点不可向多个集群发送度量信息，也不要试图从其他集群节点接收度量信息。 UDP通道通过udp_(send|receive)_channel部分创建。默认发送通道如下：123456udp_send_channel &#123; #bind_hostname = yes mcast_join = 239.2.11.71 port = 8649 ttl = 1&#125; bind_hostname (boolean; optional, for multicast or unicast)配置是否通过机器名绑定。 mcast_join (IP; optional, for multicast only)当指定时，gmond将会通过创建udp连接并且加入广播组，该配置创建广播渠道并与host配置二选一。 mcast_if (text; optional, for multicast only)指定时，gmond通过指定接口发送数据。如：eth0 host (text or IP; optional, for unicast only)指定发送数据地址，与mcast_join二选一。 port (number; optional, for multicast and unicast)gmond发送数据使用端口，默认8649 ttl (number; optional, for multicast or unicast)time-to-live存活时间，该配置项对广播环境 尤其重要，用于限制度量信息的有效时间，越高的值，则容忍性越大。 如下是默认的接收通道配置：12345udp_recv_channel &#123; mcast_join = 239.2.11.71 port = 8649 bind = 239.2.11.71&#125; mcast_join (IP; optional, for multicast only)当指定时，gmond将从该IP所在的广播群组中接收广播信息，如果不指定广播属性，gmond将会通过特定端口创建UDP单播服务。 mcast_if (text; optional, for multicast only)同上； bind (IP; optional, for multicast or unicast)指定后，gmond将会和本地地址绑定。 port (number; optional, for multicast or unicast)接收端口，默认8649 family (inet4|inet6; optional, for multicast or unicast)ip版本，默认inet4。如果想监听ipv4和ipv6两个网络，需要配置两个接收渠道。 acl (ACL definition; optional, for multicast or unicast)access control list：细粒度的接受渠道控制。详见“Access control”章节。 Section: TCP Accept Channels.gmond与gmetad或者其它轮询者交互通过TCP通道。可选如下配置项，默认如下：123456789tcp_accept_channel &#123; port = 8649&#125;bind (IP; optional)port (number)family (inet4|inet6; optional)interface (text; optional)acl (ACL definition; optional) Access control. 即acl，udp_recv_channel和tcp_accept_channel 的配置项。这个配置可以指定具体地址或者地址范围来添加gmond的连接许可。如下是个ACL示例：12345678910111213acl &#123; default = &quot;deny&quot; access &#123; ip = 192.168.0.0 mask = 24 action = &quot;allow&quot; &#125;access &#123; ip = ::ff:1.2.3.0 mask = 120 action = &quot;deny&quot; &#125;&#125; 配置遵从第一优先级。mask可指定路由范围。 Optional section: sFlow. sFlow是产品级的管理高速网络交换的技术。起初设想嵌入网络硬件内，现在存在于操作系统级别，如同其他应用一样如tomcat等web容器。gmond可以配置为成为sFlow的收集器，打包sFlow的数据包并发送给gmetad。在第八章中详细介绍。该配置全部可选，以下是默认配置：12345678910#sflow &#123;# udp_port = 6343# accept_vm_metrics = yes# accept_jvm_metrics = yes# multiple_jvm_instances = no# accept_http_metrics = yes# multiple_http_instances = no# accept_memcache_metrics = yes# multiple_memcache_instances = no#&#125; udp_port (number; optional)用于接收sFlow数据的端口。 Section: modules.该配置包含了加载度量模块的必须参数。gmond可收集所有动态加载的可扩展的度量模块。详见第五章。每个模块至少包含一个module子目录。该子目录由5个属性组成。默认配置包括所有已安装的度量插件，所以除非你有新增的度量插件，不然无需更改。示例如下：123456789101112131415modules &#123; module &#123; name = &quot;example_module&quot; language = &quot;C/C++&quot; enabled = yes path = &quot;modexample.so&quot; params = &quot;An extra raw parameter&quot; param RandomMax &#123; value = 75 &#125; param ConstantValue &#123; value = 25 &#125; &#125;&#125; name (text)如果使用c/c++实现，则该参数由模块结构决定。如果使用phthon等解释性语言编写，则由源文件决定。 language (text; optional)文件源码的实现语言，默认是c/c++，目前只支持c/c++或者python。 enabled (boolean; optional)方便该度量插件的启停。默认为yes； path (text)gmond加载度量插件的路径(c/c++动态加载)，如果不是绝对路径，则在前补上globals模块下的module_path属性。 params (text; optional)加载插件时的string参数。 Section: collection_group.该目录指定gmond收集那些度量信息，以及收集和广播的频率。你可以 尽可能多的将待收集的度量信息分组。每个分组至少包含一个 metric 模块。这是根据采样间隔做的逻辑分组。在gmond.conf下的不会影响web下的分组结果，默认配置如下：1234567891011121314151617181920212223collection_group &#123; collect_once = yes time_threshold = 1200 metric &#123; name = &quot;cpu_num&quot; title = &quot;CPU Count&quot; &#125; &#125;collection_group &#123; collect_every = 20 time_threshold = 90 /* CPU status */ metric &#123; name = &quot;cpu_user&quot; value_threshold = &quot;1.0&quot; title = &quot;CPU User&quot; &#125; metric &#123; name = &quot;cpu_system&quot; value_threshold = &quot;1.0&quot; title = &quot;CPU System&quot; &#125;&#125; collect_once (boolean)某些度量信息除非重启否则不会改变，如操作系统类型，cpu核数等。这些参数只需要启动时采集一次即可。该参数和collect_every互斥。 collect_every (seconds)频繁轮训采集时间，如cpu_user，cpu_system每20秒采集一次。 time_threshold (seconds)最大等待时间，gmond发送collection_group数据到所有udp_send_channels的时间。 name (text)度量信息在度量模块中的名称。典型的，每个度量模块定义多个度量信息名称，一个可选的name可以是name_match，如果使用name_match代替name，可以匹配多个度量名称，例如：namematch = “multicpu([a-z]+)([0-9]+)” value_threshold (number)每次收集度量信息，新值会和上一次的值进行比对。如果发生变化并且当前值大于配置值，则整个收集群组会发送到udp_send_channels定义的通道内。 title (text)一个用于web展示的用户友好的度量标题。 gmetadgmetad,the Ganlia Meta Daemon，安装在运行了收集度量信息的gmond节点之上，负责度量信息的收集和聚合。默认情况下，gmetad收集并聚合度量信息存储到RRD文件，但可以配置gmetad向其他系统汇总数据，如Graphite。 gmetad监听tcp端口8651，连接远程的gmetad并提供授权节点的xml dump状态文件。通过8652tcp端口响应其他节点的请求。交互的设备接受简易的子树和xml网格状态的总览。gweb通过使用这些查询在展现不适合存储在RRD中的数据，比如操作系统版本信息。 gmetad拓扑一个最简单的拓扑结构如下图所示，只存在一个gmetad负责轮训多个gmond实例: 高可用性是通常的需求也比较容易实现。如下图所示，两个gmetad和多个gmond实例。gmetad如果从node1拉取不到，将会从node2拉取。两个gmetad也会同时工作: gmetad并未限制轮训gmond，也可以拉取gmetad以创造另一个gmetad层级。如下图： 在更大的集群中，IO成为性能瓶颈，rrdcached作为gmetad和RRD文件的中间缓存，如图: gmetad.confgmetad.conf配置文件由单行属性和相对应的值组成。名称大小写不敏感，但value不同。如下所示属性表示的是相同的名称： name NAME Name NaMe 大多数属性是可选的；另外的是必须的。有些可被定义多次，有些只能被定义一次。 The data_source attribute. data_source是gmetad的核心配置。每一行data_source描述一个gmond集群或者一个由gmetad负责收集和聚合的网络。gmetad可以自行区分是一个cluster还是一个由gmetad主导的网格，所以data_source对二者都是相等的。如果gmetad发现data_source指向一个cluster ，它从data_source将维持完成的轮训列表。否则，gmetad会认为data_source指向网格，它只保存RRD的相关概要信息。 设置scalable属性为false，会强制gmetad保持完整的RRD文件集合以用于网格数据源。 以默认配置文件为例： data_source &quot;my cluster&quot; 10 localhost my.machine.edu:8649 1.2.3.5:8655 data_source &quot;my grid&quot; 50 1.3.4.7:8655 grid.org:8651 grid-backup.org:8651 data_source &quot;another source&quot; 1.3.4.8:8655 1.3.4.8 每个data_source由三部分构成。第一部分唯一标识该数据源。第二部分指定轮训时间间隔，单位是秒。第二部分表示gmetad轮训数据的空格分割的host地址列表，可以使用IP或者DNS识别的域名。最后一部分表示tcp端口，默认8649。 gmetad将按顺序检查列表中每个Host，带着第一个节点的状态信息做出响应。所以没必要在data_source中列举集群内所有节点。两三个既能保证数据不会出错。 gmetad daemon behavior. 属性列举如下： gridname (text) 字符串类型，唯一标识一个gmetad网格。这个字符串不能与gmond集合中的名称冲突。在gmond.conf（cluster中的配置项{name=”XXX”}）用于表示哪些特定的gmond实例负责收集。而gridname属性将会用Grid标签包裹数据源的数据。可以定义为data_source的收集者。 authority (URL)网格有效的URL，用于其他gmetad实例访问当前数据源的图表信息，默认地址为：http://hostname/ganglia/ trusted_hosts (text)gmetad的信任地址，localhost是用被信任的，空格分割。 all_trusted (on|off)设置为on则重写trusted_hosts配置，任何节点都被信任。 setuid_username (UID)gmetad使用的用户id，默认是nobody。 setuid (on|off)是否禁用uid。 xml_port (number)gmetad的监听端口，默认8651. interactive_port (number)gmetad交互端口，默认8652。与上个配置项对应。 server_threads (number)连接到监听端口的最大连接数。默认为4。 case_sensitive_hostnames (1|0)在早期的版本中，RRD文件使用大小写敏感的hostname创建，但如今已不在使用。3.2版本后默认为0。 RRDtool attributes. 配置RRD文件的创建和处理。 RRAs (text)These specify custom Round Robin Archive values. The default is (with a “stepsize” of 15 seconds): “RRA:AVERAGE:0.5:1:5856” “RRA:AVERAGE:0.5:4:20160” “RRA:AVERAGE:0.5:40:52704” The full details of an RRA specification are contained in the manpage forrrdcreate(1). umask (number)Specifies the umask to apply to created RRD files and the directory structure containing them. It defaults to 022. rrd_rootdir (path)Specifies the Graphite support. 可以将gmetad收集的度量数据全部导出到Graphite，一个第三方的开源的度量信息存储和可视化展示工具，配置参数如下： carbon_server (address)hostname或者ip，远程的daemon地址。 carbon_port (number)远程端口号，默认2003。 graphite_prefix (text)Graphite使用点号分割的路径组织和引用度量信息，所以更合适写一个前缀来表示描述度量信息。如datacenter1.gmetad carbon_timeout (number)gmetad等待Graphite的响应时间，单位毫秒。这个设置十分重要，因为gmetad的sender不是线程的并且会产生阻塞。默认500。 gmetad interactive port query syntax. 正如前面所提，gmetad监听tcp端口8652用于响应请求信息。这个请求基础的功能是获取grid中他们感兴趣的xml类型的dump状态信息。 通过文本协议，如SMTP和HTTP。请求有层级结构，以（/）斜线开始。比如如下请求会返回所有度量信息： / 如果要更明确请求内容，可以指定集群名称，如： /cluster1 如果要更明确节点内容，可以指定节点名称，如： /cluster1/host1 请求也可以通过指定后缀参数设置过滤器来修正度量值，如你可以获取cluster1的概要信息： /cluster1?filter=summary gweb相比ganglia的三个部分而言，gweb是最需要配置的组件。实际上，你不需要改变任何参数，gweb即可运行完整功能的web UI。 Apache virtual host configuration 尽管gweb本身不需要配置，但一些web容器想运行gweb需要作出部分配置。每个需要支持PHP的web容器需要做以下工作，还有许多是本书中未涉及的web配置参数。Apache Web Server是常用的web容器。假设gweb安装在/var/www/html/ganglia2，域名为 myganglia.example.org，则配置如下：1234567NameVirtualHost *.80&lt;VirtualHost *:80&gt;ServerName myganglia.example.orgServerAlias mygangliaDocumentRoot /var/www/html/ganglia2# Other directives here&lt;/VirtualHost&gt; 这只是一个简单的例子，更多详见：http://httpd.apache.org/docs/2.0/vhosts/ gweb options gweb的配置通过conf.php文件。实际上这个文件覆盖了默认文件conf_default.php。该文件在web目录的根目录下。该文档已编写完善，超过80个配置项不需要一一配置，只需使用时修改其中重要的几个即可。 该文件，正如名称所讲，是由许多参数组成的PHP脚本，不像其他的配置文件，参数有多行组成。这些属性名都是以$conf这种gweb格式组成，大小写敏感，看起来像PHP数组。如下参数表示gweb使用的RRDtool所在目录： $conf[&apos;rrdtool&apos;] = &quot;/usr/bin/rrdtool&quot;; 所有的参数都是必须的，也有一些被定义多次，也有被定义一次，也有引用其他的变量值，如： $conf[&apos;rrds&apos;] = &quot;${conf[&apos;gmetad_root&apos;]}/rrds&quot;; Application settings.该目录下的参数影响gweb的基础功能，它自己的家目录，比如，RRDs或者templates。这些很少被用户修改但偶尔会被提及。 templates (path)指定gweb的templates路径。就像一个站点的皮肤样式。 graphdir (path)指定定义图表的json文件所在路径。如下章所讲，用户会用json自定义图表样式，并将其存放在该目录下，用于web的UI展示。 rrds (path)指定RRD文件的所在目录。 又如第七章所提到的，不同的Nagios的特性在gweb的conf.php文件中集成。所以Nagios可以通过请求gweb获取度量信息，而不是Nagios Service Check Acceptor(NSCA)和Nagios Remote Plugin Executor (NRPE)。 Look and feel.gweb可以配置一次显示的图表数量(max_graphs)，也可以用来指定列数量和host视图。也有一些布尔类型的配置影响UI的默认行为，如 metric_groups_initially_collapsed。 config.php文件定义了大量样式，以及自定义时间范围。 Security.该属性参数如下： auth_system (readonly|enabled|disabled) gweb做了简单的安全认证机制，允许或禁止个别用户对部分应用功能的访问。如果设置为enabled则为可用。 Advanced features.参数如下：rrdcached_socket (path)指定rddcached的socket地址，用于高并发下的缓存策略。 graph_engine (rrdtool|graphite)gweb可以使用Graphite代替RRDtool作为图形引擎。该配置要求你已安装Graphite和Graphite webapp在该服务器上。详见：sourceforge.net/apps/trac/ganglia/wiki/ganglia-web-2#UsingGraphiteasthegraphingengine 部署运行到此为止，ganglia已经安装配置完成，是时候运行一下它们了！验证它们的基础功能是否完善并确保它们之间的通讯完成。 Starting Up the Processes 虽然没必要保证启动的先后顺序，如果按此处推荐顺序启动，能避免对元数据转换成udp聚合包的等待延迟，并且用户不会再web端看到错误数据。 如果你使用UDP单播模式，先启动UDP的收集节点。这样能确保该收集器能收集到各节点第一次发送的元数据。 启动其他的gmond实例。 如果你使用了rddcached，启动它。 从最低的层级开始启动gmetad。 启动其他层级的gmetad。 启动apache web server，比gmetad后启动，如果PHP脚本不能连接gmetad，可通过端口8652监控。 Testing Your Installationgmond和gmetad都监听tcp端口，为了测试gmond，使用telnet： user@host:$ telnet localhost 8649 作为回复，gmond会输出一段xml格式的dump信息，包含其度量信息。如果gmond是deaf模式或者mute模式，会返回一个空的xml文档，仅仅含有cluster标签。测试gmetad可以使用 user@host:$ telnet localhost 8651 一个有效的gmetad会返回一段xml格式的dump度量信息。详见第六章，了解更多验证程序状态的方法。 Firewalls防火墙问题是初装ganglia最普遍的问题，我们整理问题如下： gmond默认使用广播模式，所以跨越子网的集群需要配置单播的发送者和监听者，在之前的拓扑章节有介绍。如果gmond必须穿过防火墙与其他节点交互，允许端口udp/8649。对于广播模式，路由和防火墙必须支持IGMP协议。 gmond从端口TCP 8649监听gmetad连接。如果gmetad必须穿过防火墙，则保证其gmond的tcp 8649端口畅通。 gmetad使用tcp 8651，8652。前者类似于gmond的8649端口，后者作为“交互查询端口”用于响应指定的查询请求。被gweb使用，且通常与gmetad安装在同一台机器，所以除非你有使用更多的集成特性，如Nagios集成，或者有自定义的查询gmetad的脚本，不然不需要有防火墙的ACLs。 gweb运行在web容器中，通常监听80或者443端口(如果你使用SSL，则443)。如果web服务器被防火墙隔离，那么开放Tcp80和tcp443端口。 如果ganglia集成了sFlow收集器，并且每个sFlow收集器都要与gmond交互 ，为gmond的监听开放udp 6343端口。]]></content>
      <categories>
        <category>Ganglia</category>
      </categories>
      <tags>
        <tag>监控</tag>
        <tag>Ganglia</tag>
      </tags>
  </entry>
</search>
